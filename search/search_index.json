{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"customers/","title":"Customers Overview","text":"<p>Warning</p> <p>Do not edit this page, it's auto-generated by Github Actions</p>"},{"location":"customers/#ito-customers","title":"ITO customers","text":"Customer ERP Note Balocco Spa @ C2402 CR230225 ITO H24:  Servizio di backup da Veeam on-premise su Amazon S3   Buccellati Holding Italia SpA @ C2411 CR230248 ITO H24:  Buccellati e-commerce in Cina - Magento2.4 www.buccellati.com.cn Consulman S.r.l @ C0422 CR210526 ITO H24:  Gestione account AWS 645128989513 (M1, Poseidone, Thor, Domino, consulman.it e progettifinanziati.ferrari.com)  CR220148 ITO H24:  Rivendita account AWS   Diffusione Tessile S.r.l. @ C1701 CR230352 ITO H24:  MaxMara B2C in AWS - BIA  CR230325 ITO H24:  MaxMara B2C in Cina - Gestione infrastruttura Tencent  CR230363 ITO H24:  B2C in AWS - Moses (ex on-premise in BT)   Dinova S.r.l. (ex Hibo) @\u00a0C2475 CR210753 ITO H24:  Fondazione MAST www.mast.org, www.mastphotogrant.com Divitech SPA @ C0814 CR220556 ITO H24:  Helpme su AWS (helpme.divitech.it)   Energie S.r.l. @ C0971 Franz Kraler S.r.l. @ C2217 CR210268 ITO H24:  e-commerce B2B e B2C basato su Mulesoft franzkraler.force.com  CR200600 ITO H24:  PIM Core pim.franzkraler.it   Gruppo Buffetti S.p.A. @ C0324 CR230128 ITO H24:  B2B su AWS (b2b.buffetti.it)  CR210421 ITO H24:  B2C su AWS (www.buffetti.it)   Hippocrates @ C2377 CR240362 ITO H24:  Migrazione Linode TopFarmacia e LaFarmacia   Presenti Jakala @ C2249 CR210361 ITO H24:  TIM Party su GCP timparty.tim.it   Presenti Medspa @ C2371 CR220510 ITO H24:  e-commerce Magento2 miamo.com  CR230232 ITO H24:  Magento2 www.miamoacademy.it CR230232 ITO H24:  medspa.it   Microgame Spa @ C2311 CR230146 ITO H24:  Kubernetes on-premise su VMWare. Il servizio ITO copre l'ambiente di produzione anche se non sono ancora realmente in produzione. L'ambiente di dev e staging sono in best effort.   Mobilesoft @ C1887 CR220377 ITO H24:  FNMPay - Portale di pagamenti sul Cloud su Azure   New Penta Srl @ C2429 CR230291 ITO H24:  Account AWS 280010477674 - datahub.npdiet.com   SEQUAR S.r.l. @ C2113 CR190704 ITO H24:  ERP su Kubernetes on-premise con Odoo e RabbitMQ. (erp.sequar.com, erp.libellula.eu)   Presenti Silent Pool Spa @ C0002 CR010934 ITO H24:  Gestione account AWS cc-poc-test 281670075220   Travelmatic srl @ C0677 CR230430 ITO H24:  Account AWS 686952707636 (travelmatic.net, simplecrs.it)   WEBSOLUTE Spa @ C2325 CR230172 ITO H24:  Implementazione sito Teamsystem su AWS teamsystem.com   Presenti"},{"location":"customers/#non-ito-customers","title":"Non ITO customers","text":"Customer ERP Note AKQA Srl @ C0909 Presenti Atelier @ C2392 Atlantica Digital SpA @ C2058 Comau S.p.A. @ C1317 Epipoli Eurovita SPA GO GLOBAL ECOMMERCE S.R.L. @ C2269 GSPED @ C2234 Humanativa Group S.r.l. @ C2369 IDT S.p.A. - Brandsdistribution @ C2047 M-Iot SB SRL @ C2268 Omicron Consulting s.r.l. @ C2231 Quantico Games S.R.L. @ C2415 Realize S.R.L. @ C2269 Presenti SharryLand Sugar S.r.l. @ C2070 WHILE1 Wedoo Srl @ C0050 Wevee @ C2404 Worldmatch @ C2214 Presenti Presenti Presenti Presenti"},{"location":"customers/akqa-C0909/","title":"AKQA Srl @ C0909","text":""},{"location":"customers/akqa-C0909/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR220401: Qatar su Azure (qatar2022.qa)</li> </ul>"},{"location":"customers/akqa-C0909/#note-h24","title":"Note H24","text":"<p>~~Il cliente deve SEMPRE aprire un ticket, pu\u00f2 chiamare per far aumentare la priorit\u00e0 dell'incident oppure avere maggior informazioni sullo stato di un ticket aperto.~~</p> <p>Internal data:</p> <ul> <li>Start date: 01/09/2022</li> <li>Sales: Morena Scudieri</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Enrico Vedovo</li> <li>Email: Enrico.Vedovo@akqa.com</li> </ul>"},{"location":"customers/akqa-C0909/QATAR/","title":"QATAR","text":"<p>Website qatar2022.qa - CR220401</p>"},{"location":"customers/akqa-C0909/QATAR/#data-center","title":"Data Center","text":"<ul> <li> Azure: scqa0.onmicrosoft.com</li> </ul> Text Only<pre><code>Domain: scqa0.onmicrosoft.com\nName: Supreme Committee for Delivery and Legacy\nSubscription prod: 2eefea3d-c80a-46e4-a856-cd8aefad60fe\nSubscription dev: 97597f1e-2556-4072-a1b7-d3123837d21f\n</code></pre>"},{"location":"customers/akqa-C0909/QATAR/#operations","title":"Operations","text":"<ul> <li>Cloudflare: Load Balancer, Block a public IP address, Redirects and DR (public static website)</li> <li>Azure WAF: firewall on Application Gateway LB <code>403 Forbidden</code></li> <li>Azure Firewall Premium: inbound and outbound firewall <code>504 Gateway timeout</code></li> <li>DR Procedure: Drupal CMS Restore</li> </ul>"},{"location":"customers/akqa-C0909/QATAR/#teleport","title":"Teleport","text":"<p>Access to the VMs is possible via teleport trusted cluster:</p> <p>Select the cluster:</p> <ul> <li>PROD: <code>tp.prod-akqa-qatar.criticalcasecloud.com</code></li> <li>DR: <code>tp.dr-akqa-qatar.criticalcasecloud.com</code></li> </ul> <p>Or by vpn then teleport to this url:</p> <ul> <li>PROD: https://tp.prod-akqa-qatar.criticalcasecloud.com</li> <li>DR: https://tp.dr-akqa-qatar.criticalcasecloud.com</li> </ul>"},{"location":"customers/akqa-C0909/QATAR/#vpn","title":"VPN","text":"<ul> <li>prod: linux and mac, windows</li> <li>dr: linux and mac, windows</li> </ul>"},{"location":"customers/akqa-C0909/QATAR/#azure-environments","title":"Azure Environments","text":"<ul> <li>prod: rg-prod-we-01 West Europe</li> <li>dr: rg-dr-ne-01 North Europe</li> </ul>"},{"location":"customers/akqa-C0909/QATAR/#architecture","title":"Architecture","text":"<ul> <li>Download drawio</li> </ul>"},{"location":"customers/akqa-C0909/QATAR/#network-architecture","title":"Network Architecture","text":"<ul> <li>Download drawio</li> </ul>"},{"location":"customers/akqa-C0909/QATAR/Cloudflare/","title":"Cloudlfare","text":""},{"location":"customers/akqa-C0909/QATAR/Cloudflare/#block-ip-address-or-ip-subnet-waf-rules","title":"Block IP Address or IP subnet (WAF Rules)","text":"<p>With rule block_selected_ip all the IP addresses in a specific list are blocked from the Cloudflare WAF. This block is effective for all the environments (PROD + STG).</p> <p>In case one or more IP addresses or subnet must be added to/ removed from the list of blocked IP addresses, it is possible to add/remove them to the ban_ip_criticalcase list.</p> <ul> <li>Go to the list</li> <li>Press Add items</li> <li>Insert the IP address(es)</li> <li>Specify the ticket ID as comment</li> <li>Press: Add to list when done</li> </ul>"},{"location":"customers/akqa-C0909/QATAR/Cloudflare/#block-ipv6-addresses","title":"Block IPv6 addresses","text":"<p>Since it is not possible to add individual IPv6 addresses to a list, the address to block must be added directly in the rule.</p> <ul> <li>Go to the block_selected_IPv6 rule</li> <li>Click the \"OR\" button to add a new row   </li> <li>Select</li> <li>Field: IP Source Address</li> <li>Operator: equals</li> <li>Value: \\ <li>Press: Save when done</li>"},{"location":"customers/akqa-C0909/QATAR/Cloudflare/#block-user-agent","title":"Block User Agent","text":"<ul> <li>Go to the block_user_agent rule</li> <li>Click the \"OR\" button to add a new row</li> <li>Select <code>User Agent</code> as Field and complete the row</li> <li>Press: Save when done</li> </ul>"},{"location":"customers/akqa-C0909/QATAR/Cloudflare/#redirect","title":"Redirect","text":"<p>Cloudflare redirects are enabled from the Bulk Redirects section. You can enabled/disable the redirect lists from here.</p> <ul> <li>Edit redirect list bulkredirect2</li> <li>Press: Add items</li> <li>Source URL must starts with: www.qatar2022.qa/</li> <li>Press: Add to list when done</li> </ul>"},{"location":"customers/akqa-C0909/QATAR/Cloudflare/#clear-cache","title":"Clear cache","text":"<ul> <li>Go to cache configuration settings</li> <li>Press: Purge Everything</li> </ul>"},{"location":"customers/akqa-C0909/QATAR/Cloudflare/#disable-production-region","title":"Disable production region","text":"<p>Go to www.qatar2022.qa Traffic Load Balancer.</p> <p>Disable the Production Pool: Qatar2022-Prod</p> <p></p>"},{"location":"customers/akqa-C0909/QATAR/Cloudflare/#load-balancer","title":"Load balancer","text":"<ul> <li>www.qatar2022.qa is managed from the Traffic Load Balancer.</li> <li>Health check are managed from Monitors section</li> <li>Backend pool are managed from the origin pools section.</li> </ul>"},{"location":"customers/akqa-C0909/QATAR/Cloudflare/#transform-rules-lowercase","title":"Transform rules lowercase","text":"<p>In cloudflare two transform rules have been created to convert all the requested paths to lowercase, to avoid 404 pages</p> <p>Links to cloudflare transform rules:</p> <ul> <li>stage rule</li> <li>prod rule</li> </ul> <p>Some specific paths are excluded:</p> <ul> <li> <p><code>/book</code> : managed by third-party</p> </li> <li> <p><code>/logistics</code> : managed by third-party</p> </li> <li> <p><code>/sites/default/files</code> : some uppercase letters can be present in file names</p> </li> <li> <p><code>/themes/custom/qatar_2022</code> : some uppercase letters can be present in file names</p> </li> </ul>"},{"location":"customers/akqa-C0909/QATAR/DR-Procedure/","title":"QATAR: DR Procedure","text":"<ul> <li>Website qatar2022.qa - CR220401: Main Docs</li> </ul>"},{"location":"customers/akqa-C0909/QATAR/DR-Procedure/#switch-to-dr-enviroment-procedure","title":"Switch to DR enviroment procedure","text":"<ol> <li> <p>Connect to the Azure portal and go to the recovery service vault section, select prod-recovery-vault then backup items and secondary region:    </p> </li> <li> <p>Click on 'Azure Virtual Machine' Backup Management Type and select view details in the 'prod-drupal-vm-1' line </p> </li> <li> <p>Then select 'Restore to secondary region' and chose a restore point (possibly the most recent one):    </p> </li> <li> <p>Configure with the following parameters the creation of the new VM from the backup. In the end, click restore:    </p> </li> <li> <p>Repeat all the previous steps for 'prod-drupal-vm-2', naming it 'dr-drupal-vm-2'.</p> </li> <li> <p>Both the machines already have a startup script (<code>/usr/local/bin/recovery_dr.sh</code>) which is executed only if the network interface is in dr environment. Its job consists in configuring nfs, redis, teleport and hostname for the dr environment. The machines, however, lack all the files hosted on the nfs and the mysql paas database is empty. We then proceed to restore files and databases.</p> </li> <li> <p>If the VMs do not start automatically, you should turn them on from the Azure portal. After a few minutes they should appear under teleport. If they are not visible, use the serial console of the vm from the azure portal and check the problem. The accesses via the <code>azureadmin</code>user and the password can be found here (drupal-vm-qatar-password)</p> </li> <li> <p>The backups are located in the <code>drqatarnfsmysqlbackup</code> storage account in the <code>file</code> and <code>mysql</code> containers. The <code>DRTOKEN.txt</code> file located in the <code>/root</code> of the two restored VMs contains the coordinates and the token to access the storage account.</p> </li> <li> <p>From one of the two vm (it does not matter which one as we will populate an nfs and a db), log in as root, download the most recent files (check on Azure in the file and mysql container which are the most recent files, since backup are executed every three hours from each of the prod VMs, consequently one of the two will be more recent than the other) of backup via azcopy. Let's proceed with the restore of the mysql database.        For example, to copy the most recent dump, which in this case is prod-drupal-vm-1.qatar2022_prod.data.sql.gz, use the command:</p> </li> </ol> Bash<pre><code># azcopy copy 'https://complete_url_of_file?TOKEN' &lt;destination&gt;\nazcopy copy 'https://drqatarnfsmysqlbackup.blob.core.windows.net/mysql/prod-drupal-vm1.qatar2022_prod.data.sql.gz?sv=2021-06-08&amp;ss=bfqt&amp;srt=sco&amp;sp=rwdlacupiytfx&amp;se=2032-09-13T16:08:38Z&amp;st=2022-09-13T08:08:38Z&amp;spr=https&amp;sig=NglUBbVXT4LKX58B2Wkt2hd1SgDCa1JvBzr7rBfvmm0%3D'.\n</code></pre> <p></p> <ol> <li>Now we can unzip the file and restore the dump:</li> </ol> Bash<pre><code>#unzip\ngunzip prod-drupal-vm-1.qatar2022_prod.data.sql.gz\n#restore\nmysql qatar2022_prod &lt;prod-drupal-vm-1.qatar2022_prod.data.sql\n</code></pre> <ol> <li>Now we can move on to restore the files on the nfs shares. Download the most recent file from the container using azcopy.</li> </ol> <p>IMPORTANT, go under <code>/root</code> and use the root user</p> Bash<pre><code>azcopy copy 'https://drqatarnfsmysqlbackup.blob.core.windows.net/file/prod-drupal-vm-2.tar.gz?  sv=2021-06-08&amp;ss=bfqt&amp;srt=sco&amp;sp=rwdlacupiytfx&amp;se=2032-09-13T16:08:38Z&amp;st=2022-09-13T08:08:38Z&amp;spr=https&amp;sig=NglUBbVXT4LKX58B2Wkt2hd1SgDCa1JvBzr7rBfvmm0%3D'.\n</code></pre> <p></p> <ol> <li>We then extract the archive in the correct path     &gt; ATTENTION to the '-C /' which specifies the tar to recreate the archive by proceeding from the root     &gt; {.is-warning}</li> </ol> Bash<pre><code>tar -zxvf prod-drupal-vm-2.tar.gz -C /\n</code></pre> <ol> <li> <p>At this point, if everything went well, we can test the site by connecting to the DR vpn and pointing the browser at https://dr-cms.qatar2022.qa we should see the cms page or the site.</p> </li> <li> <p>Check the permissions in <code>/var/www/drupal/htdocs/web/sites/default/files</code>. It must be:</p> </li> <li> <p>user=qatar</p> </li> <li>group=qatar   otherwise fix the permissions by chown:</li> </ol> Bash<pre><code>chown -R qatar:qatar /var/www/drupal/htdocs/web/sites/default/files\n</code></pre>"},{"location":"customers/akqa-C0909/QATAR/Firewall/","title":"Azure Firewall Premium","text":"<p>If users receive a <code>504 gateway timeout</code>, it is probably the firewall that is blocking the requests and it is necessary to switch both ipds and threat intelligence to detection mode following the procedure indicated:</p>"},{"location":"customers/akqa-C0909/QATAR/Firewall/#switch-to-alert-only","title":"Switch to alert only","text":"<p>This change must be done in prod and dr environment</p> <ul> <li>Search in the Azure portal for the Firewall manager</li> <li> <p>Open the Azure Firewall policy related to the Firewall to set in alert only (hub-prod-fw-policy)   </p> </li> <li> <p>Set Threat Intelligence to \"Alert Only\"   </p> </li> <li> <p>Set IDPS to \"Alert\"   </p> </li> <li> <p>Open the Azure Firewall policy related to the Firewall to set in alert only (hub-dr-fw-policy)</p> </li> <li>Set Threat Intelligence to \"Alert Only\"</li> <li>Set IDPS to \"Alert\"</li> </ul>"},{"location":"customers/akqa-C0909/QATAR/Firewall/#log-analytics-workspace-queries","title":"Log analytics workspace queries","text":""},{"location":"customers/akqa-C0909/QATAR/Firewall/#find-ipsids-matched-rules-details","title":"Find ips/ids matched rules details","text":"SQL<pre><code>// Azure Firewall log data\n// Start from this query if you want to parse the logs from network rules, application rules, NAT rules, IDS, threat intelligence and more to understand why certain traffic was allowed or denied. This query will show the last 100 log records but by adding simple filter statements at the end of the query the results can be tweaked.\n// Parses the azure firewall rule log data.\n// Includes network rules, application rules, threat intelligence, ips/ids, ...\nAzureDiagnostics\n//| where Category == \"AzureFirewallNetworkRule\" or Category == \"AzureFirewallApplicationRule\"\n//optionally apply filters to only look at a certain type of log data\n//| where OperationName == \"AzureFirewallNetworkRuleLog\"\n//| where OperationName == \"AzureFirewallNatRuleLog\"\n//| where OperationName == \"AzureFirewallApplicationRuleLog\"\n| where OperationName == \"AzureFirewallIDSLog\"\n//| where OperationName == \"AzureFirewallThreatIntelLog\"\n| extend msg_original = msg_s\n// normalize data so it's eassier to parse later\n| extend msg_s = replace(@'. Action: Deny. Reason: SNI TLS extension was missing.', @' to no_data:no_data. Action: Deny. Rule Collection: default behavior. Rule: SNI TLS extension missing', msg_s)\n| extend msg_s = replace(@'No rule matched. Proceeding with default action', @'Rule Collection: default behavior. Rule: no rule matched', msg_s)\n// extract web category, then remove it from further parsing\n| parse msg_s with * \" Web Category: \" WebCategory\n| extend msg_s = replace(@'(. Web Category:).*','', msg_s)\n// extract RuleCollection and Rule information, then remove it from further parsing\n| parse msg_s with * \". Rule Collection: \" RuleCollection \". Rule: \" Rule\n| extend msg_s = replace(@'(. Rule Collection:).*','', msg_s)\n// extract Rule Collection Group information, then remove it from further parsing\n| parse msg_s with * \". Rule Collection Group: \" RuleCollectionGroup\n| extend msg_s = replace(@'(. Rule Collection Group:).*','', msg_s)\n// extract Policy information, then remove it from further parsing\n| parse msg_s with * \". Policy: \" Policy\n| extend msg_s = replace(@'(. Policy:).*','', msg_s)\n// extract IDS fields, for now it's always add the end, then remove it from further parsing\n| parse msg_s with * \". Signature: \" IDSSignatureIDInt \". IDS: \" IDSSignatureDescription \". Priority: \" IDSPriorityInt \". Classification: \" IDSClassification\n| extend msg_s = replace(@'(. Signature:).*','', msg_s)\n// extra NAT info, then remove it from further parsing\n| parse msg_s with * \" was DNAT'ed to \" NatDestination\n| extend msg_s = replace(@\"( was DNAT'ed to ).*\",\". Action: DNAT\", msg_s)\n// extract Threat Intellingence info, then remove it from further parsing\n| parse msg_s with * \". ThreatIntel: \" ThreatIntel\n| extend msg_s = replace(@'(. ThreatIntel:).*','', msg_s)\n// extract URL, then remove it from further parsing\n| extend URL = extract(@\"(Url: )(.*)(\\. Action)\",2,msg_s)\n| extend msg_s=replace(@\"(Url: .*)(Action)\",@\"\\2\",msg_s)\n// parse remaining \"simple\" fields\n| parse msg_s with Protocol \" request from \" SourceIP \" to \" Target \". Action: \" Action\n| extend\n    SourceIP = iif(SourceIP contains \":\",strcat_array(split(SourceIP,\":\",0),\"\"),SourceIP),\n    SourcePort = iif(SourceIP contains \":\",strcat_array(split(SourceIP,\":\",1),\"\"),\"\"),\n    Target = iif(Target contains \":\",strcat_array(split(Target,\":\",0),\"\"),Target),\n    TargetPort = iif(SourceIP contains \":\",strcat_array(split(Target,\":\",1),\"\"),\"\"),\n    Action = iif(Action contains \".\",strcat_array(split(Action,\".\",0),\"\"),Action),\n    Policy = case(RuleCollection contains \":\", split(RuleCollection, \":\")[0] ,Policy),\n    RuleCollectionGroup = case(RuleCollection contains \":\", split(RuleCollection, \":\")[1], RuleCollectionGroup),\n    RuleCollection = case(RuleCollection contains \":\", split(RuleCollection, \":\")[2], RuleCollection),\n    IDSSignatureID = tostring(IDSSignatureIDInt),\n    IDSPriority = tostring(IDSPriorityInt)\n| project msg_original,TimeGenerated,Protocol,SourceIP,SourcePort,Target,TargetPort,URL,Action, NatDestination, OperationName,ThreatIntel,IDSSignatureID,IDSSignatureDescription,IDSPriority,IDSClassification,Policy,RuleCollectionGroup,RuleCollection,Rule,WebCategory\n| order by TimeGenerated\n//| limit 100\n</code></pre>"},{"location":"customers/akqa-C0909/QATAR/ImportWafRules/","title":"ImportWafRule command","text":"<p>This program generates a bash script to import exclusions from one Azure Waf Policy Managed Rule into another. This is useful for keeping different environments aligned with the same hand made rules via the Azure portal. The program is available in single binary file for win64, osx64, linux64. The program can be downloaded at https://github.com/criticalcase/ImportWafRule/releases/tag/v1.0.0</p> <p>The necessary arguments are:</p> <ul> <li>resource group of the target policy</li> <li>policy name of the target policy</li> <li>json of the source policy</li> </ul> <p>The json of the source policy is retrievable by az cli:</p> Text Only<pre><code># login to Azure by (use pim with your user in the Azure portal to get the correct permissions)\naz login\n\n# set the correct subscription by\naz account set --subscription mysubscriptionid\n\n# export the rules in the json format\naz network application-gateway waf - policy managed - rule exclusion rule-set list--policy - name 'MyPolicy'--resource-group 'MyResourceGroup' &gt; myfile.json\n</code></pre> <p>Use:</p> Text Only<pre><code>./ImportWafRule \"MyResourceGroup\" \"MyPolicy\" \"myfile.json\" &gt; myscript.sh\n</code></pre> <p>Example:</p> Text Only<pre><code>./ImportWafRule \"rg-dr-ne-01\" \"dr-drupal\" \"rules_from_prod.json\" &gt; import_dr.sh\n</code></pre>"},{"location":"customers/akqa-C0909/QATAR/ImportWafRules/#using-the-generated-script","title":"Using the generated script","text":"<p>This example was run in linux environment, adapt the commands for your own environment. In this example, we exported production rules (see example above), created by hand, to align them in the disaster recovery environment for the drupal waf.</p> <p>First, login to Azure and set the correct subscription:</p> Text Only<pre><code># login to Azure by (use pim with your user in the Azure portal to get the correct permissions)\naz login\n\n# set the correct subscription by\naz account set --subscription mysubscriptionid\n</code></pre> <p>Now run the previous generated script (make it executable)</p> Text Only<pre><code>chmod a+x import_dr.sh\n\n./import_dr.sh\n</code></pre> <p>The will be:</p> Text Only<pre><code>WAF exclusion rules script for dr-drupal policy - resource group rg-dr-ne-01\n--------------------------------------------------------------------------------------------------------------------------------------\n\nWARNING:\n\nI will now DELETE all the dr-drupal waf policy rules\nin the rg-dr-ne-01 resource group.\nThen I will ADD the new rules previously generated from the rl.json json file.\n\nDo you want to proceed? (y/n)\n</code></pre> <p>Type y, the output will be:</p> Text Only<pre><code>Do you want to proceed? (y/n) y\n\nDeleting waf policy rules...done\nAdd RULE 1...done\nAdd RULE 2...done\nAdd RULE 3...done\nAdd RULE 4...done\nAdd RULE 5...done\nAdd RULE 6...done\nAdd RULE 7...done\nAdd RULE 8...done\nAdd RULE 9...done\nAdd RULE 10...done\nAdd RULE 11...done\nAdd RULE 12...done\nAdd RULE 13...done\nAdd RULE 14...done\nAdd RULE 15...done\nAdd RULE 16...done\nAdd RULE 17...done\nAdd RULE 18...done\nAdd RULE 19...done\nAdd RULE 20...done\nAdd RULE 21...done\nAdd RULE 22...done\nAdd RULE 23...done\nAdd RULE 24...done\nAdd RULE 25...done\nAdd RULE 26...done\nAdd RULE 27...done\nAdd RULE 28...done\nAdd RULE 29...done\nAdd RULE 30...done\nAdd RULE 31...done\nAdd RULE 32...done\nAdd RULE 33...done\nAdd RULE 34...done\nAdd RULE 35...done\nAdd RULE 36...done\nAdd RULE 37...done\nAdd RULE 38...done\nAdd RULE 39...done\nAdd RULE 40...done\nAdd RULE 41...done\nAdd RULE 42...done\nAdd RULE 43...done\nAdd RULE 44...done\nAdd RULE 45...done\nAdd RULE 46...done\nAdd RULE 47...done\nAdd RULE 48...done\nAdd RULE 49...done\nAdd RULE 50...done\nAdd RULE 51...done\n--------------------------------------------------------------------------------------------------------------------------------------\nTotal exclusion = 30 - Rules Generated = 51 - Policy name = dr-drupal - Resource Group = rg-dr-ne-01\nLog file = importwafrule.log\n</code></pre> <p>Now the dr environment has the same rules as the prod environment. If any error appears, please look at the file importwafrule.log in the same script folder.</p>"},{"location":"customers/akqa-C0909/QATAR/VpnCertificates/","title":"VPN Certificates","text":"<p>Vpn certificates are managed by EasyRSA Private CA placed on vaultds.rz2.cc.local.</p> <p>https://cc-ds1.criticalcasecloud.com/web/cluster/rz2-ds/nodes?search=vaultds.rz2.cc.local&amp;sort=hostname:asc</p> <p>There are 2 script, one to create a new certificate and one to revoke a certificate</p> <ul> <li>/akqa-easy-rsa/EasyRSA-3.1.0/generate_ovpn_conf.sh</li> <li>/akqa-easy-rsa/EasyRSA-3.1.0/revoke_cert.sh</li> </ul>"},{"location":"customers/akqa-C0909/QATAR/VpnCertificates/#create-a-new-certificate","title":"Create a new certificate","text":"<p>The script /akqa-easy-rsa/EasyRSA-3.1.0/generate_ovpn_conf.sh, creates a new certificate, creates OpenVPN configuration files for Windows and Mac computer then for PROD and DR enviroment, also sends a mail to the final user with 4 OpenVpn configuration files and a message with instructions.</p> <p>Please follow this steps for creating a new certificate and configuration:</p> Text Only<pre><code># login by root in vault vm then change directory\n\ncd /akqa-easy-rsa/EasyRSA-3.1.0\n\n# launch the script with email of the new vpn client for example:\n./generate_ovpn_conf.sh andrea.barbaglia@gmail.com\n</code></pre> <p>The output will be:</p> Text Only<pre><code>Generating certificate and private key for andrea.barbaglia@gmail.com...* Notice:\nUsing Easy-RSA configuration from: /root/akqa-easy-rsa/EasyRSA-3.1.0/pki/vars\n\n* Notice:\nUsing SSL: openssl OpenSSL 1.1.1  11 Sep 2018\n\nGenerating a RSA private key\n......................+++++\n......+++++\nwriting new private key to '/root/akqa-easy-rsa/EasyRSA-3.1.0/pki/41c59d71/temp.4e8396e6'\n-----\n* Notice:\n\nKeypair and certificate request completed. Your files are:\nreq: /root/akqa-easy-rsa/EasyRSA-3.1.0/pki/reqs/andrea.barbaglia@gmail.com.req\nkey: /root/akqa-easy-rsa/EasyRSA-3.1.0/pki/private/andrea.barbaglia@gmail.com.key\n\nUsing configuration from /root/akqa-easy-rsa/EasyRSA-3.1.0/pki/safessl-easyrsa.cnf.init-tmp\nCheck that the request matches the signature\nSignature ok\nThe Subject's Distinguished Name is as follows\ncommonName            :ASN.1 12:'andrea.barbaglia@gmail.com'\nCertificate is to be certified until Feb 27 18:30:40 2025 GMT (825 days)\n\nWrite out database with 1 new entries\nData Base Updated\n\n* Notice:\nCertificate created at: /root/akqa-easy-rsa/EasyRSA-3.1.0/pki/issued/andrea.barbaglia@gmail.com.crt\n\n\ndone\nCreating hub-prod mac ovpn conf for andrea.barbaglia@gmail.com...done\nCreating hub-dr mac ovpn conf for andrea.barbaglia@gmail.com...done\nCreating hub-prod win ovpn conf for andrea.barbaglia@gmail.com...done\nCreating hub-dr win ovpn conf for andrea.barbaglia@gmail.com...done\nMail sent to andrea.barbaglia@gmail.com\n</code></pre> <p>and the user will receive the email:</p> <p></p>"},{"location":"customers/akqa-C0909/QATAR/VpnCertificates/#revoke-a-certificate","title":"Revoke a certificate","text":"<p>The script /akqa-easy-rsa/EasyRSA-3.1.0/revoke_cert.sh, revokes the certificate and generates the SHA1 Fingerprint needed for Azure Virtual Network Gateway.</p> <p>Please follow this steps for revoke a certificate and generate the fingerprint:</p> Text Only<pre><code># revoke certificate by email\n\n./revoke_cert.sh enrico.vedovo@akqa.com\n</code></pre> <p>The output will be:</p> Text Only<pre><code>Revoking certificate and private key for enrico.vedovo@akqa.com...* Notice:\nUsing Easy-RSA configuration from: /root/akqa-easy-rsa/EasyRSA-3.1.0/pki/vars\n\n* Notice:\nUsing SSL: openssl OpenSSL 1.1.1  11 Sep 2018\n\n\n  Please confirm you wish to revoke the certificate\n  with the following subject:\n\n  subject=\n    commonName                = enrico.vedovo@akqa.com\n\n  serial-number: 318927F5B11B778244F72FD7E86E561A\n\n\nType the word 'yes' to continue, or any other input to abort.\n    Continue with revocation:\n</code></pre> <p>Type \"yes\":</p> Text Only<pre><code>    Continue with revocation: yes\n\nUsing configuration from /root/akqa-easy-rsa/EasyRSA-3.1.0/pki/safessl-easyrsa.cnf.init-tmp\nRevoking Certificate 318927F5B11B778244F72FD7E86E561A.\nData Base Updated\n\n* Notice:\n\nIMPORTANT!!!\n\nRevocation was successful. You must run gen-crl and upload a CRL to your\ninfrastructure in order to prevent the revoked cert from being accepted.\n\ndone\n\nGenerating SHA1 Fingerprint...done\n\nPlease copy the following SHA1 FingerPrint on PROD and DR Azure Virtual Network Gateway in the Point-to-site configuration section:\nSHA1 Fingerprint=B4564F01B119AC315A48835E881D1B82E271C5FA\n</code></pre> <p>Please copy the SHA1 FingerPrint on PROD and DR Azure Virtual Network Gateway in the Point-to-site configuration section:</p> <p></p> <p>Click on the Save button. Repeat the last step, Azure configuration, on DR Virtual Network Gateway.</p>"},{"location":"customers/akqa-C0909/QATAR/WAF/","title":"Azure WAF","text":"<p>If users receive a 403 http error code, it is probably the waf that is blocking the requests and it is necessary to switch to detection mode following the procedure indicated:</p>"},{"location":"customers/akqa-C0909/QATAR/WAF/#switch-to-detection-mode","title":"Switch to detection mode","text":"<p>This change must be done in prod and dr environment</p> <ul> <li>Search in the Azure portal for the Web Application Firewall policies</li> <li>Select the policy related to the WAF to set in detection mode (prod-drupal)</li> <li> <p>Press <code>Switch to detection mode</code></p> </li> <li> <p>Select the policy related to the WAF to set in detection mode (dr-drupal)</p> </li> <li>Press <code>Switch to detection mode</code></li> </ul> <p></p>"},{"location":"customers/akqa-C0909/QATAR/WAF/#log-analytics-workspace-queries","title":"Log analytics workspace queries","text":""},{"location":"customers/akqa-C0909/QATAR/WAF/#find-waf-matched-rules-details-specific-appgw","title":"Find WAF matched rules details - specific appgw","text":"SQL<pre><code>AzureDiagnostics\n| where ResourceProvider == \"MICROSOFT.NETWORK\" and Category == \"ApplicationGatewayFirewallLog\" and Resource == \"PROD-DRUPAL\"\n| summarize AggregatedValue = count() by ruleId_s, Message, details_file_s, details_data_s\n| order by AggregatedValue desc\n</code></pre>"},{"location":"customers/akqa-C0909/QATAR/WAF/#find-waf-matched-rules-details-specific-client","title":"Find WAF matched rules details - specific client","text":"SQL<pre><code>AzureDiagnostics\n| where ResourceProvider == \"MICROSOFT.NETWORK\" and Category == \"ApplicationGatewayFirewallLog\" and clientIp_s == \"172.16.59.2\"\n| summarize AggregatedValue = count() by ruleId_s, Message, details_file_s, details_data_s\n| order by AggregatedValue desc\n</code></pre>"},{"location":"customers/akqa-C0909/QATAR/WAF/#find-waf-matched-rules-details-specific-url","title":"Find WAF matched rules details - specific url","text":"SQL<pre><code>AzureDiagnostics\n| where ResourceProvider == \"MICROSOFT.NETWORK\" and Category == \"ApplicationGatewayFirewallLog\" and requestUri_s contains \"en/node/645/edit\" and Message notcontains \"Mandatory\"\n</code></pre>"},{"location":"customers/akqa-C0909/QATAR/WAF/#find-waf-403-status-for-specific-url","title":"Find WAF 403 status for specific url","text":"SQL<pre><code>AzureDiagnostics\n| where ResourceType == \"APPLICATIONGATEWAYS\" and OperationName == \"ApplicationGatewayAccess\" and requestUri_s contains \"/book\" and httpStatus_d == \"403\"\n</code></pre>"},{"location":"customers/akqa-C0909/QATAR/enable-contributor/","title":"Enable Contributor Role","text":"<p>In order to access the resources in all subscriptions, you need to enable your role, at least once every 8 hours.</p> <ol> <li>Login to Azure portal with your credentials.</li> <li>Search for PIM in the main search bar.</li> </ol> <p></p> <ol> <li>Click on Azure AD Privileged Identity Management</li> <li>Click on My Roles.</li> </ol> <p></p> <ol> <li>Click on Azure Resources and Activate your Role.    </li> <li>Once you Activate and it will ask you MFA and Justification, please provide the details as below. You can reduce the hours as per your convenient.</li> </ol> <p></p> <ol> <li> <p>Click on Activate once you provided above MFA and Justification.</p> </li> <li> <p>It will be activated.</p> </li> </ol>"},{"location":"customers/atelier-C2392/","title":"Atelier @ C2392","text":""},{"location":"customers/atelier-C2392/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR230143: Infrastruttura su AWS</li> </ul> <p>Internal data:</p> <ul> <li>Sales: Morena Scudieri</li> <li>Phone: +39 366 777 6512</li> <li>Email: m.scudieri@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name:</li> <li>Email:</li> <li>Phone: +39</li> </ul>"},{"location":"customers/atelier-C2392/AWS/","title":"Atelier su AWS CR230143","text":""},{"location":"customers/atelier-C2392/AWS/#data-center","title":"Data center","text":"<ul> <li> <p> AWS: atelier-pavidas</p> </li> <li> <p> AWS: atelier-polosoftware</p> </li> </ul> Text Only<pre><code>Staging Account\nAccount Alias: atelier-cloud-pavidas\nAccount ID: 641845516403\n\nProduction Account\nAccount Alias: atelier-cloud-polosoftware\nAccount ID: 390962764411\n</code></pre>"},{"location":"customers/atelier-C2392/AWS/#architecture","title":"Architecture","text":"<ul> <li>Documento consegnata</li> </ul>"},{"location":"customers/atelier-C2392/AWS/#dns","title":"DNS","text":"<p>DNS \u00e8 gestito dai clienti dell'Atelier.</p> <p>Quando il DNS non supporta il puntamento del dominio root su CNAME (in questo caso, endpoint del cloudfront),il dominio root (esempio pavidas.com) punta all'istanza EC2 che ha un reindirizzamento nel livello dell'applicazione all'indirizzo www (esempio www.pavidas.com). Con ci\u00f2 la chiamata torna ai domini e quindi al cloudfront.</p> <p>Reindirizzamento effettuato dall'Atelier all'interno di EC2:</p> Text Only<pre><code>RewriteCond %{HTTP_HOST} !^www\\.\n  RewriteRule ^(.*)$ http://www.%{HTTP_HOST}/$1 [R=301,L]\n\nRewriteCond %{HTTPS} off\n  RewriteRule (.*) https://%{HTTP_HOST}%{REQUEST_URI} [R,L]\n</code></pre>"},{"location":"customers/atelier-C2392/AWS/#cloudfront-behaviors-e-cache-policy","title":"CloudFront Behaviors e Cache Policy","text":"<p>Atelier ha richiesto di utilizzare la stessa configurazione di behaviors e Cache Policy di Franzese. Con ci\u00f2, sono state create due policy personalizzate: all-headers e host-header-policy.</p>"},{"location":"customers/atelier-C2392/AWS/#atelier-pavidas","title":"Atelier Pavidas","text":""},{"location":"customers/atelier-C2392/AWS/#atelier-polosoftware","title":"Atelier Polosoftware","text":"<p> Ogni sito ha un'istanza EC2 e un bucket S3 che contiene le immagini del sito.</p>"},{"location":"customers/atelier-C2392/AWS/#ec2-reservations","title":"EC2 Reservations","text":"<p>Il controllo delle reserves EC2 di Atelier viene effettuato utilizzando i siti Web riportati di seguito:</p> <p>Visione ristretta del cliente Vista completa Visione per account</p>"},{"location":"customers/atlantica-C2058/","title":"Atlantica Digital SpA @ C2058","text":""},{"location":"customers/atlantica-C2058/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>~~CR200804: Gestione delle VM in produzione~~ disdetto!</li> </ul>"},{"location":"customers/atlantica-C2058/#atlantica","title":"Atlantica","text":"<p>Internal data:</p> <ul> <li>Sales: Vittorino Aprile</li> <li>Phone: +39 392 912 2831</li> <li>Email: v.aprile@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Daniele Valenti</li> <li>Email: daniele.valenti@atlantica.it</li> <li>Phone: +39 328 7228663</li> </ul>"},{"location":"customers/atlantica-C2058/#vcenter","title":"vCenter","text":"<ul> <li> RZ1 Linux: https://vc6-is3.rz1.vdc.ccws.it</li> <li> RZ1 WIN://vc5-is3.rz1.vdc.ccws.it</li> </ul>"},{"location":"customers/atlantica-C2058/#agent-installation-script","title":"Agent installation script","text":""},{"location":"customers/atlantica-C2058/#centos","title":"CentOS","text":"Bash<pre><code>cat /etc/redhat-release\n\n# CentOS 8\nrpm -Uvh https://repo.zabbix.com/zabbix/5.0/rhel/8/x86_64/zabbix-release-5.0-1.el8.noarch.rpm\ndnf clean all\ndnf install zabbix-agent\n\n# CentOS 7\nrpm -Uvh https://repo.zabbix.com/zabbix/5.0/rhel/7/x86_64/zabbix-release-5.0-1.el7.noarch.rpm\nyum install zabbix-agent\n\n## All\n# Alternative IP 10.10.90.65\nsed -i \"s/Server=127.0.0.1/Server=10.167.201.65/\" /etc/zabbix/zabbix_agentd.conf\nsed -i \"s/ServerActive=127.0.0.1/ServerActive=10.167.201.65/\" /etc/zabbix/zabbix_agentd.conf\n\nsystemctl restart zabbix-agent\nsystemctl enable zabbix-agent\n\n## If the VM firewall is enabled\nfirewall-cmd --permanent --add-port=10050/tcp\nfirewall-cmd --reload\n</code></pre>"},{"location":"customers/atlantica-C2058/#windows","title":"Windows","text":"<ol> <li>Download: https://cdn.zabbix.com/zabbix/binaries/stable/5.0/5.0.9/zabbix_agent2-5.0.9-windows-amd64-openssl.msi</li> <li>Configure Server and Server Active: <code>10.167.201.65</code> or <code>10.10.90.65</code></li> </ol>"},{"location":"customers/atlantica-C2058/vpn/","title":"VPN atlantica","text":""},{"location":"customers/atlantica-C2058/vpn/#remote-access","title":"Remote access","text":"<ul> <li>Connection Name: Altantica</li> <li>Remote Gateway: 213.61.203.142</li> <li> <p>Pre shared key: credentials</p> </li> <li> <p>Advanced Settings &gt; Phase 1   Local ID: Commerciale</p> </li> </ul> <p>User: a.sosso_adm Password: credentials</p>"},{"location":"customers/balocco-C2402/","title":"Balocco Spa @ C2402","text":""},{"location":"customers/balocco-C2402/#ito-h24","title":"ITO H24","text":"<ul> <li>CR230225: Servizio di backup da Veeam on-premise su Amazon S3</li> </ul> <p>Internal data:</p> <ul> <li>Sales: Morena Scudieri</li> <li>Phone: +39 366 777 6512</li> <li>Email: m.scudieri@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Fabio Bozzolo</li> <li>Email: fbozzolo@balocco.it</li> <li>Phone: +39 0172 653411</li> </ul>"},{"location":"customers/balocco-C2402/Backup/","title":"CR230225: Backup su Amazon S3","text":"<p>Backup con Veeam su bucket S3</p>"},{"location":"customers/betacom-C0381/","title":"Betacom Srl @ C0381","text":""},{"location":"customers/betacom-C0381/BENE/","title":"Bene assicurazioni","text":""},{"location":"customers/betacom-C0381/BENE/#account-aws","title":"Account AWS","text":"Text Only<pre><code>Account: 232461306709\nAlias: NOT_SET\n</code></pre> <ul> <li>Credentials</li> </ul>"},{"location":"customers/brandsdistribution-C2047/","title":"IDT S.p.A. - Brandsdistribution @ C2047","text":""},{"location":"customers/brandsdistribution-C2047/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>~~CR200652: Account AWS 374031459967 (www.mediabd.it, media.bdroppy.com, appsrv e appsrv02)~~ disdetto!</li> </ul> <p>Internal data:</p> <ul> <li>Sales: Silvano Griot</li> <li>Phone: +39 335 142 6087</li> <li>Email: s.griot@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Ana Lublin</li> <li>Email: a.lublin@brandsdistribution.com</li> <li>Phone: +39 331 5241014</li> </ul>"},{"location":"customers/brandsdistribution-C2047/#aws","title":"AWS","text":"<ul> <li>Controllare Alerta per allarmi derivanti da Cloudwatch Credenziali</li> </ul>"},{"location":"customers/brandsdistribution-C2047/#account-principale","title":"Account principale","text":"Text Only<pre><code>Account: 374031459967\nAlias: brandsdistribution\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/brandsdistribution-C2047/#note","title":"Note","text":"<ul> <li>L'account sotto ITO \u00e8 monitorato da Cloudtrail che invia i logs ad S3, Logstash On-Premise recupera questi dati e li mostra a [Kibana]</li> </ul>"},{"location":"customers/brandsdistribution-C2047/#account-gestito-dal-cliente","title":"Account gestito dal cliente","text":"<p>Questo account non \u00e8 coperto dal servizio ITO</p> Text Only<pre><code>Account: 391155834396\nAlias: brandsdistribution-lightsail\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/brandsdistribution-C2047/#gitops","title":"GitOps","text":"<ul> <li>Repo: https://github.com/criticalcase/brandsdistribution</li> <li>Terraform: https://github.com/criticalcase/brandsdistribution/tree/main/terraform</li> <li>Ansible: https://github.com/criticalcase/brandsdistribution/tree/main/ansible</li> </ul>"},{"location":"customers/brandsdistribution-C2047/#drawio","title":"Drawio","text":"<p>Al Waf \u00e8 associato un Kinesis che ha il compito di inviare i logs al Bucket S3. On-Premise Logstash recupera questi logs e li invia al nostro Kibana(Ricordarsi di selezionare l'host corretto)</p> <p></p>"},{"location":"customers/brandsdistribution-C2047/#certificati","title":"Certificati","text":"<p>I certificati wildcard *.bdroppy.com vengono forniti da Criticalcase (servizio). Noi dobbiamo solo copiare il crt e ca e sostituirli a tutti i siti all'interno di nginx, sia sul server appsrv che appsrv02 anche se il DNS non punta al server in quanto ogni tanto possono fare lo switch del DNS per cambiare server, esempio: /etc/nginx/sites-available/admin.bdroppy.com I certificati sono nella cartella: /etc/ssl/certs/bdroppy/</p>"},{"location":"customers/buccellati-C2411/","title":"Buccellati Holding Italia SpA @ C2411","text":""},{"location":"customers/buccellati-C2411/#ito-h24","title":"ITO H24","text":"<ul> <li>CR230248: Buccellati e-commerce in Cina - Magento2.4 www.buccellati.com.cn</li> </ul> <p>Internal data:</p> <ul> <li>Sales: Morena Scudieri</li> <li>Phone: +39 366 777 6512</li> <li>Email: m.scudieri@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Chiara Ronzoni</li> <li>Phone: +39 340 162 5361</li> <li>Email: chiara.ronzoni@buccellati.com</li> </ul> <p>Customer's Alarms: Email: xxx@xxx.xx</p> <p>CDN SPOC:</p> <p>Tel: (H24/7 365 day) +39.0110888767 Email: performance_support@criticalcase.com</p> <p>Is the primary point of contact in case of blocking issues in the production environment during non working hours or to escalate priority of an issue. Level1 support might ask to submit the issue also by email for better management and tracking. The Email address is the standard point of contact in case of non-blocking issues during non working hours. In case an assigned engineer would be available it is strongly suggested to add her/his email address cc\u2019ed in the loop.</p>"},{"location":"customers/buccellati-C2411/China_e-commerce/","title":"Buccellati E-commerce China","text":""},{"location":"customers/buccellati-C2411/China_e-commerce/#data-center","title":"Data center","text":"<ul> <li> Tencent: buccellati</li> </ul> Text Only<pre><code>Account Alias: buccellati\nAccount ID: 200030041042\nRoot account: tencent-buccellati@criticalcase.com\n</code></pre>"},{"location":"customers/buccellati-C2411/China_e-commerce/#teleport-how-to-access-to-virtual-machines","title":"Teleport (How to access to Virtual Machines)","text":"<p>The VMs are accessible via the following link:</p> <ul> <li>https://tp.buccellati.com.cn</li> </ul> <p>and the following credentials:</p> <ul> <li> <p>https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/show/ds/customers/buccellati-C2411/tp-prod</p> </li> <li> <p>Mysql access through Teleport</p> </li> </ul> Text Only<pre><code>### Login to Teleport\ntsh login --proxy=tp.buccellati.com.cn --auth=local --user=gianluca.gambino@jakala.com\n\n### Make a tunnel with port 3306 on test-magento vm\ntsh ssh -L 3306:test-mysql.test.buccellati.local:3306 buccellati@test-magento\n\n### Open new term (leave the previous open) and connect to localhost (the tunnel)\nmysql --host=127.0.0.1 --port=3306 -u \"buccellati_test\" -p\n</code></pre>"},{"location":"customers/buccellati-C2411/China_e-commerce/#architecture","title":"Architecture","text":""},{"location":"customers/buccellati-C2411/China_e-commerce/#notes","title":"Notes","text":"<ul> <li>DNS and certificates are managed by the customer</li> <li>Credentials are under Sherlock</li> <li>The project is deployed via Terraform (Digger) and Ansible</li> <li>The code is on the GitHub</li> <li>We use Rundeck to shut down test instances in the evening from 8 p.m. to 8 a.m. and on weekends</li> <li>On all servers emergency parameters for php8.1-fpm added in the /etc/php/8.1/fpm/php-fpm.conf file</li> </ul> Text Only<pre><code>emergency_restart_threshold = 3\nemergency_restart_interval = 1m\nprocess_control_timeout = 10s\n</code></pre>"},{"location":"customers/buccellati-C2411/China_e-commerce/#network","title":"Network","text":"<p>VPC: https://console.tencentcloud.com/vpc/vpc?rid=4</p> <p>It is based on 3 VPCs. The first VPC is the bastion VPC, which is connected via peering to the Prod and Test VPCs. The Prod and Test VPCs do not see each other. Security groups are linked to each individual vm or PaaS resource. Prod and Test VPCs go out through their respective Nat Gateways, the bastion has public ip. The bastion is stand alone Teleport Server for access to VMs and MySql. ~~with additional iptables rules for access to mysql-prod and mysql-test. The access is permitted, via security gorup of the bastion VM, only to Criticalcase and Jakala fixed ip addresses 151.11.216.242. The iptables script are located under /etc/network/if-up.d/pf and automated by /etc/networkd-dispatcher/routable.d/50-ifup-hooks script (now using teleport ssh tunnel, see Teleport section)~~.</p> <p>VPCs: | ID | Name | CIDR | |-------------- |--------- |----------------- | | vpc-idmcedk6 | prod | 10.100.0.0/16 | | vpc-pxvwicmk | test | 10.10.0.0/16 | | vpc-fgaxm7ze | bastion | 10.0.0.0/16 |</p> <p>Subnets: | ID | Name | Network | CIDR | Availability zone | |----------------- |--------------- |-------------- |--------------- |------------------- | | subnet-dsu13z27 | subnet-zone-8 | vpc-idmcedk6 | 10.100.8.0/24 | ap-shanghai-8 | | subnet-nvjh3apn | subnet-zone-5 | vpc-idmcedk6 | 10.100.5.0/24 | ap-shanghai-5 | | subnet-nzkw3w6r | subnet-zone-4 | vpc-idmcedk6 | 10.100.4.0/24 | ap-shanghai-4 | | subnet-pq5ueij5 | subnet-zone-4 | vpc-pxvwicmk | 10.10.4.0/24 | ap-shanghai-4 | | subnet-g3p9bil7 | subnet-zone-4 | vpc-fgaxm7ze | 10.0.4.0/24 | ap-shanghai-4 |</p>"},{"location":"customers/buccellati-C2411/China_e-commerce/#test","title":"Test","text":"<p>https://www.test.buccellati.com.cn/</p> <p></p> <p>The test environment is located on single az (Shanghai 4). There is a test.buccellati.local private DNS zone with the following records:</p> Type Host Value A test-cfs 10.10.4.21 A test-redis 10.10.4.7 A test-search 10.10.4.30 A test-magento 10.10.4.20 A test-mysql 10.10.4.22 <p>The test environment includes two Standard S6 instances with 2 Cpu and 8GB of Ram and 30GB of disk, a MySql Paas, an NFS (CFS) Paas, and a Load Balancer (CLB) PaaS.</p> <p>The test-search instance, hosts an rke-2 cluster, which hosts an Elastisearch pod. The ingress is NodePort type on tcp port 30002. The persistent volume is on local disk.</p> <p>The test-magento instance, hosts all the middleware needed to run Magento 2.4.5-p2 (https://experienceleague.adobe.com/docs/commerce-operations/installation-guide/system-requirements.html?lang=en) except for the services offered via PaaS such as Redis, Mysql. Specifically, Apache 2.4.41, php-fpm 8.1.17 and related modules are installed. In addition, through docker-ce, there is an instance of Varnish 7.1.2 which receives requests from the CLB and forwards them, if not cached, to Apache. The docker is configured with fixed network and ip address for permit to configure correctly remoteip mod for Apache. The ip of the client is necessary for Magento ip filter maintanance page. The ip of the client is not store by Apache log as per customer request. Finally, a HAProxy is installed listening on localhost and port 9200, which forwards requests to the rke-2 cluster on port 30002, then to Elasticsearch ver. 7.17.9 pod. This allows the Elastcsearch section of Magento to be configured to the HAProxy. The root of the site is located under /var/www/magento/pub. Test Magento is protected by .htaccess file in web root directory and managed by deploy.sh script. The .htaccess file also contains the redirect rule to prevent access to certain paths in Magento.</p> <p>Private MySql is a single Basic instance with 2 CPUs, 8GB Ram and 100GB storage, hosts MySql version 8. The test database is test_buccellati.</p> <p>The private Redis is a single Memory Edition instance with 2GB Ram, hosts Redis version 6.2, and is used to maintain php sessions and for Magento.</p> <p>The public CLB is configured to redirect http traffic to https, for this purpose it hosts certificate (bundle) and private key for the domain www.test.buccellati.com.cn. The healt check is performed on the path /health_check.php and follows the following rules:</p> Text Only<pre><code>clb_health_check_interval_time = 30\nclb_health_check_health_num = 3\nclb_health_check_unhealth_num = 3\nclb_health_check_time_out = 30\nclb_health_check_http_code = 6\n</code></pre> <p>The CLB forward the requests to the Magento node on port 8080 where Varnish is listening. If the request is not in cache it is forwarded to local Apache on port 80, otherwise Varnish responds directly.</p> <p>The CFS is mounted under the /mnt/nfs/media folder and linked to /var/www/magento/pub/media.</p> <p>In the /var/www folder there is the <code>deploy_fe.sh</code> script. This script is used by the web agency (Jakala) to deploy Magento. The script is responsible for downloading from the China Cloud Object Storage (swrepo-1317209187) the Magento package in <code>/var/www/releases/[build_date_time]</code>. In this folder, the script performs a series of operations via <code>bin/magento</code> and then symlinks the /var/www/magento folder to this release. The script, by default, downloads the database to the same build folder. The Magento package is created and uploaded to the Cloud Object Storage (eu-swrepo-1317209187) by the <code>deploy.sh</code> script located in the buccellati user's home directory of the Frankfurt VM . On the Frankfurt VM, the script performs only the <code>git clone</code> and <code>composer install</code> operations. The Chinese Object Storage is synchronized with the Frankfurt Object Storage and both have a lifecycle policy to keep old versions of files and folders up to 7 days. Finally, there is a cron for buccellati user, the user with that php runs, which takes care of maintaining the 6 most recent releases and deleting the older ones. This script, also add to /var/www/magento/pub/.htaccess a set of redirect rule to protect some path and a commented Basic Auth section.</p>"},{"location":"customers/buccellati-C2411/China_e-commerce/#prod","title":"Prod","text":"<p>www.buccellati.com.cn</p> <p></p> <p>The production environment is located on 3 different az: Shanghai 4,5 and 8. PaaS services such as Redis and Mysql are MultiAZ. Unlike the test environment, the production environment is accelerated by the Quantil CDN and protected by the Tencent WAF connected to the CLB.</p> <p>There is a prod.buccellati.local private DNS zone with the following records:</p> Type Host Value A prod-magento-4 10.100.4.20 A prod-magento-5 10.100.5.20 A prod-magento-admin-4 10.100.4.10 A prod-search-8 10.100.8.30 A prod-search-5 10.100.5.30 A prod-search-4 10.100.4.30 A prod-redis 10.100.4.3 A prod-mysql 10.100.4.24 <p>The prod environment includes six Standard S6 instances with 2 Cpu and 8GB of Ram and 30GB of system disk, a MySql Paas and a Load Balancer (CLB) PaaS. The three instances of rke-2 also have a 100GB data disk.</p> <p>The prod-search instances, hosts an rke-2 cluster, which hosts an 3 Elastisearch pod, one for each node. The ingress for each node is NodePort type on tcp port 30002. The persistent volume is on local disk on each node. The prod-search instances, also host a glusterfs cluster, which provides the \"data\" volume. This volume is mounted under the /mnt/nfs/media directory of the 3 Magento nodes.</p> <p>The prod-magento instance, hosts all the middleware needed to run Magento 2.4.5-p2 (https://experienceleague.adobe.com/docs/commerce-operations/installation-guide/system-requirements.html?lang=en) except for the services offered via PaaS such as Redis, Mysql. Specifically, Apache 2.4.41, php-fpm 8.1.17 and related modules are installed. In addition, through docker-ce, there is an instance of Varnish 7.1.2 which receives requests from the CLB and forwards them, if not cached, to Apache. The docker is configured with fixed network and ip address for permit to configure correctly remoteip mod for Apache. The ip of the client is necessary for Magento ip filter maintanance page. The ip of the client is not store by Apache log as per customer request. Finally, a HAProxy is installed listening on localhost and port 9200 for each Magento node, which forwards requests to the node of rke-2 cluster on port 30002 available in the same subnet of Magento node, then the request is forward to one of three Elasticsearch ver. 7.17.9 pod. This allows the Elastcsearch section of Magento to be configured to the HAProxy. The root of the site is located under /var/www/magento/pub. Magento has an .htaccess file in web root directory and managed by deploy.sh script. The .htaccess file contains the redirect rule to prevent access to certain paths in Magento.</p> <p>Private MySql is a Two Node (Master/Slave wit semy-sinc replica) General instance with 4 CPUs, 16B Ram and 200GB storage, hosts MySql version 8. The Master node is on Shanghai 4 zone, the replica node is on Shanghai 5 zone. The prod database is prod_buccellati.</p> <p>The private Redis is a MultiAZ Memory Edition instance with 2GB Ram, hosts Redis version 6.2, and is used to maintain php sessions and for Magento. The Master node is on Shanghai 4 zone, the replica node is on Shanghai 5 zone.</p> <p>The public CLB is configured to redirect http traffic to https, for this purpose it hosts certificate (bundle) and private key for the domain origin.buccellati.com.cn. In fact, the domain www.buccellati.com.cn is hosted on the CDN, which forwards requests by changing the Host header to origin.buccellati.com.cn then forward the request to the CLB. The certificate for the domain www.buccellati.com.cn is also hosted on the CDN. The healt check is performed on the path /health_check.php and follows the following rules:</p> Text Only<pre><code>clb_health_check_interval_time = 30\nclb_health_check_health_num = 3\nclb_health_check_unhealth_num = 3\nclb_health_check_time_out = 30\nclb_health_check_http_code = 6\n</code></pre> <p>The CLB forwards the requests to the connected WAF which, if the requests pass all checks, returns them to the CLB, they are then turned over to one of the two Magento nodes (prod-magento-4 and prod-magento-5) on port 8080 where Varnish is listening. If the request is not in cache it is forwarded to local Apache on port 80, otherwise Varnish responds directly. If the request is directed to the /bucc1919admin_china path, the request is forward to the prod-magento-admin node and then to the Varnish and Apache like the normal www requests.</p> <p>The glusterfs volume is mounted under the /mnt/nfs/media folder and linked to /var/www/magento/pub/media on the two Magento front end nodes and Magento admin node.</p> <p>The prod-magento-admin-4, is used for all Magento configuration, update and deploy operations. Nothing is and should be done on the two front end nodes that are updated by the prod-magento-admin-4 via the the <code>deploy.sh</code> script. The prod-magento-admin-4 is the only one that contains the Magento crons.</p> <p>In the /var/www folder on the prod-magento-admin-4 there is the <code>deploy_fe.sh</code> script. This script is used by the web agency (Jakala) to deploy Magento. The script is responsible for downloading from the China Cloud Object Storage (swrepo-1317209187) the Magento package in <code>/var/www/releases/[build_date_time]</code>. In this folder, the script performs a series of operations via <code>bin/magento</code> and then symlinks the /var/www/magento folder to this release. Also the script takes care of aligning via rsync, the two front-end Magento nodes. The script, by default, downloads the database to the same build folder. The Magento package is created and uploaded to the Cloud Object Storage (eu-swrepo-1317209187) by the <code>deploy.sh</code> script located in the buccellati user's home directory of the Frankfurt VM . On the Frankfurt VM, the script performs only the <code>git clone</code> and <code>composer install</code> operations. The Chinese Object Storage is synchronized with the Frankfurt Object Storage and both have a lifecycle policy to keep old versions of files and folders up to 7 days. Finally, there is a cron on all Magento nodes for buccellati user, the user with that php runs, which takes care of maintaining the 6 most recent releases and deleting the older ones. This script, also add to /var/www/magento/pub/.htaccess a set of redirect rule to protect some path and a commented Basic Auth section.</p>"},{"location":"customers/buccellati-C2411/China_e-commerce/#backup","title":"Backup","text":"<ul> <li>Mysql: https://console.tencentcloud.com/mysql/backup/index</li> <li>CFS: https://console.tencentcloud.com/cfs/snapshot/list?rid=4</li> <li>CBS: https://console.tencentcloud.com/cvm/cbs/index?rid=4</li> <li>Redis: https://console.tencentcloud.com/redis/backup#/</li> </ul>"},{"location":"customers/buccellati-C2411/China_e-commerce/#waf","title":"WAF","text":"<p>WAF (Premium) is connet to CLB domain name listeners. It's used only in production enviroment. CLB receive the requests from CDN then forward to WAF. WAF check the requests and if respect all checks, WAF forward to CLB again otherwise the requests are blocked and blocked page are sent to the client. WAF pass the requests trough the following categories of Basic secutity section (https://console.tencentcloud.com/guanjia/tea-baseconfig):</p> <ul> <li>Web security: contains the preconfigured OSWAP rules, in this section it is possible to enabled/disabled the rule(s), the defense level and the defense mode (block/observe). We use a default configuration.</li> <li>Access control: contains the rule(s) to check various thinks. For example check the headers, then check if the headers is present or has a specific value, then block or allow the request. We have one rule to check if is present the X-CC-Forwaderd-For (an header added by cdn) and block all the request without this header (not pass from cdn).</li> <li>CC protection: contains a rule(s) for traffic management. We use a single rule that block for one minute an ip that made over 500 request in one minute. Here you can find blocked ip: https://console.tencentcloud.com/guanjia/tea-iplist</li> </ul>"},{"location":"customers/buccellati-C2411/China_e-commerce/#cos-and-deploy-env","title":"Cos and Deploy env","text":"<p>The dev enviroment consist in one VM with Public IP 43.131.61.63. The access is made by ssh and ssh keys, so Fail2ban is isnstalled for blocking ssh brute force attack. The script <code>deploy.sh</code> in the buccellati home directory, is used to generate by git, from Jakala bitbucket repository, composer, tar and bzip a Magento Package for test and prod env. This package is uploaded to Frankfurt Cloud Object Storage and replicate to China Cloud Object Storage This package is downloaded from China Cloud Object Storage, extract and installed by deploy_fe.sh script on <code>/var/www/</code> in test-magento and prod-magento-admin-4 that also replicates on the front-end prod nodes. The 2 Cloud Object Storage, are one in Frankfurt <code>eu-swrepo-1317209187</code> and one in Shanghai <code>swrepo-1317209187</code>. The buckets are used to transfer large files quickly and without censored applied by Chinese Great Firewall, in fact the Frankfurt bucket replicates all its contents in the Shanghai one, in this way the traffic passes into the internal Tencent network effectively bypassing the Chinese Great Firewall. In addition, the buckets are used by Criticalcase to update teleport quickly. Buckets have automatic cleaning jobs for older version of objects (7 day retention). Scripts uses the coscli utility to operate on them.</p>"},{"location":"customers/buccellati-C2411/China_e-commerce/#operations","title":"Operations","text":""},{"location":"customers/buccellati-C2411/China_e-commerce/#certificates","title":"Certificates","text":"<p>Certificates are provided and updated by the client. Four certificates are present:</p> <ul> <li>tp.buccellati.com.cn, is used for bastion Teleport.   The certificate and private key are located under /etc/teleport-ssl/tp_buccellati_com_cn_393797820. Installation is done by hand.</li> <li>www.test.buccellati.com.cn, is used for the test site and installed on the load balancer. The certificate and private key are managed via Terraform and Secret Manager -&gt; Parameter Store.</li> <li>origin.buccellati.com.cn, is used for the production site and installed on the load balancer. The certificate and private key are managed through Terraform and Secret Manager -&gt; Parameter Store.</li> <li>www.buccellati.com.cn, is used for the production site, is installed on the CDN. Installation and configuration is managed from Roasio (r.roasio@criticalcase.com).</li> </ul>"},{"location":"customers/buccellati-C2411/China_e-commerce/#purge-varnish-cache","title":"Purge Varnish cache","text":"<p>On all Magento frontend, under /home/buccellati/, there is a purge_varnish_cache.sh script. Run the script on the fornted that needs to clear the cache. This script can also be used by the customer independently.</p>"},{"location":"customers/buccellati-C2411/China_e-commerce/#purge-cdn-cache","title":"Purge CDN cache","text":"<p>On prod-magento-admin-4 frontend, under /home/buccellati/, there is a purge_cdn_domain.sh script. Run the script to clear the CDN cache. This script can also be used by the customer independently.</p>"},{"location":"customers/buccellati-C2411/China_e-commerce/#enabledisable-waf","title":"Enable/Disable WAF","text":"<p>Go to: https://console.tencentcloud.com/guanjia/tea-domain and Activate/Deactivate the WAF switch. Also you can switch the Waf Mode to Observe Mode (alert whitout block) here: https://console.tencentcloud.com/guanjia/tea-baseconfig</p>"},{"location":"customers/buccellati-C2411/China_e-commerce/#add-waf-exclusion-view-attack-logs","title":"Add WAF Exclusion / View Attack Logs","text":"<p>If the WAF blocks legitimate requests, the customer should report the UUID number. The UUID number is shown to the client on the page that WAF provides when the request is blocked. Otherwise you can try searching the log, doing searches by url, ip, etc. but it is more complex. With this number it is possible to perform a search in the attack log.</p> <p></p> <p>Attack log can be found here: https://console.intl.cloud.tencent.com/guanjia/tea-attacklog. Near the result, ther is a link for creating a False positive correction</p> <p></p> <p>Click on it and then add the rule to allowlist </p> <p>The Block/Allow list can be found and management here: https://console.intl.cloud.tencent.com/guanjia/tea-iplist</p>"},{"location":"customers/buccellati-C2411/China_e-commerce/#extend-disk","title":"Extend disk","text":"<p>See: ExtenddiskonTencent</p>"},{"location":"customers/buffetti-C0324/","title":"Gruppo Buffetti S.p.A. @ C0324","text":""},{"location":"customers/buffetti-C0324/#ito-h24","title":"ITO H24","text":"<ul> <li>CR230128: B2B su AWS (b2b.buffetti.it)</li> <li>CR210421: B2C su AWS (www.buffetti.it)</li> </ul> <p>Internal data:</p> <ul> <li>Sales: Marina Micelli</li> <li>Phone: +39 337 155 4741</li> <li>Email: m.micelli@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Alessandro Monfregola</li> <li>Email: a.monfregola@buffetti.it</li> <li>Phone: +39 338 946 1298</li> </ul>"},{"location":"customers/buffetti-C0324/B2B-AWS/","title":"Buffetti B2B","text":"<p>Buffetti b2b.buffetti.it - CR230128</p>"},{"location":"customers/buffetti-C0324/B2B-AWS/#data-center","title":"Data center","text":"<ul> <li> AWS eu-central-1: buffetti</li> </ul> Text Only<pre><code>Account Alias: buffetti\nAccount ID: 058264218747\n</code></pre>"},{"location":"customers/buffetti-C0324/B2B-AWS/#operazioni","title":"Operazioni","text":"<ul> <li>Deploy: deploy del codice di Magento in ambiente di stage e produzione</li> <li>Proftpd: gestione e configurazione di Proftpd sulle macchine di stage e magento-admin-prd</li> </ul>"},{"location":"customers/buffetti-C0324/B2B-AWS/#teleport","title":"Teleport","text":"<p>Access to the VMs is possible via federated teleport cluster:</p> <p>Select the cluster:</p> <ul> <li><code>tp-buffetti-b2b</code></li> </ul>"},{"location":"customers/buffetti-C0324/B2B-AWS/#architettura","title":"Architettura","text":""},{"location":"customers/buffetti-C0324/B2B-AWS/#note","title":"Note","text":"<ul> <li>Il DNS \u00e8 gestito dal cliente</li> <li>I backup sono automatizzati da AWS Backup</li> <li>Non c'\u00e8 filesystem condiviso per motivi di prestazione, vedere sezione Operazioni per il deploy</li> <li>Le credenziali sono sotto Sherlock</li> <li>Essendo in ITO, l'account \u00e8 controllato sia dalle Lambda in cc-digital</li> <li>Le risorse sono sotto monitoring tramite Cloudwatch Alarms e Zabbix</li> <li>L'account sotto ITO \u00e8 monitorato da Cloudtrail che invia i logs ad S3, Logstash On-Premise recupera questi dati e li mostra a Kibana</li> </ul>"},{"location":"customers/buffetti-C0324/B2B-AWS/#network","title":"Network","text":"<p>Esiste un unica VPC. Ci sono 6 subnets private e 2 subnets pubbliche per environment (12 subnets totali). Le subnet pubbliche utilizzano tutte lo stesso Internet Gateway e le private lo stesso Nat Gateway. Non ci sono limiti di raggiungibilit\u00e0 sulle NACL delle subnets, ma \u00e8 gestito tutto dai Security Groups</p>"},{"location":"customers/buffetti-C0324/B2B-AWS/#test","title":"Test","text":"<p>L'ambiente di test ha un'architettura differente rispetto agli altri per ridurre i costi. Tutti i servizi che dovrebbero essere quelli di AWS (ElasticSearch,RDS,ecc) sono dei container all'interno della stesso Server. Esiste un Cloudfront che fa da cache per i contenuti statici</p>"},{"location":"customers/buffetti-C0324/B2B-AWS/#stg","title":"Stg","text":"<p>L'ambiente di stage \u00e8 speculare a quello di produzione, ma singola az. Tutto quello che era possibile criptare on at Rest \u00e8 stato fatto con KMS o chiavi di default di AWS, mantere questa convenzione. Esiste un Cloudfront che inoltre le richieste al bilanciatore. Esiste un Custom Header su Critical0 per far si che il bilanciatore accetti il traffico solamente dal Cloudfront e non direttamente (il traffico tra di essi \u00e8 cifrato). Al Cloufront \u00e8 associato un WAF al quale sono associate regole di rate-limit, ip permessi e ip malevoli (aggiornati ogni ora dalle nostre lambda in cc-digital). Al Waf \u00e8 associato un Kinesis che ha il compito di inviare i logs al Bucket S3. On-Premise Logstash recupera questi logs e li invia al nostro Kibana(Ricordarsi di selezionare l'host corretto). Il database \u00e8 su RDS. Esiste un Redis(non c'\u00e8 https, ma la sicurezza \u00e8 garantita a livello di networking concordato con il cliente) e un OpenSearch. Il DNS interno \u00e8 gestito da Route53. L'EFS \u00e8 montato dal file /etc/fstab:</p> Text Only<pre><code>LABEL=cloudimg-rootfs   /        ext4   discard,errors=remount-ro       0 1\nLABEL=UEFI      /boot/efi       vfat    umask=0077      0 1\nefs.b2b-stg.internal /mnt/b2b-nfs efs _netdev,noresvport,tls 0 0\n</code></pre> <p>I log vengono centralizzati su CloudWatch Logs tramite un agent, la configurazione si trova sotto /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/file_config.json</p> JSON<pre><code>{\n  \"agent\": {\n    \"run_as_user\": \"root\"\n  },\n  \"logs\": {\n    \"logs_collected\": {\n      \"files\": {\n        \"collect_list\": [\n          {\n            \"file_path\": \"/var/log/syslog\",\n            \"log_group_name\": \"/aws/ec2/magento-stg-syslog\",\n            \"log_stream_name\": \"magento-stg\"\n          },\n          {\n            \"file_path\": \"/var/log/php8.1-fpm.log\",\n            \"log_group_name\": \"/aws/ec2/magento-stg-phpfpm-logs\",\n            \"log_stream_name\": \"magento-stg\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/cron.log\",\n            \"log_group_name\": \"/aws/ec2/magento-stg-magento-cron\",\n            \"log_stream_name\": \"magento-stg\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/debug.log\",\n            \"log_group_name\": \"/aws/ec2/magento-stg-magento-debug\",\n            \"log_stream_name\": \"magento-stg\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/exception.log\",\n            \"log_group_name\": \"/aws/ec2/magento-stg-magento-exception\",\n            \"log_stream_name\": \"magento-stg\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/system.log\",\n            \"log_group_name\": \"/aws/ec2/magento-stg-magento-system\",\n            \"log_stream_name\": \"magento-stg\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/magento.cron.log\",\n            \"log_group_name\": \"/aws/ec2/magento-stg-magento-cron\",\n            \"log_stream_name\": \"magento-stg\"\n          }\n        ]\n      }\n    }\n  }\n}\n</code></pre> <p>Sulla macchine \u00e8 installato Nginx sulla porta 80 riceve le chiamate da Varnish in docker, porta 8080. Il codice si trova sulla macchina stessa, la fase di deploy viene fatta dall'EC2 Admin, vedere il paragrafo Operations per il funzionamento</p>"},{"location":"customers/buffetti-C0324/B2B-AWS/#production","title":"Production","text":"<p>L'ambiente di produzione \u00e8 stato creato singola az. Tutto quello che era possibile criptare on at Rest \u00e8 stato fatto con KMS o chiavi di default di AWS, mantere questa convenzione. Esiste un Cloudfront che inoltre le richieste al bilanciatore. Esiste un Custom Header su Critical0 per far si che il bilanciatore accetti il traffico solamente dal Cloudfront e non direttamente (il traffico tra di essi \u00e8 cifrato). Al Cloufront \u00e8 associato un WAF al quale sono associate regole di rate-limit, ip permessi e ip malevoli (aggiornati ogni ora dalle nostre lambda in cc-digital). Al Waf \u00e8 associato un Kinesis che ha il compito di inviare i logs al Bucket S3. On-Premise Logstash recupera questi logs e li invia al nostro Kibana(Ricordarsi di selezionare l'host corretto). Il database \u00e8 su RDS. Esiste un Redis(non c'\u00e8 https, ma la sicurezza \u00e8 garantita a livello di networking concordato con il cliente) e un OpenSearch. Il DNS interno \u00e8 gestito da Route53. L'EFS \u00e8 montato dal file /etc/fstab:</p> Text Only<pre><code>LABEL=cloudimg-rootfs   /        ext4   discard,errors=remount-ro       0 1\nLABEL=UEFI      /boot/efi       vfat    umask=0077      0 1\nefs.b2b-prd.internal /mnt/b2b-nfs efs _netdev,noresvport,tls 0 0\n</code></pre> <p>I log vengono centralizzati su CloudWatch Logs tramite un agent, la configurazione si trova sotto /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/file_config.json</p>"},{"location":"customers/buffetti-C0324/B2B-AWS/#server-admin","title":"Server Admin","text":"JSON<pre><code>{\n  \"agent\": {\n    \"run_as_user\": \"root\"\n  },\n  \"logs\": {\n    \"logs_collected\": {\n      \"files\": {\n        \"collect_list\": [\n          {\n            \"file_path\": \"/var/log/syslog\",\n            \"log_group_name\": \"/aws/ec2/magento-admin-prd-syslog\",\n            \"log_stream_name\": \"magento-admin-prd\"\n          },\n          {\n            \"file_path\": \"/var/log/php8.1-fpm.log\",\n            \"log_group_name\": \"/aws/ec2/magento-admin-prd-phpfpm-logs\",\n            \"log_stream_name\": \"magento-admin-prd\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/cron.log\",\n            \"log_group_name\": \"/aws/ec2/magento-admin-prd-magento-cron\",\n            \"log_stream_name\": \"magento-admin-prd\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/debug.log\",\n            \"log_group_name\": \"/aws/ec2/magento-admin-prd-magento-debug\",\n            \"log_stream_name\": \"magento-admin-prd\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/exception.log\",\n            \"log_group_name\": \"/aws/ec2/magento-admin-prd-magento-exception\",\n            \"log_stream_name\": \"magento-admin-prd\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/system.log\",\n            \"log_group_name\": \"/aws/ec2/magento-admin-prd-magento-system\",\n            \"log_stream_name\": \"magento-admin-prd\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/magento.cron.log\",\n            \"log_group_name\": \"/aws/ec2/magento-admin-prd-magento-cron\",\n            \"log_stream_name\": \"magento-admin-prd\"\n          }\n        ]\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"customers/buffetti-C0324/B2B-AWS/#server-magento-prd-1","title":"Server magento-prd-1","text":"JSON<pre><code>{\n  \"agent\": {\n    \"run_as_user\": \"root\"\n  },\n  \"logs\": {\n    \"logs_collected\": {\n      \"files\": {\n        \"collect_list\": [\n          {\n            \"file_path\": \"/var/log/syslog\",\n            \"log_group_name\": \"/aws/ec2/magento-prd-1-syslog\",\n            \"log_stream_name\": \"magento-prd-1\"\n          },\n          {\n            \"file_path\": \"/var/log/php8.1-fpm.log\",\n            \"log_group_name\": \"/aws/ec2/magento-prd-1-phpfpm-logs\",\n            \"log_stream_name\": \"magento-prd-1\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/cron.log\",\n            \"log_group_name\": \"/aws/ec2/magento-prd-1-magento-cron\",\n            \"log_stream_name\": \"magento-prd-1\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/debug.log\",\n            \"log_group_name\": \"/aws/ec2/magento-prd-1-magento-debug\",\n            \"log_stream_name\": \"magento-prd-1\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/exception.log\",\n            \"log_group_name\": \"/aws/ec2/magento-prd-1-magento-exception\",\n            \"log_stream_name\": \"magento-prd-1\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/system.log\",\n            \"log_group_name\": \"/aws/ec2/magento-prd-1-magento-system\",\n            \"log_stream_name\": \"magento-prd-1\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/magento.cron.log\",\n            \"log_group_name\": \"/aws/ec2/magento-prd-1-magento-cron\",\n            \"log_stream_name\": \"magento-prd-1\"\n          }\n        ]\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"customers/buffetti-C0324/B2B-AWS/#server-magento-prd-2","title":"Server magento-prd-2","text":"JSON<pre><code>{\n  \"agent\": {\n    \"run_as_user\": \"root\"\n  },\n  \"logs\": {\n    \"logs_collected\": {\n      \"files\": {\n        \"collect_list\": [\n          {\n            \"file_path\": \"/var/log/syslog\",\n            \"log_group_name\": \"/aws/ec2/magento-prd-2-syslog\",\n            \"log_stream_name\": \"magento-prd-2\"\n          },\n          {\n            \"file_path\": \"/var/log/php8.1-fpm.log\",\n            \"log_group_name\": \"/aws/ec2/magento-prd-2-phpfpm-logs\",\n            \"log_stream_name\": \"magento-prd-2\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/cron.log\",\n            \"log_group_name\": \"/aws/ec2/magento-prd-2-magento-cron\",\n            \"log_stream_name\": \"magento-prd-2\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/debug.log\",\n            \"log_group_name\": \"/aws/ec2/magento-prd-2-magento-debug\",\n            \"log_stream_name\": \"magento-prd-2\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/exception.log\",\n            \"log_group_name\": \"/aws/ec2/magento-prd-2-magento-exception\",\n            \"log_stream_name\": \"magento-prd-2\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/system.log\",\n            \"log_group_name\": \"/aws/ec2/magento-prd-2-magento-system\",\n            \"log_stream_name\": \"magento-prd-2\"\n          },\n          {\n            \"file_path\": \"/var/www/b2b/var/log/magento.cron.log\",\n            \"log_group_name\": \"/aws/ec2/magento-prd-2-magento-cron\",\n            \"log_stream_name\": \"magento-prd-2\"\n          }\n        ]\n      }\n    }\n  }\n}\n</code></pre> <p>Sulla macchine \u00e8 installato Nginx sulla porta 80 riceve le chiamate da Varnish in docker, porta 8080. Il codice si trova sulla macchina stessa, la fase di deploy viene fatta dall'EC2 Admin, vedere il paragrafo Operations per il funzionamento</p>"},{"location":"customers/buffetti-C0324/B2B-AWS/Deploy/","title":"Deploy","text":""},{"location":"customers/buffetti-C0324/B2B-AWS/Deploy/#deploy","title":"Deploy","text":"<p>Il cliente esegue il deploy di Magento nell'ambiente di stage e di produzione attraverso lo script deploy che si trova sotto /var/www/deploy nelle macchine di stage e magento-admin-prd. Il sorgente \u00e8 su https://github.com/criticalcase/c-buffetti-c0324-b2b/blob/main/ansible/files/deploy. Tale script \u00e8 gestito da noi e viene installato/aggiornato tramite ansible. Lo script si occupa di clonare l'ambiente corretto dai repo git del cliente in una directory \"BUILDDIR\", settando delle variabili in base alla macchina sulla quale viene eseguito:</p> Text Only<pre><code>if [ \"$HOSTNAME\" == \"magento-stg\" ] ; then\n    env=stg\n    DBNAME=stageb2b\n    GITBRANCH=test\n    DEPLOYMODE=developer\n    print_info \"hostname is $HOSTNAME env var is $env\"\nelif [ \"$HOSTNAME\" == \"magento-admin-prd\" ] ; then\n    env=prd\n    DBNAME=prodb2b\n    GITBRANCH=main\n    DEPLOYMODE=production\n    print_info \"hostname is $HOSTNAME env var is $env\"\nelse\n    print_error \"Error occourred while setting variables\"\n    exit 1\nfi\n</code></pre> <p>Lo script successivamente si occupa:</p> <ul> <li>esegue il composer install</li> <li>crea dei link simbolici ai files di configurazione di magento</li> <li>rinomina alcune directory che saranno montate via nfs</li> <li>esegue il setup:upgrade</li> <li>rimuove gli static content</li> <li>compila gli static content</li> <li>pulisce la cache di magento</li> <li>mette il sito nella BUILDDIR in maintenance mode</li> <li>mette il sito live in maintenance mode (in produzione lo fa in ssh sui nodi indicati nei parametri di lancio)</li> <li>se indicato nei parametri esegue il dump del db</li> <li>rigenera il link simbolico tra LIVEDIR e la nuova BUILDDIR</li> <li>esegue il reload di php-fpm sulla macchina di deploy</li> <li>rimuove la maintenance mode</li> <li>in produzione esegue un rsync tra la macchina di deploy e i nodi attivi</li> <li>in produzione segue il reload di php-fpm sui nodi attivi</li> </ul>"},{"location":"customers/buffetti-C0324/B2B-AWS/Proftpd/","title":"Proftpd","text":""},{"location":"customers/buffetti-C0324/B2B-AWS/Proftpd/#proftpd","title":"Proftpd","text":"<p>Il cliente aveva la necessit\u00e0 di inviare file di dati verso Magento nella directory /var/www/b2b/var/import. Si \u00e8 quindi installato Proftpd tramite un nostro ruolo ansible: https://github.com/criticalcase/ansible-proftpd. Proftpd viene installato con utenti virtuali, in questo caso l'utente buffetti_import esegue l'autenticazione attraverso la chiave pubblica fornita dal cliente. Come home ha la /var/www/b2b/var/import e come uid e gid eredita gli stessi dell'utente col quale gira il php ovvero prodb2b per produzione e stageb2b per stage. Le porte sono aperte attraverso il Bastion Teleport che gira le richieste rispettivamente alla macchina admin di stage e di produzione sulla porta 60001 dove \u00e8 in ascolto Proftpd:</p> <p>stg server: 18.193.205.97 port: 22222 user: buffetti_import</p> <p>prd server: 18.193.205.97 port: 22223 user: buffetti_import</p> <p>test server: 18.193.205.97 port: 22224 user: buffetti_import</p> <p>Le regole iptables si trovano sulla macchina Bastion sotto /etc/network/if-up.d/pf nel file pf:</p> Text Only<pre><code>#!/bin/sh\n\nif [ \"$IFACE\" = ens5 ]; then\n\n#stg\niptables -I PREROUTING 1 -t nat -i ens5 -p tcp --dport 22222 -j DNAT --to 10.111.34.32:60001\niptables -I FORWARD 1 -p tcp -d 10.111.34.32 --dport 60001 -j ACCEPT\niptables -t nat -I POSTROUTING 1 -o ens5 -p tcp -d 10.111.34.32 --dport 60001 -j SNAT --to 10.111.32.40\n\n#prd\niptables -I PREROUTING 1 -t nat -i ens5 -p tcp --dport 22223 -j DNAT --to 10.111.32.143:60001\niptables -I FORWARD 1 -p tcp -d 10.111.32.143 --dport 60001 -j ACCEPT\niptables -t nat -I POSTROUTING 1 -o ens5 -p tcp -d 10.111.32.143 --dport 60001 -j SNAT --to 10.111.32.40\n\n#test\niptables -I PREROUTING 1 -t nat -i ens5 -p tcp --dport 22224 -j DNAT --to 10.111.35.147:60001\niptables -I FORWARD 1 -p tcp -d 10.111.35.147 --dport 60001 -j ACCEPT\niptables -t nat -I POSTROUTING 1 -o ens5 -p tcp -d 10.111.35.147 --dport 60001 -j SNAT --to 10.111.32.40\n\nfi\n</code></pre> <p>La configurazione ansible si trova:</p> <ul> <li>stg: https://github.com/criticalcase/c-buffetti-c0324-b2b/blob/f94238d1110bbcb7f1869b60ca0430e4c90a8cbd/ansible/group_vars/stg.yaml#L44-L73</li> <li>prd: https://github.com/criticalcase/c-buffetti-c0324-b2b/blob/f94238d1110bbcb7f1869b60ca0430e4c90a8cbd/ansible/group_vars/prd.yaml#L21-L50</li> </ul>"},{"location":"customers/buffetti-C0324/B2C/","title":"Buffetti B2C","text":"<p>Buffetti www.buffetti.it - CR210421</p>"},{"location":"customers/buffetti-C0324/B2C/#data-center","title":"Data center","text":"<ul> <li> AWS eu-central-1: buffetti</li> </ul> Text Only<pre><code>Account Alias: buffetti\nAccount ID: 824252961091\n</code></pre>"},{"location":"customers/buffetti-C0324/B2C/#operazioni","title":"Operazioni","text":"<ul> <li>[Teleport]Teleport.md): Creazione account teleport cliente e accesso al database RDS</li> <li>[Performance Import RDS]PerformanceImportRDS.md): Problema performance Import dati su RDS</li> <li>[Clerk.io 403 access forbidden]ClerkAccessForbidden.md): Problema Clerk.io 403 access forbidden</li> <li>[Setup Magento]SetupMagento.md): Configurazione di Magento all'interno delle EC2</li> <li>[Cancellazione ambiente]CancellazioneAmbiente.md): Cancellazione di un ambiente di stg*</li> </ul>"},{"location":"customers/buffetti-C0324/B2C/#teleport","title":"Teleport","text":"<p>Access to the VMs is possible via federated teleport cluster:</p> <p>Select the cluster:</p> <ul> <li><code>aws-buffetti-b2c</code></li> </ul>"},{"location":"customers/buffetti-C0324/B2C/#architettura","title":"Architettura","text":""},{"location":"customers/buffetti-C0324/B2C/#note","title":"Note","text":"<ul> <li>Il DNS \u00e8 gestito dal cliente</li> <li>I backup sono automatizzati da AWS Backup</li> <li>Non c'\u00e8 filesystem condiviso per motivi di prestazione, vedere sezione OPERATIONS per il deploy</li> <li>Le credenziali sono sotto Sherlock</li> <li>Il progetto \u00e8 deployato tramite Terraform Cloud Prd e Stg. Il codice \u00e8 su GitHub</li> <li>Essendo in ITO, l'account \u00e8 controllato sia dalle Lambda in cc-digital</li> <li>Le risorse sono sotto monitoring tramite Cloudwatch Alarms e Zabbix</li> <li>Controllare Alerta per allarmi derivanti da Cloudwatch Credenziali</li> <li>L'account sotto ITO \u00e8 monitorato da Cloudtrail che invia i logs ad S3, Logstash On-Premise recupera questi dati e li mostra a Kibana</li> <li> <p>Utilizziamo Rundeck per spegnere le istanze di dev e stg nel weekend</p> </li> <li> <p>Su tutti i servers sono stati aggiunti parametri di emergenza per php7.4-fpm aggiunti nel file /etc/php/7.4/fpm/php-fpm.conf</p> </li> </ul> INI<pre><code>emergency_restart_threshold = 3\nemergency_restart_interval = 1m\nprocess_control_timeout = 10s\n</code></pre>"},{"location":"customers/buffetti-C0324/B2C/#network","title":"Network","text":"<p>Esiste un unica VPC. Ci sono 3 subnets private e 3 subnets pubbliche per environment (12 subnets totali con 254 IPs address ciascuno). Le subnet pubbliche utilizzano tutte lo stesso Internet Gateway e le private lo stesso Nat Gateway. Non ci sono limiti di raggiungibilit\u00e0 sulle NACL delle subnets, ma \u00e8 gestito tutto dai Security Groups</p>"},{"location":"customers/buffetti-C0324/B2C/#dev","title":"Dev","text":"<p>La macchina di Dev che fa da cache per i contenuti statici \u00e8 utilizzata come da Buffetti come ambiente di test. Ha un'architettura differente rispetto agli altri ambienti per ridurre i costi. Tutti i servizi che dovrebbero essere quelli di AWS (ElasticSearch,RDS,ecc) sono dei container all'interno della stesso Server. Esiste un Cloudfront che fa da cache per i contenuti statici</p>"},{"location":"customers/buffetti-C0324/B2C/#stg4","title":"Stg4","text":"<p>L'ambiente di stage \u00e8 speculare a quello di produzione, ma singola az. Tutto quello che era possibile criptare on at Rest \u00e8 stato fatto con KMS o chiavi di default di AWS, mantere questa convenzione. Esiste un Cloudfront che fa da cache per i contenuti statici. Esiste un Custom Header per far si che il bilanciatore accetti il traffico solamente dal Cloudfront e non direttamente (il traffico tra di essi \u00e8 cifrato). Al Cloufront \u00e8 associato un WAF al quale sono associate regole di rate-limit, ip permessi e ip malevoli (aggiornati ogni ora dalle nostre lambda in cc-digital). Al Waf \u00e8 associato un Kinesis che ha il compito di inviare i logs al Bucket S3. On-Premise Logstash recupera questi logs e li invia al nostro Kibana(Ricordarsi di selezionare l'host corretto). Il database \u00e8 su RDS. Esiste un Redis(non c'\u00e8 https, ma la sicurezza \u00e8 garantita a livello di networking concordato con il cliente) e un ElasticSearch, quest'ultimo \u00e8 obbligatorio per l'ultima versione di Magento. Il DNS interno \u00e8 gestito da Route53. L'EFS \u00e8 montato dal file /etc/fstab:</p> Bash<pre><code>LABEL=cloudimg-rootfs   /        ext4   defaults,discard        0 1\nmagento-efs-stg.buffetti.internal:/ /mnt/b2c-nfs efs _netdev,noresvport,tls 0 0\n</code></pre> <p>I log vengono centralizzati su CloudWatch Logs tramite un agent, la configurazione si trova sotto /opt/aws/amazon-cloudwatch-agent/bin/config.json</p> JSON<pre><code>{\n  \"agent\": {\n    \"run_as_user\": \"cwagent\"\n  },\n  \"logs\": {\n    \"logs_collected\": {\n      \"files\": {\n        \"collect_list\": [\n          {\n            \"file_path\": \"/var/log/syslog_test\",\n            \"log_group_name\": \"syslog_test\",\n            \"log_stream_name\": \"{instance_id}\"\n          },\n          {\n            \"file_path\": \"/var/log/nginx/access.log\",\n            \"log_group_name\": \"nginx_access\",\n            \"log_stream_name\": \"{instance_id}\"\n          },\n          {\n            \"file_path\": \"/var/log/nginx/error.log\",\n            \"log_group_name\": \"nginx_error\",\n            \"log_stream_name\": \"{instance_id}\"\n          },\n          {\n            \"file_path\": \"/var/www/b2c/var/log/connector.log\",\n            \"log_group_name\": \"magento_connector\",\n            \"log_stream_name\": \"{instance_id}\"\n          },\n          {\n            \"file_path\": \"/var/www/b2c/var/log/debug.log\",\n            \"log_group_name\": \"magento_debug\",\n            \"log_stream_name\": \"{instance_id}\"\n          },\n          {\n            \"file_path\": \"/var/www/b2c/var/log/exception.log\",\n            \"log_group_name\": \"magento_exception\",\n            \"log_stream_name\": \"{instance_id}\"\n          },\n          {\n            \"file_path\": \"/var/www/b2c/var/log/system.log\",\n            \"log_group_name\": \"magento_system\",\n            \"log_stream_name\": \"{instance_id}\"\n          }\n        ]\n      }\n    }\n  }\n}\n</code></pre> <p>Sulla macchine \u00e8 installato in docker Nginx sulla porta 80 che fa da proxy verso Varnish in docker, porta 8080. Il codice si trova sulla macchina stessa, la fase di deploy viene fatta dall'EC2 Admin, vedere il paragrafo Operations per il funzionamento</p>"},{"location":"customers/buffetti-C0324/B2C/#prd","title":"Prd","text":"<p>L'ambiente di produzione \u00e8 speculare a quello di stage, ma multi az. Tutto quello che era possibile criptare on at Rest \u00e8 stato fatto con KMS o chiavi di default di AWS, mantere questa convenzione. Esiste un Cloudfront che fa da cache per i contenuti statici. Esiste un Custom Header per far si che il bilanciatore accetti il traffico solamente dal Cloudfront e non direttamente (il traffico tra di essi \u00e8 cifrato). Al Cloufront \u00e8 associato un WAF al quale sono associate regole di rate-limit, ip permessi e ip malevoli (aggiornati ogni ora dalle nostre lambda in cc-digital). Al Waf \u00e8 associato un Kinesis che ha il compito di inviare i logs al Bucket S3. On-Premise Logstash recupera questi logs e li invia al nostro Kibana(Ricordarsi di selezionare l'host corretto). Sono presenti due EC2 su due AZ differenti. Il database \u00e8 su RDS. Esiste un Redis(non c'\u00e8 https, ma la sicurezza \u00e8 garantita a livello di networking concordato con il cliente) e un ElasticSearch, quest'ultimo \u00e8 obbligatorio per l'ultima versione di Magento. Il DNS interno \u00e8 gestito da Route53. L'EFS \u00e8 montato dal file /etc/fstab:</p> Text Only<pre><code>LABEL=cloudimg-rootfs   /        ext4   defaults,discard        0 1\nmagento-efs-prd.buffetti.internal:/ /mnt/b2c-nfs efs _netdev,noresvport,tls 0 0\n</code></pre> <p>I log vengono centralizzati su CloudWatch Logs tramite un agent, la configurazione si trova sotto /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/file_config.json</p> JSON<pre><code>{\n  \"agent\": {\n    \"run_as_user\": \"cwagent\"\n  },\n  \"logs\": {\n    \"logs_collected\": {\n      \"files\": {\n        \"collect_list\": [\n          {\n            \"file_path\": \"/var/log/syslog\",\n            \"log_group_name\": \"syslog\",\n            \"log_stream_name\": \"prd-fr-b\"\n          },\n          {\n            \"file_path\": \"/var/log/nginx/access.log\",\n            \"log_group_name\": \"nginx_access\",\n            \"log_stream_name\": \"prd-fr-b\"\n          },\n          {\n            \"file_path\": \"/var/log/nginx/error.log\",\n            \"log_group_name\": \"nginx_error\",\n            \"log_stream_name\": \"prd-fr-b\"\n          },\n          {\n            \"file_path\": \"/var/log/php7.4-fpm.log\",\n            \"log_group_name\": \"phpfpm_logs\",\n            \"log_stream_name\": \"prd-fr-b\"\n          },\n          {\n            \"file_path\": \"/var/www/b2c/var/log/connector.log\",\n            \"log_group_name\": \"magento_connector\",\n            \"log_stream_name\": \"prd-fr-b\"\n          },\n          {\n            \"file_path\": \"/var/www/b2c/var/log/debug.log\",\n            \"log_group_name\": \"magento_debug\",\n            \"log_stream_name\": \"prd-fr-b\"\n          },\n          {\n            \"file_path\": \"/var/www/b2c/var/log/exception.log\",\n            \"log_group_name\": \"magento_exception\",\n            \"log_stream_name\": \"prd-fr-b\"\n          },\n          {\n            \"file_path\": \"/var/www/b2c/var/log/system.log\",\n            \"log_group_name\": \"magento_system\",\n            \"log_stream_name\": \"prd-fr-b\"\n          }\n        ]\n      }\n    }\n  }\n}\n</code></pre> <p>Sulla macchine \u00e8 installato in docker Nginx sulla porta 80 che fa da proxy verso Varnish in docker, porta 8080. Il codice si trova sulla macchina stessa, la fase di deploy viene fatta dall'EC2 Admin, vedere il paragrafo Operations per il funzionamento</p>"},{"location":"customers/buffetti-C0324/B2C/CancellazioneAmbiente/","title":"Cancellazione Ambiente","text":""},{"location":"customers/buffetti-C0324/B2C/CancellazioneAmbiente/#cancellazione-ambiente","title":"Cancellazione ambiente","text":"<ul> <li>Creazione uno snapshot manuale del RDS e cancellarlo</li> <li>Eliminare il resto via terraform</li> <li>Il bastion host ha dei mount dell'efs, una volta cancellato l'ambiente, ricordarsi di smontarli: ITO-823 Bash<pre><code>umount -f -l /home/buffetti-scp/share-stg2\numount -f -l /mnt/share-stg2\n</code></pre></li> </ul>"},{"location":"customers/buffetti-C0324/B2C/ClerkAccessForbidden/","title":"Clerk.io 403 access forbidden","text":""},{"location":"customers/buffetti-C0324/B2C/ClerkAccessForbidden/#clerkio-403-access-forbidden","title":"Clerk.io 403 access forbidden","text":"<p>Il cliente lamenta un problema di sincronizzazione dei prodotti con Clerk.</p> <p>Aggiungere l'IP di clerk.io all'AllowedList del WAF su AWS del cliente.</p> <p>Lista degli IP di clerk: https://help.clerk.io/en/articles/1322205-how-to-whitelist-the-importers-ip-address</p>"},{"location":"customers/buffetti-C0324/B2C/PerformanceImportRDS/","title":"Performance Import RDS","text":""},{"location":"customers/buffetti-C0324/B2C/PerformanceImportRDS/#performance-import-rds","title":"Performance Import RDS","text":""},{"location":"customers/buffetti-C0324/B2C/PerformanceImportRDS/#problemi-noti","title":"Problemi noti","text":"<p>Ogni tanto pu\u00f2 capitare che un cron faccia molte operazioni su disco dell'RDS, consumando tutti i crediti burst del volume. In questo caso se \u00e8 in orario lavorativo bisogna contattare il cliente in modo proattivo prima che termini il credito. In orario NON H24 \u00e8 consigliato interrompere i cron che si trovano sul server Admin Maggiori dettagli sull'allarme</p>"},{"location":"customers/buffetti-C0324/B2C/SetupMagento/","title":"Setup Magento","text":""},{"location":"customers/buffetti-C0324/B2C/SetupMagento/#setup-magento","title":"Setup Magento","text":""},{"location":"customers/buffetti-C0324/B2C/SetupMagento/#setup-stg-env","title":"Setup STG env","text":"Bash<pre><code># Elasticsearch\nbin/magento config:set catalog/search/elasticsearch7_server_hostname 'https://vpc-magento-buffetti-b2c-prd-lxfyuwuvqh7xv6ylddqzmlgxdu.eu-central-1.es.amazonaws.com'\nbin/magento config:set catalog/search/elasticsearch7_index_prefix 'buffettib2c_stg'\nbin/magento config:set catalog/search/elasticsearch7_username 'magento'\nbin/magento config:set catalog/search/elasticsearch7_password 'XXXXXXXXXXXXXXX'\n\n# Hostname\nbin/magento config:set web/unsecure/base_url 'https://stg-b2c.buffetti.it/'\nbin/magento config:set web/secure/base_url 'https://stg-b2c.buffetti.it/'\nbin/magento config:set web/cookie/cookie_domain 'stg-b2c.buffetti.it'\n\n# User\nbin/magento admin:user:create --admin-user=asosso --admin-password=XXXXXXXXXXXXXXX --admin-email=a.sosso@criticalcase.com --admin-firstname=Andrea --admin-lastname=Sosso\n</code></pre>"},{"location":"customers/buffetti-C0324/B2C/SetupMagento/#setup-stg2-env","title":"Setup STG2 env","text":"Bash<pre><code># Elasticsearch\nbin/magento config:set catalog/search/elasticsearch7_server_hostname 'https://vpc-magento-buffetti-b2c-stg2-vs4r6ln2xp5ufg4rgwuqrcvt5m.eu-central-1.es.amazonaws.com'\nbin/magento config:set catalog/search/elasticsearch7_index_prefix 'buffettib2c_stg2'\nbin/magento config:set catalog/search/elasticsearch7_username 'magento'\nbin/magento config:set catalog/search/elasticsearch7_password 'XXXXXXXXXXXXXXX'\n\n# Hostname\nbin/magento config:set web/unsecure/base_url 'https://stg2-b2c.buffetti.it/'\nbin/magento config:set web/secure/base_url 'https://stg2-b2c.buffetti.it/'\nbin/magento config:set web/cookie/cookie_domain 'stg2-b2c.buffetti.it'\n\n# User\nbin/magento admin:user:create --admin-user=asosso --admin-password=XXXXXXXXXXXXXXX --admin-email=a.sosso@criticalcase.com --admin-firstname=Andrea --admin-lastname=Sosso\n</code></pre>"},{"location":"customers/buffetti-C0324/B2C/SetupMagento/#setup-stg3-env","title":"Setup STG3 env","text":"Bash<pre><code># Elasticsearch\nbin/magento config:set catalog/search/elasticsearch7_server_hostname 'https://vpc-magento-buffetti-b2c-stg3-kufxecllmcjsqp7erscjs4c2ry.eu-central-1.es.amazonaws.com'\nbin/magento config:set catalog/search/elasticsearch7_index_prefix 'buffettib2c_stg3'\nbin/magento config:set catalog/search/elasticsearch7_username 'magento'\nbin/magento config:set catalog/search/elasticsearch7_password 'XXXXXXXXXXXXXXX'\n\n# Hostname\nbin/magento config:set web/unsecure/base_url 'https://stg3-b2c.buffetti.it/'\nbin/magento config:set web/secure/base_url 'https://stg3-b2c.buffetti.it/'\nbin/magento config:set web/cookie/cookie_domain 'stg3-b2c.buffetti.it'\n\n# User\nbin/magento admin:user:create --admin-user=asosso --admin-password=XXXXXXXXXXXXXXX --admin-email=a.sosso@criticalcase.com --admin-firstname=Andrea --admin-lastname=Sosso\n</code></pre>"},{"location":"customers/buffetti-C0324/B2C/SetupMagento/#setup-dev-env","title":"Setup dev env","text":"<p>Import database da stg, comandi per renderlo compatibile con database mysql di dev</p> Bash<pre><code>sed -i '/@@GLOBAL.GTID_PURGED=/d' export_stg2_db_3.sql\nsed -i 's/utf8mb4/utf8/g' export_stg2_db_3.sql\nsed -i 's/utf8mb4_0900_ai_ci/utf8_general_ci/g' export_stg2_db_3.sql\n</code></pre> <p>Andare sotto /var/www/b2c</p> Bash<pre><code># Elasticsearch\nbin/magento config:set catalog/search/elasticsearch7_server_hostname 'http://127.0.0.1'\nbin/magento config:set catalog/search/elasticsearch7_server_port '9200'\nbin/magento config:set catalog/search/elasticsearch7_index_prefix 'buffettib2c_dev'\nbin/magento config:set catalog/search/elasticsearch7_username NULL\nbin/magento config:set catalog/search/elasticsearch7_password NULL\n\n# Hostname\nbin/magento config:set web/unsecure/base_url 'https://dev-b2c.buffetti.it/'\nbin/magento config:set web/secure/base_url 'https://dev-b2c.buffetti.it/'\nbin/magento config:set web/cookie/cookie_domain 'dev-b2c.buffetti.it'\n</code></pre>"},{"location":"customers/buffetti-C0324/B2C/SetupMagento/#useful-commands","title":"Useful commands","text":"Text Only<pre><code>bin/magento indexer:reindex\n\nbin/magento config:set web/secure/use_in_frontend 0\nbin/magento config:set web/secure/use_in_adminhtml 0\nbin/magento cache:clean config\n</code></pre>"},{"location":"customers/buffetti-C0324/B2C/Teleport/","title":"Teleport","text":""},{"location":"customers/buffetti-C0324/B2C/Teleport/#creazione-account-teleport-cliente","title":"Creazione account teleport cliente","text":"Bash<pre><code>tctl users add --roles=buffetti-stg,buffetti-prd --logins=buffetti p.marinelli@buffetti.it\n</code></pre>"},{"location":"customers/buffetti-C0324/B2C/Teleport/#database-access-login","title":"Database access login","text":"Bash<pre><code># Login to teleport server\ntsh login --proxy=tp.aws-buffetti-b2c.criticalcasecloud.com:3080 --auth=local --user=a.sosso@criticalcase.com\n\n# Login to RDS stg\ntsh db login --db-user=teleport --db-name=magento magento-buffetti-b2c-stg\ntsh proxy db -p 13036 magento-buffetti-b2c-stg\n\n# Login to RDS prd\ntsh db login --db-user=teleport --db-name=magento magento-buffetti-b2c-prd\ntsh proxy db -p 13037 magento-buffetti-b2c-prd\n\n# Login to RDS stg2\ntsh db login --db-user=teleport --db-name=magento magento-buffetti-b2c-stg2\ntsh proxy db -p 13038 magento-buffetti-b2c-stg2\n</code></pre>"},{"location":"customers/buffetti-C0324/B2C/Teleport/#database-permissions","title":"Database permissions","text":"SQL<pre><code>-- Grants for 'teleport'@'%'\nCREATE USER IF NOT EXISTS 'teleport'@'%';\nALTER USER 'teleport'@'%' IDENTIFIED WITH 'AWSAuthenticationPlugin' AS 'RDS' REQUIRE NONE PASSWORD EXPIRE DEFAULT ACCOUNT UNLOCK PASSWORD HISTORY DEFAULT PASSWORD REUSE INTERVAL DEFAULT PASSWORD REQUIRE CURRENT DEFAULT;\nGRANT ALL PRIVILEGES ON `magento`.* TO `teleport`@`%`;\nGRANT USAGE ON *.* TO `teleport`@`%`;\n</code></pre>"},{"location":"customers/campomarzio-C0778/","title":"Campo Marzio Signature S.r.l. @ C0778","text":""},{"location":"customers/campomarzio-C0778/#progetti-senza-ito","title":"Progetti Senza ITO","text":"<ul> <li>~~CR210724: gestione risorse su account AWS 658357154369~~ disdetto!</li> </ul>"},{"location":"customers/campomarzio-C0778/#note-h24","title":"Note H24","text":"<ul> <li>Il DNS \u00e8 gestito da Criticalcase e alcuni domini dal cliente</li> <li>Le risorse sono sotto backup con AWS Backup</li> <li>Le risorse sono tutte criptate at Rest, mantenere questa convenzione il pi\u00f9 possibile</li> <li>Le credenziali sono su Sherlock</li> <li>Controllare Alerta per allarmi derivanti da Cloudwatch Credenziali</li> <li>Il progetto \u00e8 deployato tramite Terraform Cloud Prd.Il codice \u00e8 su GitHub</li> <li>Essendo in ITO, l'account \u00e8 controllato sia dalle Lambda in cc-digital</li> </ul>"},{"location":"customers/campomarzio-C0778/#campo-marzio","title":"Campo Marzio","text":"<p>Internal data:</p> <ul> <li>Sales: Marina Micelli</li> <li>Phone: +39 337.1554741</li> <li>Email: m.micelli@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Renata Romanelli</li> <li>Email: r.romanelli@campomarzio.eu</li> <li>Phone: +39 327 6555509</li> </ul>"},{"location":"customers/campomarzio-C0778/#accesso","title":"Accesso","text":"<p>https://cc-ds1.criticalcasecloud.com/web/cluster/tp.realizesrl.criticalcasecloud.com/nodes</p> <ul> <li>Credenziali: https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/list/ds/customers/campomarzio/</li> </ul>"},{"location":"customers/campomarzio-C0778/#elasticsearch","title":"ElasticSearch","text":"<ul> <li>Credenziali: https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/show/ds/customers/campomarzio-C0778/waf-elasticsearch-cc</li> </ul>"},{"location":"customers/campomarzio-C0778/#aws","title":"Aws","text":"Text Only<pre><code>Account: 658357154369\nAlias: campomarziosrl\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/campomarzio-C0778/#campomarzioit","title":"CampoMarzio.it","text":"<p>La macchina \u00e8 stata migrata dal Datacenter di Criticalcase in AWS</p> <p>Esiste una NAT nel bastion host (3.69.56.165) sulla porta 22222 verso la 22 del server campomarzio. In questo modo il cliente riesce a connettersi all'istanza privata dal suo ufficio in sftp. Le regole iptables vengono aggiunte all'avvio della macchina. File di configurazione:</p> <p>/etc/networkd-dispatcher/routable.d/50-ifup-hooks</p> Text Only<pre><code>#!/bin/sh\n\nfor d in up post-up; do\n    hookdir=/etc/network/if-${d}.d\n    [ -e $hookdir ] &amp;&amp; /bin/run-parts $hookdir\ndone\nexit 0\n</code></pre> <p>/etc/network/if-up.d/pf</p> Text Only<pre><code>#!/bin/sh\n\nif [ \"$IFACE\" = ens5 ]; then\n\n#Campomarzio\niptables -I PREROUTING 1 -t nat -i ens5 -p tcp --dport 22222 -j DNAT --to 10.0.1.143:22\niptables -I FORWARD 1 -p tcp -d 10.0.1.143 --dport 22 -j ACCEPT\niptables -t nat -I POSTROUTING 1 -o ens5 -p tcp -d 10.0.1.143 --dport 22 -j SNAT --to 10.0.10.86\n\nfi\n</code></pre> <p>Non tutti i domini DNS sono stati migrati sotto Criticalcase. Esiste un server redirect-CampoMarzio che si occupa di fare il redirect verso www.campomarzio.it I domini che puntano a questo server attualemente sono campomarzio.eu,campomarzio.store,campomarzio.hk</p> <p>I certificati sono gestiti da Let's Encrypt. HaProxy \u00e8 gestito da Ansible. Esistono the cron per il rinnovo automatico:</p> Text Only<pre><code>#Ansible: CleanTempFolder CRON\n18 8 * * * find /tmp -ctime +10 -exec rm -rf {} +\n#Ansible: Renew letsencrypt\n0 6 * * * letsencrypt renew --register-unsafely-without-email --agree-tos &gt; /dev/null 2&gt;&amp;1\n#Ansible: Reload haproxy\n18 6 * * * /etc/haproxy/letsencrypt-haproxy-standalone &gt; /dev/null 2&gt;&amp;1\n</code></pre> <p></p> <p>Risorse</p> <ul> <li>Cloudfront</li> <li>S3</li> <li>ALB</li> <li>EC2</li> <li>CloudWatch</li> <li>SNS</li> </ul>"},{"location":"customers/comau-C1317/","title":"Comau S.p.A. @ C1317","text":""},{"location":"customers/comau-C1317/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR210496: Pacchetto di Ore di Consulenza Tecnica per piattaforma EDO cliente COMAU SPA.</li> </ul> <p>Internal data:</p> <ul> <li>Sales: Vittorino Aprile</li> <li>Phone: +39 392 912 2831</li> <li>Email: v.aprile@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Stefano Pesce</li> <li>Email: stefano.pesce@comau.com</li> </ul> <ul> <li>Full name: Alessandro Renato</li> <li>Email: alessandro.renato@external.comau.com</li> </ul> <p>Il cliente non \u00e8 gestito da ITO e non ha un account su JIRA.</p>"},{"location":"customers/comau-C1317/#riepilogo","title":"Riepilogo","text":"<p>Abbiamo effettuato inizialmente una POC con HAProxy per esporre alcuni raspberry su rete pubblica tramite https/wss aggiungendo un layer di authenticazione.</p> <p>Successivamente \u00e8 stato installato un Raspberry PI per le operazioni di loadbalancer.</p>"},{"location":"customers/comau-C1317/#firewall-pfsense","title":"Firewall pfsense","text":"<ul> <li>Internal VPN URL: https://10.99.99.1:44301 (credentials)</li> <li>Public URL: https://195.88.6.193:44301</li> </ul> <p>IP Addresses:</p> <ul> <li><code>10.0.0.254</code></li> <li><code>10.42.0.254</code></li> <li><code>195.88.6.193</code></li> <li><code>192.168.103.2</code></li> </ul>"},{"location":"customers/comau-C1317/#vpn","title":"VPN","text":"<ul> <li>Download the VPN Configuration file</li> </ul> <p>VPN Network: <code>10.99.99.0/24</code> Routed Networks: <code>10.42.0.0/24</code>, <code>192.168.10.0/24</code></p>"},{"location":"customers/comau-C1317/#nat-rules","title":"Nat rules","text":"<ul> <li>Url: https://10.99.99.1:44301/firewall_nat.php</li> </ul> <p>Le seguenti regole abilitano: HTTP 80 (+letsencrypt), HTTPS 443, HAProxy stats 1936.</p> Text Only<pre><code>Interface   Protocol    Source Address  Source Ports    Dest. Address   Dest. Ports NAT IP  NAT Ports   Description Actions\nWAN TCP *   *   192.168.103.3   80 (HTTP)   10.0.0.200  80 (HTTP)   letsencrypt haproxy raspberry\nWAN TCP *   *   192.168.103.3   1936    10.0.0.200  1936    dashboard haproxy raspberry\nWAN TCP *   *   192.168.103.3   443 (HTTPS) 10.0.0.200  443 (HTTPS) https haproxy raspberry\n</code></pre> <p>L'indirizzo IP pubblic <code>195.88.6.207</code> corrisponde all'IP interno <code>192.168.103.3</code></p>"},{"location":"customers/comau-C1317/#raspberry-pi-haproxy","title":"Raspberry PI HAProxy","text":"<ul> <li>Internal IP: <code>10.0.0.200/24</code> gw <code>10.0.0.254</code></li> <li>Public DNS: <code>*.rpi.comau.ccws.it</code></li> <li>Public URL example: https://web-129.rpi.comau.ccws.it</li> <li>HAProxy stats dashboard: https://dashboard.rpi.comau.ccws.it:1936 (credentials)</li> </ul>"},{"location":"customers/comau-C1317/#ansible","title":"Ansible","text":"<ul> <li>Ansible repo: https://github.com/criticalcase/comau-edocloud</li> <li>User and password configuration: https://github.com/criticalcase/comau-edocloud/blob/main/group_vars/haproxy.yaml#L3</li> </ul>"},{"location":"customers/comau-C1317/#connecting-from-outside","title":"Connecting from outside","text":"Bash<pre><code>ssh 195.88.6.193 -p 2222\n</code></pre>"},{"location":"customers/comau-C1317/#test-backend-raspberry-connection","title":"Test backend raspberry connection","text":"Bash<pre><code>curl 10.42.0.128:9090\nCan \"Upgrade\" only to \"WebSocket\"\n</code></pre>"},{"location":"customers/comau-C1317/#test-private-websocket-connection","title":"Test private websocket connection","text":"Bash<pre><code>curl --http1.1 --include -k  \\\n     --header \"Connection: Upgrade\" \\\n     --header \"Upgrade: websocket\" \\\n     --header \"Sec-WebSocket-Key: 3SwEQwvZ9L0NhaP6NAtSPQ==\" \\\n     --header \"Sec-WebSocket-Version: 13\" \\\n     \\\n     -u comau:XXXXXXXX \\\n     --header \"Host: web-129.rpi.comau.ccws.it\" \\\n     --header \"Origin: https://web-129.rpi.comau.ccws.it\" \\\n     https://localhost\nHTTP/1.1 101 Switching Protocols\n</code></pre>"},{"location":"customers/comau-C1317/#test-public-websocket-connection","title":"Test public websocket connection","text":"Bash<pre><code>curl --http1.1 --include \\\n     --header \"Connection: Upgrade\" \\\n     --header \"Upgrade: websocket\" \\\n     --header \"Sec-WebSocket-Key: 3SwEQwvZ9L0NhaP6NAtSPQ==\" \\\n     --header \"Sec-WebSocket-Version: 13\" \\\n     \\\n     -u comau:XXXXXXXX \\\n     --header \"Origin: https://web-129.rpi.comau.ccws.it\" \\\n     https://web-129.rpi.comau.ccws.it\nHTTP/1.1 101 Switching Protocols\n</code></pre>"},{"location":"customers/comau-C1317/#test-websocket-via-browser","title":"Test websocket via browser","text":"<ol> <li>Authenticate to: https://web-129.rpi.comau.ccws.it</li> <li>Go to websocket page test: https://www.websocket.org/echo.html</li> <li>Fill the location: <code>wss://web-129.rpi.comau.ccws.it</code></li> <li>Press connect</li> <li>Send a message</li> <li>Disconnect</li> </ol> <p>Output:</p> Text Only<pre><code>CONNECTED\n\nSENT: Rock it with HTML5 WebSocket\n\nDISCONNECTED\n</code></pre>"},{"location":"customers/comau-C1317/#test-cors","title":"Test CORS","text":"Bash<pre><code>curl -H \"Host: web-129.rpi.comau.ccws.it\" -k -X OPTIONS https://localhost -I\n[...]\naccess-control-allow-origin: *\naccess-control-allow-headers: *\n[...]\n</code></pre>"},{"location":"customers/comau-C1317/#drawio","title":"Draw.io","text":"<p>Schema fornito dal cliente</p> <p></p>"},{"location":"customers/comau-C1317/#flusso-di-richiesta-principale","title":"Flusso di richiesta principale","text":"<p>Passaggi da 1 a 5 nello schema.</p> <ol> <li>Viene inviata una richiesta a https://web-101.rpi.comau.ccws.it. HAProxy risponder\u00e0 con un certificato valido rilasciato da letsencrypt.</li> <li>Il sistema controlla se nella query string <code>?apikey=</code> \u00e8 presente un Token JWT valido e firmato e non scaduto. In caso contrario la richiesta viene negata (con un codice di errore: <code>403 Forbidden</code>).</li> <li>Viene controllato lo scope, se trova rp-101 la richiesta pu\u00f2 procedere verso il raspberry 101, altrimenti viene negata (con un codice di errore: <code>403 Forbidden</code>).</li> <li>Vengono aggiunti degli header utili per il debug:</li> <li>X-VIP: \u00e8 il nome del server HAProxy che sta rispondendo</li> <li>X-Server: \u00e8 il nome del raspberry che sta rispondendo</li> <li>La richiesta viene inviata in http al raspberry 10.42.0.101 sulla porta 9090</li> </ol>"},{"location":"customers/comau-C1317/#aggiunta-header-cors","title":"Aggiunta header CORS","text":"<p>Passaggio 6 nello schema.</p> <p>Quando la richiesta arriva da un'interfaccia web pubblicata su un dominio diverso da <code>web-101.rpi.comau.ccws.it</code>, il browser fa una richiesta CORS (per maggiori dettagli sul funzionamento clicca qui: https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS).</p> <p>Sfortunatamente i raspberry non sono in grado di rispondere alle richieste CORS, quindi HAProxy \u00e8 stato configurato per interccettare tutte le richieste con method OPTION e aggiunge gli header CORS rispondendo al brwoser con status code <code>204 NO Content</code>.</p>"},{"location":"customers/comau-C1317/#richiesta-certificato-letsencrypt","title":"Richiesta certificato letsencrypt","text":"<p>Passaggio 7 nello schema.</p> <p>Per richiedere un certificato HTTPS per un nuovo hostname, es <code>web-000.rpi.comau.ccws.it</code> \u00e8 necessario far puntare i record DNS verso <code>195.88.6.207</code></p> Text Only<pre><code>web-000.rpi.comau.ccws.it. 3600 IN  A   195.88.6.207\n</code></pre> <p>Successivamente \u00e8 necessario richiedere il certificato dalla VM dove gira HAProxy. HAProxy reindirizzer\u00e0 tutte le richieste verso <code>/.well-know/acme-challenges</code> al demone di letsencrypt che sar\u00e0 in ascolto sulla porta 8888 quando viene lanciato il seguente comando:</p> Bash<pre><code>$ letsencrypt certonly --standalone --non-interactive --agree-tos --http-01-port=8888 -d web-000.rpi.comau.ccws.it\n\nSaving debug log to /var/log/letsencrypt/letsencrypt.log\nPlugins selected: Authenticator standalone, Installer None\nObtaining a new certificate\nPerforming the following challenges:\nhttp-01 challenge for web-000.rpi.comau.ccws.it\nWaiting for verification...\nCleaning up challenges\n\nIMPORTANT NOTES:\n - Congratulations! Your certificate and chain have been saved at:\n   /etc/letsencrypt/live/web-000.rpi.comau.ccws.it/fullchain.pem\n   Your key file has been saved at:\n   /etc/letsencrypt/live/web-000.rpi.comau.ccws.it/privkey.pem\n   Your cert will expire on 2022-03-02. To obtain a new or tweaked\n   version of this certificate in the future, simply run certbot\n   again. To non-interactively renew *all* of your certificates, run\n   \"certbot renew\"\n - If you like Certbot, please consider supporting our work by:\n\n   Donating to ISRG / Let's Encrypt:   https://letsencrypt.org/donate\n   Donating to EFF:                    https://eff.org/donate-le\n</code></pre> <p>Una volta installato il certificato va aggiunto alla configurazione di HAProxy eseguendo il comando:</p> Bash<pre><code>$ /etc/haproxy/letsencrypt-haproxy-standalone\nCombined: web-000.rpi.comau.ccws.it\nCombined: web-100.rpi.comau.ccws.it\nCombined: web-101.rpi.comau.ccws.it\n[...]\n</code></pre> <p>Lo script far\u00e0 automaticamente il reload di HAProxy.</p>"},{"location":"customers/comau-C1317/#rinnovo-certificato-letsencrypt","title":"Rinnovo certificato letsencrypt","text":"<p>Passaggio 8 nello schema.</p> <p>Abbiamo inserito nel cron della VM i seguenti script</p>"},{"location":"customers/comau-C1317/#rinnovare-i-certificati-letsencrypt","title":"Rinnovare i certificati letsencrypt","text":"Bash<pre><code>#Ansible: Renew letsencrypt\n0 6 * * * letsencrypt renew --register-unsafely-without-email --agree-tos &gt; /dev/null 2&gt;&amp;1\n</code></pre> <p>Il task cron \u00e8 schedulato per operare ogni giorno.</p>"},{"location":"customers/comau-C1317/#raspberry-di-staging","title":"Raspberry di staging","text":"<p>Passaggio 9 nello schema.</p> <p>Se l'hostname richiesto fa parte di un raspberry di staging <code>STG</code> la richiesta viene inoltrata ad una seconda istanza di HAProxy che risponde in localhost sulla porta 8081.</p> <p>Lista di hostname che seguono questo flusso:</p> <ul> <li>https://web-103.rpi.comau.ccws.it</li> <li>https://web-104.rpi.comau.ccws.it</li> <li>https://web-105.rpi.comau.ccws.it</li> </ul> <p>Il secondo HAProxy si occuper\u00e0 solo dei seguenti punti:</p> <ul> <li>Il sistema controlla se nella query string <code>?apikey=</code> \u00e8 presente un Token JWT valido e firmato e non scaduto. In caso contrario la richiesta viene negata (con un codice di errore: <code>403 Forbidden</code>).</li> <li>Viene controllato lo scope, se trova rp-103, rp-104 o rp-105 la richiesta pu\u00f2 procedere verso il raspberry 101, altrimenti viene negata (con un codice di errore: <code>403 Forbidden</code>).</li> <li>La richiesta viene inviata in http al raspberry 10.42.0.103, 10.42.0.104 o 10.42.0.105 sulla porta 9090</li> </ul>"},{"location":"customers/comau-C1317/#caricare-i-certificati-su-haproxy-e-fare-il-reload","title":"Caricare i certificati su HAProxy e fare il reload","text":"Bash<pre><code>#Ansible: Combine certs and Reload haproxy\n18 6 * * * /etc/haproxy/letsencrypt-haproxy-standalone &gt; /dev/null 2&gt;&amp;1\n</code></pre> <p>Il task cron \u00e8 schedulato per operare ogni giorno dopo il rinnovo dei certificati.</p>"},{"location":"customers/comau-C1317/#offerta-su-aws","title":"Offerta su AWS","text":"<ul> <li>https://calculator.aws/#/estimate?id=cff9e67866cfabe9ef70a4b34befe5c6c5180666</li> </ul>"},{"location":"customers/connexia-C1881/","title":"Connexia @ C1881","text":""},{"location":"customers/connexia-C1881/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR210410: Connexia Configuratore Florim - Gestione di un server ed un Database su Account Azure del Cliente (Florim).</li> </ul> <p>ITO / H24: Se il cliente dovesse chiamare in reperibilit\u00e0 rispondergli che il servizio non \u00e8 sotto ITO e di inviare una mail a riccardo.fabbri@nohup.it</p> <p>~~Intervenire solo se nel messaggio \u00e8 presente la parola \"configuratore\".~~ &gt; ~~Arriva un'email alla lista di distribuzione d.service@criticalcase.com.~~ &gt; ~~Le email hanno tra i destinatari florim-sistemi@connexia.com.~~</p>"},{"location":"customers/connexia-C1881/#note-h24","title":"Note H24","text":"<ul> <li>Il progetto \u00e8 per Connexia ma il customer finale \u00e8 Florim</li> </ul>"},{"location":"customers/connexia-C1881/#connexia","title":"Connexia","text":"<p>Internal data:</p> <ul> <li>Start date: 14/04/2021</li> <li>Referente Commerciale: Silvano Griot</li> <li>Telefono referente: +39 335 142 6087</li> <li>Email: s.griot@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Monica Rossi</li> <li>Email: monica.rossi@connexia.com</li> <li>Phone: +39 335 8468426</li> </ul>"},{"location":"customers/connexia-C1881/#data-center","title":"Data Center","text":"<ul> <li> AWS</li> <li> Azure</li> <li> GCP</li> <li> Tencent</li> <li> Critical Case RZ1</li> <li> Critical Case RZ2</li> </ul>"},{"location":"customers/connexia-C1881/#descrizione-ambito-funzionale-e-tecnologico","title":"Descrizione ambito funzionale e tecnologico","text":"<p>E' presente un solo ambiente consistente in 1 VM ed 1 DB nell'account di Azure di florim, L'environment \u00e8 tutto nel RG_EU_VirtualMachines_Sites Fare un filtro sulle risorse 04 per ottenere le sole componenti in nostra gestione (figura).</p> <p></p>"},{"location":"customers/connexia-C1881/#accessi","title":"Accessi","text":"<p>Accessi su vault per la VM ed il DB: https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/show/ds/customers/connexia-C1881</p> <p>Accesso teleport: https://cc-ds1.criticalcasecloud.com</p>"},{"location":"customers/consulman-C0422/","title":"Consulman S.r.l @ C0422","text":""},{"location":"customers/consulman-C0422/#ito-h24","title":"ITO H24","text":"<ul> <li>CR210526: Gestione account AWS 645128989513 (M1, Poseidone, Thor, Domino, consulman.it e progettifinanziati.ferrari.com)</li> <li>CR220148: Rivendita account AWS</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Piero Gennusa</li> <li>Email: p.gennusa@consulman.it</li> <li>Phone: +39 3923490565</li> </ul>"},{"location":"customers/consulman-C0422/#operations","title":"Operations","text":"<ul> <li>Server: gestione dei server</li> <li>Controllare Alerta per allarmi derivanti da Cloudwatch Credenziali</li> </ul>"},{"location":"customers/consulman-C0422/#teleport","title":"Teleport","text":"<ul> <li>Access to the VMs is possible via teleport trusted cluster: <code>tp.consulman.criticalcasecloud.com</code></li> <li>Teleport proxy: https://tp.consulman.criticalcasecloud.com (credentials)</li> </ul>"},{"location":"customers/consulman-C0422/#data-center","title":"Data Center","text":"<ul> <li> AWS 645128989513</li> </ul> Text Only<pre><code>Account: 645128989513\nAlias: consulman\n</code></pre>"},{"location":"customers/consulman-C0422/#architettura","title":"Architettura","text":""},{"location":"customers/consulman-C0422/#load-balancers","title":"Load balancers","text":"<ul> <li> <p>Application load balancer <code>fr-alb-01</code>. Questo bilanciatore gestisce numerosi domini e li reindirizza sulle EC2 corrette (utilizzano le rules del bilanciatore)</p> </li> <li> <p>Network load balancer <code>fr-nlb-01</code> punta direttamente all'istanza Thor, \u00e8 stato scelto un NLB poich\u00e9 Ferrari non vuole usare CNAME nel suo DNS server. I domini associati sono:</p> </li> <li>progettifinanziati.ferrari.com</li> <li>www.progettifinanziati.ferrari.com</li> </ul>"},{"location":"customers/consulman-C0422/#singoli-backups-databases-giornalieri-di-rds","title":"Singoli backups databases giornalieri di RDS","text":"<p>Il cliente ha chiesto di avere backups singoli dei databases all'interno di RDS con una determinata schedulazione.</p> <ul> <li> <p>Regole di CRON</p> </li> <li> <p>Bucket   sono presenti Lifecycle rules per la cancellazione dei vecchi backups in automatico</p> </li> </ul> <p></p>"},{"location":"customers/consulman-C0422/#singoli-backups-di-cartelle-allinterno-del-server-m1-verso-efs","title":"Singoli backups di cartelle all'interno del server M1 verso EFS","text":"<p>Il cliente ha chiesto di avere dei backups dei files giornalmente su EFS. Questo viene fatto attraverso una Run Commands di System Manager che esegue un rsync nella macchina allineando i files della macchina con l'EFS</p> <ul> <li>Regole di CRON cercare m1</li> </ul> <p></p>"},{"location":"customers/consulman-C0422/#auto-deploy-website","title":"Auto Deploy website","text":"<p>Deploying a static website on AWS involves pushing code to a GitHub repository triggering AWS CodePipeline for automatic deployment to an S3 bucket. CloudFront is set up to distribute content globally, ensuring HTTPS redirection for site traffic.</p> <p>The integration of GitHub as the repository source allows for a systematic approach to trigger AWS CodePipeline upon code changes. This triggers a streamlined process, ensuring that any modifications pushed to the GitHub repository prompt an automated deployment pipeline.</p> <p>Amazon S3 serves as a robust hosting solution for the website's updated codebase. The S3 bucket, configured with stringent privacy settings, guarantees a secure environment to store and distribute the website's content.</p> <p>AWS CodePipeline orchestrates the deployment flow, comprising stages for GitHub repository synchronization and the subsequent deployment of code changes to the designated S3 bucket. This pipeline automation minimizes manual intervention, thereby enhancing efficiency and accuracy in delivering updated website content.</p> <p>The implementation of an AWS CloudFront distribution marks a significant enhancement in content delivery, ensuring a globally optimized and accelerated user experience. With HTTPS redirection enforced, it guarantees a secure and encrypted channel for website traffic.</p> <p>To maintain the integrity and freshness of the website content, a cache invalidation mechanism has been integrated into the CodePipeline flow. This addition ensures that any changes made to the website are promptly reflected by invalidating CloudFront caches, so using Lambda function that receives an event triggered by an AWS CodePipeline job. It tries to create an invalidation for a specific CloudFront distribution thereby delivering the most recent content to users worldwide.</p> <p>Moreover, the CORS configuration in the CloudFront distribution sets up secure access policies, defining permitted origins, headers, and methods for accessing resources. This safeguards the website against unauthorized access and ensures controlled interactions between the server and requesting resources.</p> <p>This deployment workflow ensures automatic updates to the static website on code changes, global content distribution via CloudFront with HTTPS, cache invalidation, and secure access configurations through CORS and Origin Access Control.</p> <p></p>"},{"location":"customers/consulman-C0422/Server/","title":"Server","text":""},{"location":"customers/consulman-C0422/Server/#qb_import","title":"qb_import","text":"<ul> <li>AWS Console: qb_import</li> </ul> <p>E' un debian 7, non ha systemd, quindi per evitare problemi non \u00e8 stato installato teleport o l'SSM Agent. Per accedere al server, connettersi al bastion host tramite teleport o SSM, impersonare l'utente root e accedere all'utente root sul server qb_import tramite la porta 22000:</p> <p>Connettersi a bastion-host-teleport</p> Bash<pre><code>sudo -i\nssh root@172.31.48.15 -p 22000\n</code></pre> <p>Questo server monta un disco di un'altro server M1. Il comando utilizzato \u00e8 il seguente:</p> Bash<pre><code>sshfs -o allow_other,IdentityFile=~/.ssh/id_rsa ubuntu@m1.consulman.internal:/media/support_disk/backup/qb/production /home/questionbin/www/tormento2/upload_allegati_m1/production\n</code></pre>"},{"location":"customers/consulman-C0422/Server/#m1","title":"M1","text":"<ul> <li>AWS Console: M1</li> </ul> <p>Questo \u00e8 un elenco di servizi attualmente in funzione sulla macchina:</p> <ul> <li>apache2 -&gt; porta 80/443/8080/8081</li> <li>memcached</li> <li>dovecot</li> <li>amazon-efs-mount-watchdog</li> <li>amazon-ssm-agent</li> <li>mysql -&gt; porta 3306</li> <li>php-5.5.26-fpm</li> <li>php-5.5.28-fpm</li> <li>php7.0-fpm</li> <li>redis-server</li> <li>teleport</li> <li>zabbix-agent</li> </ul> <p>Esiste un backup di determinate cartelle e files su EFS schedulati intorno alle 23:00 di ogni giorno. EventBridge lancia una RUN COMMAND sulla macchina lanciando degli rsync verso EFS:</p> <ul> <li>WEB19</li> <li>WEB22</li> <li>WEB23</li> <li>WEB37</li> <li>WEB39</li> <li>WEB40</li> </ul> <p>L'EFS \u00e8 montato sotto ** /media/efs. Situazione /etc/fstab**:</p> Bash<pre><code># root\nLABEL=cloudimg-rootfs   /        ext4   defaults,discard        0 0\n\n/dev/nvme1n1 /media/support_disk/ ext4 defaults,discard 0 0\n/dev/nvme2n1p1 /media/disk2 ext4 defaults,discard 0 0\n/dev/nvme3n1p1 /media/support_disk/backup/qb/ ext4 defaults,discard 0 0\n\n###EFS\nefs-m1.consulman.internal:/ /media/efs efs _netdev,noresvport,tls 0 0\n</code></pre>"},{"location":"customers/consulman-C0422/Server/#thor","title":"Thor","text":"<ul> <li>AWS Console: Thor</li> </ul> <p>Questo \u00e8 un elenco di servizi attualmente in funzione sulla macchina:</p> <ul> <li>apache2 -&gt; porta 80/443/8080/8081</li> <li>teleport</li> <li>zabbix-agent</li> </ul>"},{"location":"customers/consulman-C0422/Server/#windows_dms","title":"windows_DMS","text":"<ul> <li>AWS Console: windows_DMS</li> </ul> <p>Server Windows, credenziali -&gt; Bisogna accedere tramite ponte ssh con il bastion host Contiene un'istanza SQL Server installata. Il server qb_import monta due cartelle:</p> Text Only<pre><code>c:/repository -&gt; /home/questionbin/www/tormento2/upload_allegati_j\nc:/repository_crm -&gt; /home/questionbin/www/crm_consulman/crm_dms_directory\n</code></pre>"},{"location":"customers/consulman-C0422/Server/#windows-gateway-1","title":"Windows-gateway-1","text":"<ul> <li>AWS Console: Windows-gateway-1</li> </ul> <p>Server Windows, credenziali -&gt; Bisogna accedere tramite ponte ssh con il bastion host Funge unicamente da gateway di comunicazione tra powerApps di Microsoft (attualmente sul tennant Consulman, poi su Coca Cola) e rdsv8.</p>"},{"location":"customers/consulman-C0422/Server/#poseidone_pre_migrated_recover","title":"Poseidone_pre_migrated_recover","text":"<ul> <li>AWS Console: Poseidone_pre_migrated_recover</li> </ul> <p>Questo \u00e8 un elenco di servizi attualmente in funzione sulla macchina:</p> <ul> <li>apache2 -&gt; porta 80/443</li> <li>bitbucket</li> <li>jira</li> <li>jasper-server</li> <li>amazon-ssm-agent</li> <li>nginx -&gt; porta 9080</li> <li>php7.2-fpm</li> <li>teleport</li> <li>zabbix-agent</li> </ul> <p>Dopo la migrazione del server Poseidone in una subnet privata dietro al bilanciatore, si sono verificati problemi con il Content-Length dei PDF, impedendone il download. Per risolvere il problema, \u00e8 stato necessario introdurre ngnix che forza il HTTP/1.1 facendolo accettare correttamente la richiesta ad apache2, scaricando il file richiesto.</p>"},{"location":"customers/criticalcase-c0001/","title":"Criticalcase srl @ C0001","text":""},{"location":"customers/criticalcase-c0001/Amministrazione/","title":"Amministrazione Criticalcase","text":""},{"location":"customers/criticalcase-c0001/Amministrazione/#email-sdi-gestionale","title":"Email SDI - Gestionale","text":"<p>Ricezione delle fatture passive dell\u2019interscambio SDI/gestionale</p> <p>Office365 non supporta pi\u00f9 l'autenticazione basic: Deprecation of Basic authentication in Exchange Online</p> <p>Pertanto gli strumenti dell'amministrazione non riescono pi\u00f9 a scaricare le fatture dalla vecchia mail <code>b2b@criticalcase.com</code>.</p> <p>Abbiamo quindi scelto di usare il cPanel dove risede www.criticalcase.com per creare delle caselle di posta tradizionali</p>"},{"location":"customers/criticalcase-c0001/Amministrazione/#dominio-extcriticalcasecom","title":"Dominio: ext.criticalcase.com","text":"<p>Accesso cPanel: https://elwing.to1.ccws.it:2087 (credentials)</p> <p>Accesso webmail: https://elwing.to1.ccws.it:2096 (credentials)</p>"},{"location":"customers/criticalcase-c0001/Amministrazione/#impostazione-di-ppenmanager","title":"Impostazione di PpenManager","text":"<p>Vedi lo screenshot:</p> <p></p>"},{"location":"customers/criticalcase-c0001/Amministrazione/#impostazione-di-aziendadigitalecloud","title":"Impostazione di aziendadigitale.cloud","text":"<p>Aggiornare la mail in: Gestione Azienda &gt; Critical case srl &gt; Lente di ingradimento &gt; Gestione Indirizzi posta Elettronica</p>"},{"location":"customers/criticalcase-c0001/Intellinyze/","title":"Intellinyze","text":"<p>Il progetto non \u00e8 gestito da ITO</p>"},{"location":"customers/criticalcase-c0001/Intellinyze/#lista-server","title":"Lista server","text":"Text Only<pre><code>abm-cl01-nod11 10.12.0.111 BC:30:5B:F0:55:CC 192.168.200.11 5c:f9:dd:d6:10:ce\nabm-cl01-nod12 10.12.0.112 BC:30:5B:F0:4B:A4 192.168.200.12 5c:f9:dd:d6:62:ec\nabm-cl01-nod13 10.12.0.113 BC:30:5B:EF:D4:AC 192.168.200.13 5c:f9:dd:d6:5b:d2\nabm-cl01-nod14 10.12.0.114 BC:30:5B:F0:46:E4 192.168.200.14 5c:f9:dd:d6:10:c8\nabm-cl01-nod15 10.12.0.115 BC:30:5B:F0:48:80 192.168.200.15 5c:f9:dd:d6:56:d0\nabm-cl01-nod16 10.12.0.116 BC:30:5B:F0:49:10 192.168.200.16 5c:f9:dd:d6:5b:a0\nabm-cl01-nod17 10.12.0.117 BC:30:5B:F0:45:9C 192.168.200.17 5c:f9:dd:d6:5b:56\nabm-cl01-nod18 10.12.0.118 BC:30:5B:F0:47:AC 192.168.200.18 5c:f9:dd:d6:57:36\nabm-cl01-nod19 10.12.0.119 BC:30:5B:F0:55:00 192.168.200.19 5c:f9:dd:d5:e0:82\nabm-cl01-nod20 10.12.0.120 BC:30:5B:F0:53:DC 192.168.200.20 5c:f9:dd:d6:10:0c\nabm-cl01-nod21 10.12.0.121 BC:30:5B:F0:50:A8 192.168.200.21 5c:f9:dd:d6:0d:c4\nabm-cl01-nod22 10.12.0.122 BC:30:5B:F0:43:30 192.168.200.22 5c:f9:dd:d6:0e:34\nabm-cl01-nod23 10.12.0.123 BC:30:5B:F0:52:44 192.168.200.23 5c:f9:dd:d6:57:02\nabm-cl01-nod24 10.12.0.124 BC:30:5B:F0:4C:C8 192.168.200.24 5c:f9:dd:d6:12:7a\nabm-cl01-nod25 10.12.0.125 BC:30:5B:F0:49:18 192.168.200.25 5c:f9:dd:d6:10:16\nabm-cl01-nod26 10.12.0.126 BC:30:5B:ED:8E:B4 192.168.200.26 5c:f9:dd:d6:eb:08\n</code></pre>"},{"location":"customers/criticalcase-c0001/Intellinyze/#script-bash","title":"Script bash","text":"<p>Replace password in INSERT_PASSWORD_HERE</p> Text Only<pre><code>#!/bin/bash\nwhile read -r line\ndo\n    FOR_NAME=$(echo $line | awk '{print $1}')\n    FOR_IP=$(echo $line | awk '{print $2}')\n    FOR_MAC=$(echo $line | awk '{print $3}')\n    FOR_BMC_IP=$(echo $line | awk '{print $4}')\n    FOR_BMC_MAC=$(echo $line | awk '{print $5}')\n\n    echo \"Creating $FOR_NAME (IP: $FOR_IP - MAC: $FOR_MAC)\";\n    hammer host create --hostgroup-id 45 --interface \"mac=$FOR_MAC\" --interface \"mac=${FOR_MAC}, ip=$FOR_IP, type=interface, subnet_id=28, managed=true, virtual=true, tag=703, attached_to=eth0, identifier=eth0.703\" --interface=\"mac=$FOR_BMC_MAC, ip=$FOR_BMC_IP, type=bmc, provider=ipmi, username=root, password=\\\"INSERT_PASSWORD_HERE\\\"\" --name $FOR_NAME\ndone &lt; line.txt\n</code></pre>"},{"location":"customers/criticalcase-c0001/Intellinyze/#script-per-dns","title":"Script per DNS","text":"Text Only<pre><code>for i in `seq 11 26`\ndo\n    echo \"abm-cl01-nod$i.local.ccws.it 600 IN A \"; hammer host info --name abm-cl01-nod$i.local.ccws.it | grep \": 10.101\" | awk '{print $3}'\ndone\n</code></pre>"},{"location":"customers/criticalcase-c0001/Intellinyze/#script-label","title":"Script label","text":"Text Only<pre><code>for i in `seq 11 26`\ndo\n    kubectl label node abm-cl01-nod$i node-role.kubernetes.io/compute=\ndone\n</code></pre>"},{"location":"customers/criticalcase-c0001/Intellinyze/#squid","title":"SQUID","text":"Text Only<pre><code>export http_proxy='https://abmstag:SUPERSECUREPASSWORD@abm-prx01.ccws.it:3129'\n</code></pre>"},{"location":"customers/criticalcase-c0001/Office365/","title":"Microsoft Office 365","text":"<ul> <li>Enable modern authentication: https://admin.microsoft.com/#/homepage/:/Settings/L1/ModernAuthentication</li> </ul> <p>POP / IMAP / SMTP</p> <ul> <li>View logins: https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/SignIns</li> </ul>"},{"location":"customers/daito-C2215/","title":"Daito Solutions s.r.l. @ C2265","text":"<p>Stiamo facendo l'analisi dei requsiti per il progetto load-balancer</p>"},{"location":"customers/diffusione-tessile-C1701/","title":"Diffusione Tessile S.r.l. @ C1701","text":""},{"location":"customers/diffusione-tessile-C1701/#ito-h24","title":"ITO H24","text":"<ul> <li>CR230352: MaxMara B2C in AWS - BIA</li> <li>CR230325: MaxMara B2C in Cina - Gestione infrastruttura Tencent</li> <li>CR230363: B2C in AWS - Moses (ex on-premise in BT)</li> </ul> <p>Sales: Alessandro Zoncu</p> <p>Internal data:</p> <ul> <li>Start date: 20/07/2021</li> <li>Sales: Alessandro Zoncu</li> <li>Phone: +39 3357899388</li> <li>Email: a.zoncu@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Luca Pelli</li> <li>Email: pelli.l@diffusionetessile.it</li> <li>Phone: +39 348 2810077</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/","title":"B2C in AWS - BIA","text":"<ul> <li>CR230352 Diffusione Tessile (MaxMara).</li> <li>Riferimento vecchio di fatturazione della societ\u00e0 MaxMara: CR230271</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/#operations","title":"Operations","text":"<ul> <li>Upgrade EKS nodes via Karpenter and AWS managed node-group. Each cluster has 3 worker node managed by node group. Other nodes are managed by Karpenter.</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/#accessi","title":"Accessi","text":"<ul> <li>AWS SSO</li> <li>JIRA Tickets</li> <li>Confluence WIKI EKS Runbook</li> <li>GitHub b2c-management</li> <li>EKS via Cloud9</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/#best-practice","title":"Best practice","text":"<ul> <li>Aggiungere il numero del ticket BIA-XXX nella commit</li> <li>Una volta rivisto il plan di terraform mettere: Review and add the comment \"Approved\"</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/#note","title":"Note","text":"<ul> <li>Le immagini di tutti i container vengono scaricati da un registry privato. C'\u00e8 una pipeline che sincornizza le immagini da docker hub. L'ECR si trova in b2c-cicd-prod</li> <li>Karpenter riavvia e aggiorna i nodi ogni 2 settimane. Karpenter gira sui managed node-group.</li> <li>Hanno due tipi di addons:</li> <li>managed by aws: coredns, cni, csi, guard duty agent</li> <li>not managed by aws: metrics server, external dns, alb, integration</li> <li>Il codice di adf si trova in b2c-adf-deployment.</li> <li>Bisogna rivedere i passaggi su come creare la workstation di cloud9 (Aggiungere SG e modificare iam role)</li> <li>Lo stato Blocker di JIRA \u00e8 il pi\u00f9 improtante, bisogna vedere tutto subito. Gli altri task non sono urgenti.</li> <li>(Terraform) In eks-cluster abbiamo una cartella tfvars che viene usata per impostare le variabili. Ma nulla vieta di mettere codice custom di terraform</li> <li>(Terraform) global.auto.tfvars viene applicato a tutti gli account, mentre entrando nell'account id posso fare override.</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/#workstation-configuration","title":"Workstation configuration","text":"<p>Docs moved to: https://maxmarafashiongroup.atlassian.net/wiki/spaces/BIA/pages/3919151165/Workstation+console+setup</p>"},{"location":"customers/diffusione-tessile-C1701/AWS-OnPremises-Moses/","title":"AWS OnPremises Moses","text":"<ul> <li>CR230363 Diffusione Tessile (MaxMara).</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-OnPremises-Moses/#operations","title":"Operations","text":"<ul> <li>Kill lock Oracle</li> <li>Abilitazioni regole bilanciatori</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-OnPremises-Moses/#accessi","title":"Accessi","text":"<ul> <li>AWS Granted - b2c-onprem</li> <li>JIRA Tickets</li> <li>Confluence WIKI</li> <li>GitHub b2c-management</li> <li>New Relic - Hybris Database Dashboard</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-OnPremises-Moses/#best-practice","title":"Best practice","text":"<ul> <li>Aggiungere il numero del ticket INFRA-XXX nella commit</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-OnPremises-Moses/#teleport","title":"Teleport","text":"<p>Access to the VMs is possible via federated teleport cluster:</p> <p>Select the cluster:</p> <ul> <li><code>tp.diffusionetessile-moses.criticalcasecloud.com</code></li> </ul> <p>E' possibile accedere ai servers anche tramite VPN in SSH. La lista degli ip \u00e8 nell'Ansible o su Confluence Sono gi\u00e0 presenti le nostre chiavi, per le private dei servers su Leaf o nei AWS Secrets Manager del cliente</p> <p>Le password delle utenze di sistema operativo sono nei AWS Secrets Manager del cliente</p>"},{"location":"customers/diffusione-tessile-C1701/AWS-OnPremises-Moses/#monitoraggio","title":"Monitoraggio","text":"<p>Documentazione dettagliata nel Confluence del cliente</p> <p>Il monitoraggio dell'infrastruttura \u00e8 fatta tramite CloudWatch al quale \u00e8 collegato un SNS il quale manda la notifica alla nostra solita lambda ITO e ci arrivano gli allarmi tramite email.</p> <p>Per il monitoraggio dei servers (CPU,RAM,DISCO), utilizziamo NewRelic del cliente che invia la notifica tramite EMAIL</p> <p>Esiste anche una Dashboard dedicata ad RDS</p> <p>In generale sono presenti Dashboard per vedere l'andamento dei servers/applicativo</p>"},{"location":"customers/diffusione-tessile-C1701/AWS-OnPremises-Moses/#note","title":"Note","text":"<ul> <li>Al momento il codice terraform e ansible si applicano proprio computer</li> <li>In generale i segreti sono nei AWS Secrets Manager del cliente</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/","title":"B2C in Tencent - Cina","text":"<p>CR230325 Diffusione Tessile (MaxMara)</p>"},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/#note-h24","title":"Note H24","text":"<ul> <li>12/06/2023: Abbiamo disattivato le notifiche verso digitalerts@criticalcase.com dato che il cliente non sta risolvendo il problema. Vedi ticket ITO-807</li> </ul> Text Only<pre><code># Non dovrebbe pi\u00f9 essere cos\u00ec\nil servizio hybris viene killato sui nodi CNA101, CNA102 e CNA201 in caso di riavvio del servizio teleport.\nIl cliente \u00e8 stato avvisato ed \u00e8 al corrente che deve usare la procedura corretta, visibile nel ticket allegato, che non genera questo problema.\nIn ogni caso, assicurarsi in caso di riavvio di teleport, che il servizio hybris sia attivo e, in caso negativo, riavviarlo tramite su - appuser e sudo systemctl start hybris.service.\nPer info https://criticalcase.atlassian.net/browse/ITO-199\n{.is-info}\n</code></pre>"},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/#data-center","title":"Data Center","text":"<ul> <li> AWS (solo Lambda per Snapshot management)</li> <li> Tencent: maxmara</li> </ul> Text Only<pre><code>Account Alias: MaxMara\nAccount ID: 200000158220\nRoot account: t.petronio@criticalcase.com\n</code></pre>"},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/#descrizione-ambito-funzionale-e-tecnologico","title":"Descrizione ambito funzionale e tecnologico","text":"<p>Il progetto \u00e8 stato rilasciato in data 20/09/2021</p> <p>B2C https://maxmara.cn per il mercato cinese</p> <p>Sono disponibili 2 differenti ambienti su Tencent, PROD ed UAT.</p> <p>Il servizio ITO \u00e8 attivo SOLO per gli ambienti di Produzione non per UAT</p>"},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/#accessi","title":"Accessi","text":""},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/#teleport","title":"Teleport","text":"<p>Tutte le vm sono accessibili tramite Teleport con interfaccia web o client tsh.</p> <ul> <li>Web interface Teleport Dedicata: https://b2c-teleport.linmara.com</li> </ul> <p>non sono disponibili accessi centralizzati, il teleport cinese \u00e8 dedicato ed isolato {.is-warning}</p> <ul> <li>Download Teleport from https://goteleport.com/teleport/download/</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/#vault-keys-to-access-teleport","title":"VAULT keys to access Teleport","text":"<ul> <li>https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/show/ds/customers/maxmara-C1362/teleport</li> </ul> <p>Altre credenziali diverse dall'accesso teleport</p> <ul> <li>https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/show/ds/customers/maxmara-C1362/tencent-AZ200513F</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/#tencent-cloud-access","title":"Tencent Cloud Access","text":"<p>Portal login</p> <ul> <li>https://intl.cloud.tencent.com/login   Usare l'account 200016379624   Login with Tencet account Credentials (for Critical Service available here https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/show/ds/customers/maxmara-C1362/tencent_criticalcase_operation_login)</li> </ul> <p>UAT Environment is NOT COVERED BY ITO</p>"},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/#production","title":"Production","text":""},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/#architecture","title":"Architecture","text":""},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/#components","title":"Components","text":"<p>VPC-Prod La piattaforma \u00e8 interamente implementata all'interno di una Network privata. I VPC di PROD ed UAT non comunicano tra di loro, ma entrambi sono in peering con il VPN del Bastion (Teleport) dove \u00e8 presente anche una macchina Control usata per la gestione della CI/CD da parte di Maxmara (\u00e8 stata messa qui perch\u00e9 deve raggiungere sia UAT che PROD)</p> <p>VMs Servers PROD https://console.intl.cloud.tencent.com/cvm/instance/index?vagueIp=10.1&amp;InstanceIpAddress=10.1&amp;rid=4 L'intera infrastruttura applicativa \u00e8 implementata su servizio IaaS ed \u00e8 accessibile tramite Teleport. Fanno eccezione i bilanciatori il db, l'object storage, l'NFS Non ci sono autoscaling group.</p> <p>Load Balancers PROD https://console.intl.cloud.tencent.com/clb/instance?rid=4&amp;keyword=prod mmaraprod-clbfe-cn is the one pointed by the CDN on maxmara.cn mmaraprod-clbfe is used for test purposes on b2cp.linmara.com mmaraprod-clbbe is an external pointing for Backend storebe.linmara.com mmaraprod-clbmule is an exposition of Mulesoft API on etlbe.linmara.com</p> <p>MySQL PaaS PaaS di MySQL, gi\u00e0 in HA, backup periodico marte\u00ec e venerd\u00ec e point-in-time recovery a 7 giorni PROD https://console.intl.cloud.tencent.com/cdb?ins=4-cdb-ai8no0qu</p> <p>Clod Filestore PROD https://console.intl.cloud.tencent.com/cfs/fs?VpcId=vpc-nzuxho06&amp;rid=4&amp;action=EditTag</p> <p>COS for log backup https://console.intl.cloud.tencent.com/cos5/bucket/setting?type=filelist&amp;bucketName=mmarabackup-1304124894&amp;path=&amp;region=ap-shanghai</p> <p>Other COS are not covered by ITO service</p>"},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/#elenco-midlleware-installati-sui-server","title":"Elenco Midlleware Installati Sui Server","text":""},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/#hybris-cna101-cna102-cna201-and-mule-cnb301","title":"Hybris (CNA101, CNA102, CNA201) and Mule (CNB301)","text":"<ul> <li>git</li> <li>TCCLI and coscmd</li> <li>Apache 2.4 with mod_proxy and mod_ssl</li> <li>java Oracle JDK 8u202</li> <li>ntpd daemon with Europe / Dublin timezone configuration</li> <li>local configuration en_US.UTF-8</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/#solr-cna301-cna302-cna303","title":"Solr (CNA301, CNA302, CNA303)","text":"<ul> <li>git</li> <li>TCCLI and coscmd</li> <li>java Oracle JDK 8u202</li> <li>ntpd daemon with Europe / Dublin timezone configuration</li> <li>local configuration en_US.UTF-8</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/#start-up-services","title":"Start-up Services","text":"<p>hybris-server service is available as systemd start-up on CNA1/2xx solr7-server service is available as systemd start-up on CNA3xx</p>"},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/#monitoring-logging","title":"Monitoring &amp; Logging","text":"<p>Everything is on Tencent</p> <ul> <li>Monitoring https://console.intl.cloud.tencent.com/monitor/overview   For VMs go in Monitoring section related to each VM on CVM Service</li> <li>Alerts https://console.intl.cloud.tencent.com/monitor/alarm2/policy</li> <li>Logging (FE Load Balancer only) https://console.intl.cloud.tencent.com/cls/search?region=ap-shanghai&amp;logset_id=ad7ce0d9-ec97-4c1f-ae88-e87bdc680a49&amp;topic_id=56cd0bce-bd3c-4c84-81cc-81184ac6921a&amp;filter=%255B%255D&amp;rainbowFilter=%255B%255D</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/#snapshot-policy","title":"Snapshot Policy","text":"<ul> <li>Questo link non \u00e8 direttamente rintracciabile nella console   https://console.intl.cloud.tencent.com/cvm/snapshot/asp/detail?rid=4&amp;id=asp-9qfop180&amp;searchParams=cmlkPTQ</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/TENCENT-B2C/#bastion-vm","title":"Bastion VM","text":"<p>La macchina Bastion \u00e8 una VM hardened usata per permettere un accesso SSH al VPC privato. Solo sshuser pu\u00f2 accedere al server usando la chiave ssh. I permessi di sshuser sono solo quelli necessari a permettere un ulteriore ssh verso altre macchine, tutti gli altri comandi sono bloccati. L'accesso alla rete privata virtuale pu\u00f2 passare attraverso un tunnel SSH Bastion nel caso in cui Teleport non funzionasse correttamente.</p> <p>Utilizzare sempre Teleport per gli accessi alle VM, sshuser \u00e8 solo per emergenze dovute a malfunzionamenti di Teleport</p> <p>Terraform Code (da utilizzare solo per gli ambienti di Prod): https://github.com/criticalcase/terraform_tencent_maxmara</p> <p>Documentazione di Progetto (accesso riservato al gruppo Digital Solutions) https://criticalcaseazure.sharepoint.com/:w:/r/sites/DigitalSolution/_layouts/15/Doc.aspx?sourcedoc=%7BF58FF080-D952-4288-90C2-4F238B3D3CEE%7D&amp;file=MaxMara_LLD_Architecture.0.4.docx&amp;action=default&amp;mobileredirect=true</p>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/","title":"Brand onboarding","text":"<ul> <li>https://maxmarafashiongroup.atlassian.net/wiki/spaces/BIA/pages/3910500448/00.+Prerequisites</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#notes","title":"Notes","text":"<p>Important notes to be aware</p> <p>Every time we deploy things via terraform, check multiple times for <code>Plan:</code> in the log to see all the changes. Every pipeline has multiple terrafrom code execution.</p>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#hint","title":"Hint","text":"<ul> <li>Account pipeline is in b2c-management-account</li> <li>Terraform pipelines are in b2c-adf-deployment</li> <li>Workload pipelines are in b2c-cicd-prod</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#accounts","title":"Accounts","text":"<p>Accounts are created and managed by b2c-management-account.</p> <ul> <li> <p><code>[assume -c b2c-management-account]</code> Add the configuration for 6 AWS accounts. Approval is automatic as it's managed by CloudFormation. codecommit aws-deployment-framework-bootstrap</p> </li> <li> <p><code>[assume -c b2c-management-account]</code> Once the code for creating the account is committed, please verify</p> </li> <li> <p>statemachine AccountBootstrappingStateMachine-HCZxJ2DA7ACE</p> </li> <li> <p>codepipeline aws-deployment-framework-base-templates</p> </li> </ul> <p>The pipeline is failing on operations in other regions due to SCP restrictions. For now, it can be ignored. https://us-east-1.console.aws.amazon.com/codesuite/codebuild/081734585140/projects/aws-deployment-framework-base-templates/build/aws-deployment-framework-base-templates%3A2baf8217-03ed-4d63-9b2e-0e4af606249e/?region=us-east-1</p>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#sso","title":"SSO","text":"<p>You won't see the newly created AWS accounts in the list until permissions are added.</p> <ul> <li> <p><code>[assume -c b2c-adf-source]</code> Add permissions codecommit sso-tf</p> </li> <li> <p><code>[assume -c b2c-adf-deployment]</code> Check permissions plan and approve: codepipeline adf-pipeline-sso-tf <code>(Plan: 48 to add, 0 to change, 0 to destroy.)</code></p> </li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#vpc","title":"VPC","text":"<p>Only 3 VPCs are used in the network account, one for each environment: test, uat and prod</p> <ul> <li>Check the IP of the last created brand and add 4 from the last created subnet:</li> </ul> Text Only<pre><code>Workload accounts VPCs IP plan (weekendmaxmara.com)\n\nb2c-weekend-hybris-test   - 10.17.24.0/22\nb2c-weekend-magnolia-test - 10.17.28.0/22\nb2c-weekend-hybris-uat    - 10.17.32.0/22\nb2c-weekend-magnolia-uat  - 10.17.36.0/22\n\nb2c-weekend-hybris-prod   - 10.16.16.0/22\nb2c-weekend-magnolia-prod - 10.16.20.0/22\n</code></pre> <ul> <li>So we can use:</li> </ul> Text Only<pre><code>b2c-newbrand-*-test and b2c-newbrand-*-uat start from 10.17.40.0/22\nb2c-newbrand-*-prod starts from 10.16.24.0/22\n</code></pre> <ul> <li> <p><code>[assume -c b2c-network]</code> Verify that the addressing is not already in use: Route tables</p> </li> <li> <p><code>[assume -c b2c-adf-source]</code> Add networking configuration: codecommit vpc</p> </li> <li> <p><code>[assume -c b2c-adf-deployment]</code> Check permissions plan and approve: codepipeline adf-pipeline-vpc</p> </li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#elasticache","title":"Elasticache","text":"<ul> <li>https://maxmarafashiongroup.atlassian.net/wiki/spaces/BIA/pages/3924918308/elasticache-tf+-+Add+new+brand</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#rdsdb","title":"RDSDB","text":"<ul> <li>https://maxmarafashiongroup.atlassian.net/wiki/spaces/BIA/pages/3924590617/rdsdb-tf+-+Add+new+brand</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#magnoliaci-tf","title":"magnoliaci-tf","text":""},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#ask-before-start","title":"Ask before start","text":"<ul> <li>Call with Stefano Rocca</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#deploy-changes","title":"Deploy changes","text":"<ul> <li> <p><code>[assume -c b2c-adf-source]</code> Add the terraform configuration codecommit magnoliaci-tf</p> </li> <li> <p><code>[assume -c b2c-adf-deployment]</code> Please verify that everything is in order and then approve. codepipeline adf-pipeline-magnoliaci-tf - history</p> </li> <li> <p><code>[assume -c b2c-cicd-prod]</code> Check the pipeline, should fail at PRE_BUILD stage: https://eu-west-1.console.aws.amazon.com/codesuite/codepipeline/pipelines/tf-b2c-magnolia-ci-intrend-test/view?region=eu-west-1 - history - failed1.</p> </li> </ul> <p>If you have an ACCESS_DENIED at PROVISIONING stage you should relaunch the pipeline. failed2</p> <ul> <li>Andare sul repository https://github.com/b2c-management/b2c-magnoliacms-app</li> <li>Chidere a Stefano Rocca da quale branch partire, esempio: <code>release-2023.16</code></li> </ul> Bash<pre><code>git checkout release-2023.16\ngit branch feature_BIAXXXX_github_action_intrend\ngit checkout feature_BIAXXXX_github_action_intrend\n</code></pre> <ul> <li> <p>Aggiungere il brand al github actions workflow. Questo permetter\u00e0 agli sviluppatori di triggerare codepipeline. Non possiamo testarlo direttamente. https://github.com/b2c-management/b2c-magnoliacms-app/commit/0755b7e70787603d88cb58a007f7d0cfeb861a9e</p> </li> <li> <p>Fare il merge sul branch release da stefano rocca</p> </li> </ul> Bash<pre><code>git checkout release-2023.16\ngit merge feature_BIAXXXX_github_action_intrend\n</code></pre> <ul> <li>Copiare il file da un altro brand e modificare i valori dell'helm chart</li> </ul> <p>Questi verranno usati al primo deploy da: codepipeline tf-b2c-magnolia-ci-intrend-test</p> <ul> <li>... attendere il primo deploy da Stefano Rocca ...</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#magnoliacd-tf","title":"magnoliacd-tf","text":"<ul> <li>https://maxmarafashiongroup.atlassian.net/wiki/spaces/BIA/pages/3861807173</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#sapcommerceappcd-tf","title":"sapcommerceappcd-tf","text":"<ul> <li>https://maxmarafashiongroup.atlassian.net/wiki/spaces/BIA/pages/3862364161</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#ekscluster-tf","title":"ekscluster-tf","text":"<p>EKS is used by magnolia and hybris.</p>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#ask-before-start_1","title":"Ask before start","text":"<p>Apply in the pipeline could have a different commit id. Check before approve.</p> <ul> <li><code>NewRelic_Key</code> to Andrea Guerzoni</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#deploy-for-test","title":"Deploy for test","text":"<ul> <li> <p>https://maxmarafashiongroup.atlassian.net/wiki/spaces/BIA/pages/3925114892/ekscluster-tf+-+Add+new+brand</p> </li> <li> <p>Manually create 4 secrets in b2c-BRAND-magnolia-test: <code>assume -c b2c-intrend-magnolia-test</code>: https://eu-west-1.console.aws.amazon.com/secretsmanager/listsecrets?region=eu-west-1:</p> </li> <li> <p>test-mgnl-author-secrets</p> </li> <li>test-mgnl-shared-secrets</li> <li>test-mgnl-publication-keys</li> <li> <p>test-mgnl-public-secrets</p> </li> <li> <p>Manually create 2 secrets in b2c-BRAND-hybris-test: <code>assume b2c-intrend-hybris-test -c</code>:</p> </li> <li> <p>hbrs-mule-private-key-passphrase</p> </li> <li>hbrs-mule-private-key</li> </ul> <p>Make sure you're using the kube-proxy version that is matching to your cluster version. Check here: https://docs.aws.amazon.com/eks/latest/userguide/managing-kube-proxy.html</p> <p>{.is-warning}</p> <p>If some helm charts have failed you have to cleanup: i.e.</p> Bash<pre><code>helm uninstall newrelic-nri-bundle -n newrelic\n</code></pre>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#vpc-ingress-env","title":"vpc-ingress-env","text":"<ul> <li> <p><code>[assume -c b2c-source]</code> (https://eu-west-1.console.aws.amazon.com/codesuite/codecommit/repositories/vpc-ingress-test/commit/f6b6cf3de26497ea94e0145a57b0b0e7e34a2e98?region=eu-west-1)</p> </li> <li> <p><code>[assume -c b2c-adf-deployment]</code> https://eu-west-1.console.aws.amazon.com/codesuite/codepipeline/pipelines/adf-pipeline-vpc-ingress-test/view?region=eu-west-1 history</p> </li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#dnszone-tf","title":"dnszone-tf","text":"<ul> <li>https://maxmarafashiongroup.atlassian.net/wiki/spaces/BIA/pages/3924721677/dnszone-tf+-+Add+new+brand</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#solrcd-tf","title":"solrcd-tf","text":"<ul> <li>https://maxmarafashiongroup.atlassian.net/wiki/spaces/BIA/pages/3861872660/SOLR+CD+-+Add+new+brand</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#sapcommerceapplambda-tf","title":"sapcommerceapplambda-tf","text":"<ul> <li>https://maxmarafashiongroup.atlassian.net/wiki/spaces/BIA/pages/3929768021/sapcommerceapplambda-tf+-+Add+new+brand</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#issues","title":"Issues","text":""},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#release-the-terraform-lock","title":"Release the terraform lock","text":"<ul> <li> <p><code>[assume -c b2c-adf-deployment]</code> Search the LockID in DynamoDB: https://eu-west-1.console.aws.amazon.com/dynamodbv2/home?region=eu-west-1#item-explorer?filter1Comparator=CONTAINS&amp;filter1Name=LockID&amp;filter1Type=S&amp;filter1Value=eks&amp;maximize=true&amp;operation=SCAN&amp;table=adf-tflocktable</p> </li> <li> <p>Scan with a Filter: LockID contains <code>terraform-pipeline-name</code></p> </li> <li> <p>Select the item &gt; Action &gt; Delete item</p> </li> <li> <p>If you see changes on <code>resource \"aws_sns_topic_subscription\"</code>, probably is related to an unconfirmed email for the sns subscription.</p> </li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#todo","title":"ToDo","text":""},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#da-implementare","title":"Da implementare","text":"<ul> <li>Implementare su ekscluster-tf la pre-creazione dei segreti con la lifecycle rule di ignorare i cambiamenti.</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#da-discutere","title":"Da discutere","text":"<ul> <li>L'approve di terrafrom su una commit di test non dovrebbe dovrebbe essere applicata anche in produzione.</li> <li>On terraform add the account alias: Running terraform apply on account 560002725494 and region eu-west-1: For example we could add in this file: adf_terraform</li> </ul> Bash<pre><code>\u2570\u2500\u276f echo \"Account Alias: $(aws iam list-account-aliases | jq -r '.AccountAliases .[]')\"\nAccount Alias: b2c-marella-magnolia-test\n</code></pre> <ul> <li>Come procediamo quando abbiamo pi\u00f9 modifiche da applicare sui cluster eks? Ad esempio da test a uat? Adesso abbiamo due modifiche pending metric server e karpenter. Quindi la pipeline di eks-uat applicher\u00e0 entrambe le modifiche una volta approvate.</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-BIA-B2C/onboarding/#implementati","title":"Implementati","text":"<ul> <li>In eks bisogna aggiungere manulamente host_zone_id and vpc_id</li> <li>Add pre-commit to format the code before push https://maxmarafashiongroup.atlassian.net/browse/BIA-1437</li> <li>Add an empty files with the account name, could be useful to know the aws account without open any files (*-tf repo)</li> <li>Separate the brand/environment tfvars configuration into two distinct files. https://maxmarafashiongroup.atlassian.net/browse/BIA-1438</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-OnPremises-Moses/Abilitazione-Regole-Bilanciatori/","title":"Abilitazione Regole Bilanciatori","text":"<p>Possono arrivare richieste per aprire ip nel security groups del bilanciatore o aggiungere DNS alle rules dei bilanciatori come questi tickets:</p> <p>ITO-1056, per risolvere COMMIT sul branch develop ITO-1054, per risolvere COMMIT sul branch develop</p> <p>in sostanza bisogna aggiungere le rules al file alb.tf aggiungendo le regole, cambiando la priorit\u00e0 seguendo lo schema in quanto sono separate per gruppo di servers:</p> <p>Staging:</p> Bash<pre><code>* 100-199 mule\n* 200-299 backends\n* 300-399 stargate\n* 400-499 web-services\n* 500-449 frontends\n</code></pre> <p>Produzione:</p> Bash<pre><code>* 10-100 redirect\n* 100-199 mule\n* 200-299 backends\n* 300-399 stargate\n* 400-499 web-services\n* 500-449 frontends IEA101-IEA103-IEA113\n* 550-599 frontends IEA104-IEA106-IEA114\n* 650-649 frontends IEA110-IEA112-IEA116\n</code></pre>"},{"location":"customers/diffusione-tessile-C1701/AWS-OnPremises-Moses/Oracle-Locks/","title":"Oracle Procedures","text":""},{"location":"customers/diffusione-tessile-C1701/AWS-OnPremises-Moses/Oracle-Locks/#pre-requisites","title":"Pre-requisites","text":"<p>Install sqlcl:</p> <ul> <li>mac: https://formulae.brew.sh/cask/sqlcl</li> </ul>"},{"location":"customers/diffusione-tessile-C1701/AWS-OnPremises-Moses/Oracle-Locks/#connection","title":"Connection","text":"<p>Accedere al database hybris-prod-db <code>hybris-prod-db.cgp8lcayjf8x.eu-west-1.rds.amazonaws.com</code> nell'account b2c-onprem con l'utente ccadmin (credentials).</p> Bash<pre><code>sql /nolog\n</code></pre> SQL<pre><code>CONNECT username/password@hostname:port/servicename\n# You can find the service name in the configuration TAB under DB name https://eu-west-1.console.aws.amazon.com/rds/home?region=eu-west-1#database:id=hybris-prod-db;is-cluster=false;tab=configuration\n\n# i.e.\nCONNECT ccadmin/PASSWORD@hybris-prod-db.cgp8lcayjf8x.eu-west-1.rds.amazonaws.com:1521/HYBPRD\n</code></pre>"},{"location":"customers/diffusione-tessile-C1701/AWS-OnPremises-Moses/Oracle-Locks/#kill-oracle-locks","title":"Kill Oracle Locks","text":"<p>Procedura per eliminare una sessione del database Oracle che sta generando un lock:</p> <ol> <li>Verificare quale sessione sta generando il blocco utilizzando la query:</li> </ol> SQL<pre><code>SELECT BLOCKING_SESSION,count(0)\nFROM v$session WHERE event LIKE '%TX%'\nGROUP BY BLOCKING_SESSION;\n</code></pre> <ol> <li>Catturare la blocking_session dalla query eseguita in precedenza ed eseguire una nuova query per verificare che lo STATO della sessione sia INATTIVO, passando la blocking_session . Se lo stato della sessione \u00e8 ATTIVO, NON \u00c8 POSSIBILE RIMUOVERE LA SESSIONE, per far sapere ad Andrea Guerzoni che una sessione attiva sta generando un blocco.</li> </ol> SQL<pre><code>SELECT sid,serial#,status FROM v$session WHERE sid = {BLOCKING_SESSION}\n</code></pre> <ol> <li>Catturare il sid e il serial# nella query precedente e chiudere la sessione con il comando seguente, passando il valore di sid e serial#:</li> </ol> SQL<pre><code>BEGIN\n    rdsadmin.rdsadmin_util.kill(\n        sid    =&gt; {sid},\n        serial =&gt;   {serial#},\n        method =&gt; 'IMMEDIATE');\nEND;\n/\n</code></pre> <ol> <li>Eseguire nuovamente il passaggio 2 per verificare che non vi sia pi\u00f9 alcun lock. Spesso accade che, dopo aver eliminato una sessione che genera il blocco, ne compaiano altre non appena ne viene eliminata una. In questo caso, \u00e8 necessario eseguire nuovamente tutti i passaggi per eliminare la sessione.</li> </ol> <p>Questo flusso deve essere ripetuto finch\u00e9 la query del passo 2 non restituisce pi\u00f9 valori.</p>"},{"location":"customers/diffusione-tessile-C1701/AWS-OnPremises-Moses/Oracle-Locks/#kill-oracle-query","title":"Kill oracle query","text":"<ol> <li>Verificare quale sessione sta generando il blocco utilizzando la query:</li> </ol> SQL<pre><code>SELECT sid,serial#,sql_id,last_call_et/60/60,event FROM v$session WHERE  status='ACTIVE'\n# If you need, add the source server to the query `AND machine='IEA201'`\n</code></pre> <ol> <li>Usare il <code>sid</code> per verificare quali query sono in esecuzione</li> </ol> SQL<pre><code> SELECT * FROM v$sqltext WHERE sql_id='181skhyu3nz77' ORDER BY PIECE\n</code></pre> <ol> <li>Catturare il sid e il serial# nella query precedente e chiudere la sessione con il comando seguente, passando il valore di sid e serial#:</li> </ol> SQL<pre><code>BEGIN\n    rdsadmin.rdsadmin_util.kill(\n        sid    =&gt; {sid},\n        serial =&gt;   {serial#},\n        method =&gt; 'IMMEDIATE');\nEND;\n/\n</code></pre>"},{"location":"customers/dinova-C2475/","title":"Dinova S.r.l. (ex Hibo) @\u00a0C2475","text":"<p>ex. HIBO S.r.l. @ C2304</p>"},{"location":"customers/dinova-C2475/#ito-h24","title":"ITO H24","text":"<ul> <li>CR210753: Fondazione MAST www.mast.org, www.mastphotogrant.com</li> </ul> <p>Internal data:</p> <ul> <li>Start date: 21/12/2021</li> <li>Sales: Marina Micelli</li> <li>Phone: +39 011 11111111</li> <li>Email: m.micelli@criticalcase.com</li> </ul>"},{"location":"customers/dinova-C2475/#architettura","title":"Architettura","text":""},{"location":"customers/dinova-C2475/#data-center","title":"Data Center","text":"<ul> <li> Critical Case RZ1 https://vc6-is1.rz1.vdc.ccws.it</li> </ul> <p>Repository ansible: https://github.com/criticalcase/hibo-mast</p>"},{"location":"customers/dinova-C2475/#access","title":"Access","text":""},{"location":"customers/dinova-C2475/#teleport","title":"Teleport","text":"<p>All the vm can be accessed through Teleport with web interface or tsh client.</p> <ul> <li> <p>Web interface (CC cluster):</p> </li> <li> <p>https://cc-ds1.criticalcasecloud.com</p> </li> <li> <p>https://cc-ds2.criticalcasecloud.com</p> </li> <li> <p>Web interface (Teleport Server): https://tp.cc-hibo-mast.criticalcasecloud.com:3080</p> </li> </ul> <p>Credentials:</p> <ul> <li>https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/show/ds/customers/hibo-C2304/mast/teleport</li> </ul>"},{"location":"customers/divitech-C0814/","title":"Divitech SPA @ C0814","text":""},{"location":"customers/divitech-C0814/#ito-h24","title":"ITO H24","text":"<ul> <li>CR220556: Helpme su AWS (helpme.divitech.it)</li> </ul> <p>Internal data:</p> <ul> <li>Sales: Morena Scudieri</li> <li>Phone: +39 366 777 6512</li> <li>Email: m.scudieri@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Roberto Manzoni</li> <li>Email: rmanzoli@divitech.it</li> <li>Phone: +39 348 840 2777</li> </ul>"},{"location":"customers/divitech-C0814/Helpme/","title":"Divitech Helpme","text":"<p>helpme.divitech.it - CR220556</p>"},{"location":"customers/divitech-C0814/Helpme/#data-center","title":"Data center","text":"<ul> <li> AWS: divitech-spa</li> </ul> Text Only<pre><code>Account Alias: divitech-spa\nAccount ID: 669182661768\n</code></pre>"},{"location":"customers/divitech-C0814/Helpme/#teleport","title":"Teleport","text":"<p>Access to the VMs is possible via federated teleport cluster:</p> <p>Select the cluster:</p> <ul> <li><code>tp.helpme-divitech.criticalcasecloud.com</code></li> </ul>"},{"location":"customers/divitech-C0814/Helpme/#architettura","title":"Architettura","text":""},{"location":"customers/divitech-C0814/Helpme/#note","title":"Note","text":"<ul> <li>Il sito \u00e8 protetto da autenticazione tramite Google, il path di accesso \u00e8: https://helpme.divitech.it/tea</li> <li>E' presente per l'health check anche questo path raggiungibile: https://helpme.divitech.it/health-check.php</li> <li>I backup sono automatizzati da AWS Backup</li> <li>Le credenziali sono sotto Sherlock</li> <li>Il progetto \u00e8 deployato tramite Terraform Cloud Prd. Il codice \u00e8 su GitHub</li> <li>Essendo in ITO, l'account \u00e8 controllato sia dalle Lambda in cc-digital</li> <li>Le risorse sono sotto monitoring tramite Cloudwatch Alarms e Zabbix</li> <li>Controllare Alerta per allarmi derivanti da Cloudwatch Credenziali</li> <li>Il cliente effettua lui il deploy del codice in autonomia e si \u00e8 occupato di installare i servizi quali apache2, il loro codice si trova all'interno di container. Utilizzano un certificato self-signed creato da loro, utilizzato da mellon per l'autenticazione con Google</li> </ul>"},{"location":"customers/divitech-C0814/Helpme/#network","title":"Network","text":"<p>Esiste un unica VPC. Ci sono 3 subnets private e 2 subnets pubbliche. Le subnet pubbliche utilizzano tutte lo stesso Internet Gateway e le private lo stesso Nat Gateway. Non ci sono limiti di raggiungibilit\u00e0 sulle NACL delle subnets, ma \u00e8 gestito tutto dai Security Groups</p>"},{"location":"customers/divitech-C0814/Helpme/#prd","title":"Prd","text":"<p>L'ambiente di produzione \u00e8 altamente affidabile. Tutto quello che era possibile criptare on at Rest \u00e8 stato fatto con chiavi di default di AWS, mantere questa convenzione. Esiste un Cloudfront. Esiste un Custom Header per far si che il bilanciatore accetti il traffico solamente dal Cloudfront e non direttamente (il traffico tra di essi \u00e8 cifrato). Al Cloufront \u00e8 associato un WAF al quale sono associate regole di rate-limit, ip permessi e ip malevoli (aggiornati ogni ora dalle nostre lambda in cc-digital). Al Waf \u00e8 associato un Kinesis che ha il compito di inviare i logs al Bucket S3. On-Premise Logstash recupera questi logs e li invia al nostro Kibana(Ricordarsi di selezionare l'host corretto). Dietro al bilanciatore \u00e8 presente un HAProxy in Autoscaling che svolge da active-passive sui due frontends EC2 su due AZ differenti. La configurazione di HAProxy e QUI in Ansible Il database \u00e8 su RDS. Il DNS interno \u00e8 gestito da Route53</p>"},{"location":"customers/energie-C0971/","title":"Energie S.r.l. @ C0971","text":""},{"location":"customers/energie-C0971/#ito-lite","title":"ITO LITE","text":"<ul> <li>CR191083: lesacoutlet.it</li> <li>CR200569: CDN cdnetworks.com</li> </ul> <p>Le ore di assistenza vanno SEMPRE rendicontate su Clockify</p> <p>Internal data:</p> <ul> <li>Start date: 00/00/2021</li> <li>Sales: Marina Micelli</li> <li>Phone: +39 011 11111111</li> <li>Email: m.micelli@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Paolo Ghilardi</li> <li>Email: paolo.ghilardi@nrgnet.it</li> <li>Phone: +39 339 1359532</li> </ul>"},{"location":"customers/energie-C0971/#aws","title":"AWS","text":"Text Only<pre><code>Account: 196898199590\nAlias: energie\nEmail: info@nrgnet.it\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/energie-C0971/CDN/","title":"Energie S.r.l. @C0971","text":"<ul> <li>CR200569: CDN cdnetworks.com</li> </ul>"},{"location":"customers/energie-C0971/CDN/#domains-with-cdn","title":"Domains with cdn","text":"<ul> <li>www.lesacoutlet.co.uk</li> <li>www.lesacoutlet.es</li> <li>www.lesacoutlet.fr</li> <li>www.lesacoutlet.de</li> <li>www.lesacoutlet.it</li> <li>www.lesacoutlet.ro</li> <li>www.lesacoutlet.uk</li> </ul>"},{"location":"customers/energie-C0971/CDN/#panel","title":"Panel","text":"<ul> <li> <p>New panel: https://login.cdnetworks.com/cas/login (credentials)</p> </li> <li> <p>Old panel: https://control.cdnetworks.com/ (credentials)</p> </li> </ul>"},{"location":"customers/energie-C0971/CDN/#flush-cache","title":"Flush Cache:","text":"<p>If you need to clear the cache on lesac sites, follow this procedure:</p> <ol> <li>Connect to https://control.cdnetworks.com/</li> <li>In the navigation bar, link at the top \"Content Acceleration\" section \"Flush\" - \"Flush Tools\"</li> <li>In \"Select a PAD\" select the first of the \"lesac\" sites in the list. Multiple selection is not possible.</li> <li>In \"Scope of Flush\" select \"Flush All\"</li> <li>Enter your email address (the default is the default user) and confirm with the \"Flush\" button</li> <li>Check the \"Result\", must be \"Success 200\"</li> <li>Repeat 3 -&gt; 6 for all remaining sites.</li> </ol> <p>No further action is required.</p>"},{"location":"customers/energie-C0971/CDN/#ssl-certificate-update","title":"Ssl certificate update","text":"<p>if you need to update the certificates of one or more domains on the cdn, you need to follow one of these two steps (1.a or 1.b) and update the certificate on the load balancer <code>prd-lesac-lb-01</code>.</p>"},{"location":"customers/energie-C0971/CDN/#1a-open-ticket","title":"1.a) Open ticket","text":"<p>https://helpcenter.cdnetworks.com/hc/en-us/requests/new</p> <p>attaching the new certificates and private keys for security.</p>"},{"location":"customers/energie-C0971/CDN/#1b-upload-the-new-certificate-from-the-panel","title":"1.b) Upload the new certificate from the panel","text":"<p>https://portalcdn.cdnetworks.com/v2/index#/certificate/add?code=cert_mylist&amp;parentCode=CertManagement&amp;productCode=certificatemanagement</p>"},{"location":"customers/energie-C0971/CDN/#2-update-the-certificate-on-the-load-balancer-prd-lesac-lb-01","title":"2 Update the certificate on the load balancer prd-lesac-lb-01","text":"<p>Remember that they must also be updated on the balancer (the certificate bundle must be installed). To verify that the private keys and certificates match, run the following commands:</p> Text Only<pre><code>root@prd-lesac-lb-01:/etc/nginx/ssl# openssl x509 --noout -modulus -in lesacoutlet.uk.crt |openssl md5\n(stdin)= 5da29ba5aeefd62aafebde0955f7964c\nroot@prd-lesac-lb-01:/etc/nginx/ssl# openssl rsa --noout -modulus -in lesacoutlet.uk.key |openssl md5\n(stdin)= 5da29ba5aeefd62aafebde0955f7964c\n</code></pre> <p>If the md5 hash matches, the key and the certificates are correct. On the balancer check also with:</p> Text Only<pre><code>root@prd-lesac-lb-01:/etc/nginx/ssl# nginx -t\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok\nnginx: configuration file /etc/nginx/nginx.conf test is successful\n</code></pre> <p>before reloading the nginx service.</p>"},{"location":"customers/energie-C0971/ItalianDesignBrands/","title":"ItalianDesignBrands","text":""},{"location":"customers/energie-C0971/ItalianDesignBrands/#aws","title":"AWS","text":"Text Only<pre><code>Account: 351011807555\nAlias: italiandesignbrands\nEmail: p.pastore@italiandesignbrands.com\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/energie-C0971/LESACOUTLET/","title":"Energie S.r.l. @C0971","text":"<ul> <li>CR191083: lesacoutlet.it</li> </ul>"},{"location":"customers/energie-C0971/LESACOUTLET/#accesses","title":"Accesses","text":"<p>All VM are on https://vc6-is1.rz1.vdc.ccws.it Vmware Virtual Center. Credentials can be found on: https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/list/ds/customers/Energie-C0971/</p>"},{"location":"customers/energie-C0971/LESACOUTLET/#teleport","title":"Teleport","text":"<p>All the vm can be accessed through Teleport with web interface or tsh client.</p> <ul> <li>Web interface (CC cluster): https://cc-ds1.criticalcasecloud.com</li> <li>Web interface (Teleport Server): https://tp.cc-energie-lesac.criticalcasecloud.com</li> <li>Download Teleport from https://goteleport.com/teleport/download/</li> </ul> <p>tsh Login</p> Text Only<pre><code>tsh login --proxy=cc-ds1.criticalcasecloud.com --auth=local --user=my_teleport_user\n</code></pre> <p>tsh Login GitHub (open the link after the tsh login command to authorize login)</p> Text Only<pre><code>tsh login --proxy=cc-ds1.criticalcasecloud.com --auth=github --user=my_github_user\n</code></pre> <p>tsh list cluster nodes</p> Text Only<pre><code>tsh ls\n</code></pre> <p>tsh ssh</p> Text Only<pre><code>tsh ssh root@nodename\n</code></pre> <p>tsh scp</p> Text Only<pre><code>tsh scp test.txt root@nodename:/root/\n</code></pre>"},{"location":"customers/energie-C0971/LESACOUTLET/#production","title":"Production","text":""},{"location":"customers/energie-C0971/LESACOUTLET/#architecture","title":"Architecture","text":""},{"location":"customers/energie-C0971/LESACOUTLET/#components","title":"Components","text":"<ul> <li>Tutte le versioni del middleware sono quelle di default degli archivi Ubuntu 20.04. Non ci sono porting o repository terzi.</li> </ul> <p>prd-lesac-lb-01 \u00e8 il load balancer e reverse-proxy. Ha uno script iptables /etc/network/firewall.sh per il redirect e l\u2019apertura di alcune porte postgresql, http/https, sftp, masquerading per i servers alle sue spalle, infatti \u00e8 anche il default gateway per le macchine di produzione e dev. OS: Ubuntu 20.04 Public IP: 176.221.51.45, 176.221.51.225 Customer LAN IP: 10.0.2.254</p> <p>prd-lesac-outlet-fe-01 \u00e8 il front end o nodo,ha apache2.4 in mpm-event, php7.4 in fpm, php.ini per salvare le sessioni sul server memcache 10.0.2.40. Proftpd per sftp (non ftp) con utente virtuale \u201clesac\u201d che ha come home la /var/www. I siti abilitati su questa macchina sono:</p> <ul> <li>www.exclusive.lesacoutlet.it</li> <li>www.lesacmilano.it</li> <li>www.lesacoutlet.it</li> </ul> <p>Per gli script e gli errori php \u00e8 gi\u00e0 attivo un smtp smarthost che manda email come prd-lesac-outlet-fe-01@mtaccloud.criticalcase.com. (si veda SMTP) C'\u00e8 un cron (crontab -l) che si occupa di pulire la cache delle immagini in /cache (/var/www/lesacoutlet.it/www/admin/cron/clean_img_cache.sh) OS: Ubuntu 20.04 Customer LAN IP: 10.0.2.20</p> <p>prd-lesac-outlet-mc-01 \u00e8 il server memcache sia per le sessioni php del front end che admin, sia per le query del db. Aumentate le dimensioni dei record e messo 2GB di ram dedicata su memcached. OS: Ubuntu 20.04 Customer LAN IP: 10.0.2.40</p> <p>prd-lesac-outlet-db-01 \u00e8 il db aggiornato alla versione 12 di postgresql. OS: Ubuntu 20.04 Customer LAN IP: 10.0.2.30</p> <p>prd-lesac-outlet-admin-01 \u00e8 l\u2019admin per il sito. Ha la stessa identica configurazione php e apache del front end. Ci sono le fatture nel path /fatture. Usa memcached 10.0.2.40 e db 10.0.2.30, salva le sessioni su memcached. I siti abilitati sono admin.lesacoutlet.it e adminnew.lesacoutlet.it. C'\u00e8 installato e configurato anche il driver per mssql con le configurazioni odbcper parlare con il server 176.221.49.24 (il gestionale Windows). Anche qua c\u2019\u00e8 Proftpd per sftp (non ftp) con utente virtuale \u201clesac\u201d che ha come home la /var/www. Per gli script e gli errori php \u00e8 gi\u00e0 attivo un smtp smarthost che manda email come prd-lesac-outlet-admin-01@mtaccloud.criticalcase.com. Ci sono tutti i cron amministrativi (crontab -l) OS: Ubuntu 20.04 Customer LAN IP: 10.0.2.50</p> <p>prd-lesac-blog-01 \u00e8 il blog dell'outlet sviluppato con Wordpress in una all-in-one lamp. Stessa identica configurazione php e apache del front end, mpm_event e php7.4 fpm. Memcached locale per le sessioni php. php.ini per salvare le sessioni sul server memcache in localhost. Proftpd per sftp (non ftp) con utente virtuale \u201clesac\u201d che ha come home la /var/www. Il db \u00e8 un mysql 8. OS: Ubuntu 20.04 Customer LAN IP: 10.0.2.80</p> <p>tp.cc-energie-lesac.criticalcasecloud.com \u00e8 la macchina bastion host per l\u2019accesso tramite teleport alle macchine, ci si arriva via https e si utilizza l\u2019utente \u201clesac\u201d che accede come root alle macchine. OS: Ubuntu 20.04 Customer LAN IP: 10.0.2.253</p> <p>NOTE Tutti i domini hanno redirect da dominio a www.dominio lesacmilano.it ha redirect verso www.lesacoutlet.it/lesacmilano/. Sul balancer ci sono regole iptables in /etc/network/firewall.sh relatve a ftp, ssh, postgres access per Energie. Balancer ha tutti i certificati ssl in /etc/nginx/ssl/. Blog risponde al path /blog sui domini lesacoutlet.it Non abbiamo accessi amministrativi al Wordpress del blog. sftp access: ip=176.221.51.45, user=lesac, porta= 2222 per prd-lesac-outlet-fe-01 (Front End) sftp access: ip=176.221.51.45, user=lesac, porta= 2225 per prd-lesac-outlet-admin-01 (Admin) sftp access: ip=176.221.51.45, user=lesac, porta= 2228 per prd-lesac-blog-01 (Blog) c'\u00e8 un bug sul modulo memcached per le sessioni che \u00e8 stato risolto impostando ad off memcached.sess_locking (/etc/php/7.4/mods-available/memcached.ini) la configurazione ad hoc di postgresql \u00e8 sotto /etc/postgresql/12/main/conf.d/lesac_postgresql.conf (jit disabilitato)</p>"},{"location":"customers/energie-C0971/LESACOUTLET/#stage","title":"Stage","text":""},{"location":"customers/energie-C0971/LESACOUTLET/#components_1","title":"Components","text":"<p>dev-lesac-outlet-01 \u00e8 la macchina di sviluppo all-in-one Apache/Php/Postgresql. Ha la stessa identica configurazione php e apache del front end. Memcache sia per le sessioni php, sia per le query del db su localhost. Aumentate le dimensioni dei record e configurato 1GB di ram dedicata a memcached. Proftpd per sftp (non ftp) con utente virtuale \u201clesac\u201d che ha come home la /var/www. I siti abilitati su questa macchina sono: staging.lesacoutlet.it, stagingvk.lesacoutlet.it. Per gli script e gli errori php \u00e8 gi\u00e0 attivo un smtp smarthost che manda email come dev-lesac-outlet-01@mtaccloud.criticalcase.com. il db aggiornato alla versione 12 di postgresql. \u00c8 installato e configurato anche il driver per mssql con le configurazioni odbcper parlare con il server 176.221.49.24 (il gestionale Windows) per scopo sviluppo utente wol. OS: Ubuntu 20.04 Customer LAN IP: 10.0.2.60</p> <p>NOTE sftp access: ip=176.221.51.45, user=lesac, porta= 2226 per dev-lesac-outlet-01 (staging) c'\u00e8 un bug sul modulo memcached per le sessioni che \u00e8 stato risolto impostando ad off memcached.sess_locking (/etc/php/7.4/mods-available/memcached.ini) la configurazione ad hoc di postgresql \u00e8 sotto /etc/postgresql/12/main/conf.d/lesac_postgresql.conf (jit disabilitato)</p>"},{"location":"customers/energie-C0971/LESACOUTLET/#ansible","title":"Ansible","text":"<ul> <li>https://github.com/criticalcase/energie-lesac/tree/main/ansible</li> </ul>"},{"location":"customers/energie-C0971/LESACOUTLET/#openvpn","title":"OpenVPN","text":""},{"location":"customers/energie-C0971/LESACOUTLET/#components_2","title":"Components","text":"<p>dev-lesac-outlet-01</p> <p>Su questa vm risiede la CA per l'emissione dei certificati per il server openVPN presente su tp.cc-energie-lesac.criticalcasecloud.com. La CA si trova sotto /opt/EasyRSA-3.0.8/. Attraverso il comando easyrsa \u00e8 possibile operare sulla pki.</p> <ul> <li>Revocare un certificato</li> </ul> Text Only<pre><code>cd /opt/EasyRSA-3.0.8\n./easyrsa revoke-full CERTNAME #es. wol-client\n./easyrsa gen-crl\n</code></pre> <p>Copiare il file /opt/EasyRSA-3.0.8/pki/crl.pem sotto /etc/openvpn/server sulla vm tp.cc-energie-lesac.criticalcasecloud.com quindi ricaricare il demone openvpn sulla vm tp.cc-energie-lesac.criticalcasecloud.com</p> Text Only<pre><code>systemctl restart openvpn.service\n</code></pre> <ul> <li>Generare un certificato</li> </ul> Text Only<pre><code>cd /opt/EasyRSA-3.0.8\n# genero richiesta certificato\n./easyrsa gen-req CERTNAME nopass #es wol-client\n\n# firmo la richiesta ed emetto il certificato\n./easyrsa sign-req client CERTNAME #es wol-client\n\n# copio i file necessare alla creazione del file di configurazione per il client\ncp /opt/EasyRSA-3.0.8/pki/private/CERTNAME.key /opt/gen-ovpn-client-config/client-configs/keys/\ncp /opt/EasyRSA-3.0.8/pki/issued/wCERTNAME.crt /opt/gen-ovpn-client-config/client-configs/keys/\n\ncd /opt/gen-ovpn-client-config/client-configs\n\n# genero il file di configurazione da installare sul client\n./make_config.sh CERTNAME #es. wol-client\n</code></pre> <p>Copiare quindi il file risultante da /opt/gen-ovpn-client-config/client-configs/files/CERTNAME.ovpn sul client che dovr\u00e0 connettersi in vpn ed importarlo.</p> <p>tp.cc-energie-lesac.criticalcasecloud.com Su questa vm \u00e8 installato il demone openvpn da repo ufficiali Ubuntu. I logs sono nel syslog, la conf \u00e8 sotto /etc/openvpn/</p> <p>I file dei certificati, chiavi, tls-auth e dh sono:</p> Text Only<pre><code>ca.crt\ncrl.pem\ndh.pem\nserver.crt\nserver.key\nta.key\n</code></pre> <p>Il file di configurazione principale \u00e8 server.conf:</p> Text Only<pre><code>port 1194\nproto udp\ndev tun\nca ca.crt\ncert server.crt\nkey server.key\ndh dh.pem\nserver 10.8.0.0 255.255.255.0\nifconfig-pool-persist /var/log/openvpn/ipp.txt\nkeepalive 10 120\ntls-auth ta.key 0\ncipher AES-256-CBC\nauth SHA256\nuser nobody\ngroup nogroup\npersist-key\npersist-tun\nstatus /var/log/openvpn/openvpn-status.log\nverb 3\nexplicit-exit-notify 1\npush \"route 10.0.2.0 255.255.255.0\"\ncrl-verify crl.pem\n</code></pre> <p>Le configurazioni fatte per il forwarding e firewall sono le seguenti:</p> Text Only<pre><code>#uncomment net.ipv4.ip_forward=1 in /etc/sysctl.conf\nsysctl -w net.ipv4.ip_forward=1\nsysctl -p\n\n#check firewall /etc/network/firewall.sh:\n\n#openvpn\n/sbin/iptables -A INPUT -p udp -s 0.0.0.0/0 --dport 1194 -i eth0 -j ACCEPT\n\n### NAT\n#ovpn client to lesac net\niptables -t nat -A POSTROUTING -o eth1 -s 10.8.0.0/24 -d 10.0.2.0/24 -j MASQUERADE\n</code></pre> <p>Non ci sono altre particolari configurazioni.</p>"},{"location":"customers/energie-C0971/servizio-saas-sw-gioco-ferrero/","title":"CR210696: Servizio SaaS Sw gioco (Ferrero)","text":"<p>Account AWS disattivato. Progetto dismesso al 30/09/2022</p>"},{"location":"customers/energie-C0971/servizio-saas-sw-gioco-ferrero/#aws","title":"AWS","text":"Text Only<pre><code>Account: 196898199590\nAlias: energie\nEmail: info@nrgnet.it\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/energie-C0971/servizio-saas-sw-gioco-ferrero/#architettura","title":"Architettura","text":""},{"location":"customers/energie-C0971/servizio-saas-sw-gioco-ferrero/#note","title":"Note","text":"<ul> <li>Il DNS \u00e8 gestito dal cliente</li> <li>Le credenziali sono sotto Sherlock</li> <li>I backup sono automatizzati da AWS Backup</li> <li>Il progetto \u00e8 deployato tramite Terraform Cloud Prd. Il codice \u00e8 su GitHub</li> <li>Le risorse sono sotto monitoring tramite Cloudwatch Alarms e Zabbix</li> </ul> <p>L'autoscaling \u00e8 stato creato manualmente, cos\u00ec come il server e il suo security group</p>"},{"location":"customers/energie-C0971/servizio-saas-sw-gioco-ferrero/#prd","title":"Prd","text":"<p>Il cliente si serve di una software house. Lo sviluppatore ha un accesso a Teleport di root sulla macchina.</p> <p>Se cambiano il codice, ci devono avvertire poich\u00e8 bisogna effettuare l'AMI e aggiornare il template dell'autoscaling. Successivamente bisogner\u00e0 fare un Instance Refresh</p> <p>Sulle macchine \u00e8 presente solo il codice php/html/javascript/css servito da un apache. Non \u00e8 presente nessun database</p> <p></p>"},{"location":"customers/enginko-C2305/","title":"ENGINKO C2305","text":""},{"location":"customers/enginko-C2305/#aws","title":"Aws","text":"Text Only<pre><code>Account: 645128989513\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/epipoli-C2456/","title":"Epipoli","text":""},{"location":"customers/epipoli-C2456/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR230172: Progetto portale su AWS</li> </ul>"},{"location":"customers/epipoli-C2456/#aws","title":"AWS","text":"Text Only<pre><code>Account: 473640980789\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/epipoli-C2456/#teleport","title":"Teleport","text":"<p>Access to the VMs is possible via federated teleport cluster:</p> <p>Select the cluster:</p> <ul> <li><code>tp.epipoli-portale.criticalcasecloud.com</code></li> </ul>"},{"location":"customers/epipoli-C2456/#architettura","title":"Architettura","text":""},{"location":"customers/epipoli-C2456/#note","title":"Note","text":"<ul> <li>Il DNS e il Firewall di Akamai \u00e8 gestito dal cliente</li> <li>I backup sono automatizzati da AWS Backup</li> <li>I repository del cliente si trovano su Bitbucket</li> <li>Le macchine sono configurate attraverso Ansible</li> </ul>"},{"location":"customers/epipoli-C2456/#network","title":"Network","text":"<p>Esiste un unica VPC. Ci sono 6 subnets private e 2 subnets pubbliche per environment (12 subnets totali). Le subnet pubbliche utilizzano tutte lo stesso Internet Gateway. Ci sono due Nat Gateway per produzione Nat Gateway e uno per staging Nat Gateway per le subnets private. Non ci sono limiti di raggiungibilit\u00e0 sulle NACL delle subnets, ma \u00e8 gestito tutto dai Security Groups</p>"},{"location":"customers/epipoli-C2456/#stg","title":"Stg","text":"<p>L'ambiente di stage \u00e8 speculare a quello di produzione, ma singola az. Tutto quello che era possibile criptare on at Rest \u00e8 stato fatto con KMS o chiavi di default di AWS, mantere questa convenzione.</p>"},{"location":"customers/epipoli-C2456/#cloudfront","title":"Cloudfront:","text":"<p>Esiste un Cloudfront condiviso con l'ambiente di produzione per avere lo stesso dominio, la discriminante tra i due ambienti lo fanno i paths. Sono presenti delle errors page nel caso in cui ci siano problemi con il backend, viene mostrata una pagina di maintenance contenuta in questo S3 bucket</p>"},{"location":"customers/epipoli-C2456/#behaviors","title":"Behaviors:","text":"<ul> <li>/staging-digital-point* -&gt; inoltra il traffico verso un altro Cloudfront che serve i contenuti (senza metterli in cache) di questo S3 Bucket. Sono presenti delle errors page. La 403 permette di gestire il caricamento della pagine che restituisce 403 in quanto il contenuto non \u00e8 realmente presente sul bucket, ma \u00e8 React che lo gestisce, quindi c'\u00e8 una sorta di redirect alla home dove il codice gestisce la richiesta. I files vengono caricati dalla Build del codice React attraverso questa Codepipeline, la quale invalida anche la cache di Cloudfront. Il codice \u00e8 presente su Bitbucket del cliente. Per far si riesca a gestire i contenuti senza / e con / finale nel path della root, si utilizza nel Buildspec, eseguito da CodeBuild questo comando cli che copia la index.html all'interno del bucket con il nome del percorso staging-digital-point e /</li> </ul> Text Only<pre><code>- aws s3api copy-object --copy-source $BUCKET_NAME/$CLOUDFRONT_PATHS/index.html --key $CLOUDFRONT_PATHS/ --bucket $BUCKET_NAME\n- aws s3api copy-object --copy-source $BUCKET_NAME/$CLOUDFRONT_PATHS/index.html --key $CLOUDFRONT_PATHS --bucket $BUCKET_NAME\n</code></pre> <ul> <li>/staging-orders-digital-point* -&gt; inoltra il traffico verso un altro Cloudfront che serve i contenuti (senza metterli in cache) di questo S3 Bucket. Sono presenti delle errors page. La 403 permette di gestire il caricamento della pagine che restituisce 403 in quanto il contenuto non \u00e8 realmente presente sul bucket, ma \u00e8 React che lo gestisce, quindi c'\u00e8 una sorta di redirect alla home dove il codice gestisce la richiesta. I files vengono caricati dalla Build del codice React attraverso questa Codepipeline, la quale invalida anche la cache di Cloudfront. Il codice \u00e8 presente su Bitbucket del cliente. Per far si riesca a gestire i contenuti senza / e con / finale nel path della root, si utilizza nel Buildspec, eseguito da CodeBuild questo comando cli che copia la index.html all'interno del bucket con il nome del percorso staging-orders-digital-point e /</li> </ul> Text Only<pre><code>- aws s3api copy-object --copy-source $BUCKET_NAME/$CLOUDFRONT_PATHS/index.html --key $CLOUDFRONT_PATHS/ --bucket $BUCKET_NAME\n- aws s3api copy-object --copy-source $BUCKET_NAME/$CLOUDFRONT_PATHS/index.html --key $CLOUDFRONT_PATHS --bucket $BUCKET_NAME\n</code></pre> <ul> <li>/staging-manager-digital-point* -&gt; inoltra il traffico verso l' Application Load Balancers. Il traffico viene poi inviato all'Autoscaling. Sui servers risponde Magento. Il codice si trova sul Bitbucket del cliente. La Build viene effettuata dal Codepipeline. Il Codebuild si basa su una immagine custom su ECR con la cli di PHP installata, generata da una Codepipeline con questo Dockerfile. Le credenziali necessarie si trovano su Parameter Store</li> </ul> <p>Il resto delle risorse \u00e8 in PAAS:</p> <ul> <li>EFS dove sono memorizzata le immagine delle EC2 in autoscaling servite da Magento</li> <li>Redis per le sessioni di Magento</li> <li>OpenSearch per l'indicizzazione dei contenuti.</li> <li>RDS per il database</li> <li>WAF, condiviso con produzione, accetta solo le richieste proveniente dagli ip pubblici di Akamai che gestisce, insieme al DNS, il cliente finale</li> </ul>"},{"location":"customers/epipoli-C2456/#configurazione-ec2","title":"Configurazione EC2:","text":"<p>Ansible viene eseguito su una EC2 statica al di fuori dell'autoscaling. Quando \u00e8 necessario aggiungere qualche componente, \u00e8 possibile avviare il server, eseguire ansible, creare una nuova AMI e aggiornarla nel Terraform e far ruotare i servers dell'autoscaling.</p> <p>Servizi installati:</p> <ul> <li>CodeDeploy agent</li> <li>Telepor script. Al riavvio si scarica l'ultimo pacchetto caricato da noi sull'account cc-digital e rinomina il teleport agent. Cambia il nome anche al CloudWatch Agent per lo streams dei logs (Syslog,Apache,PHP,Magento). Configurazione dei logs di apache custom con separazione di ip del Client,Cloudfront,ALB,Varnish con Remote Port</li> </ul> Text Only<pre><code>RewriteEngine On\nRewriteCond %{HTTP:X-Forwarded-For} ^(.+?)(?:,\\s*(.+?))?(?:,\\s*(.+?))?$\nRewriteRule ^ - [E=CLIENT_IP:%1,E=CLOUDFRONT_IP:%2,E=ALB_IP:%3]\nLogFormat \"Client IP: %{CLIENT_IP}e CloudFront IP: %{CLOUDFRONT_IP}e ALB IP: %{ALB_IP}e Varnish IP: %a - - %h %l %u %t \\\"%r\\\" %&gt;s %b \\\"RemotePORT %{remote}p\\\" \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\"\" forwarded\n</code></pre> <ul> <li>EFS</li> <li>PHP</li> <li>Varnish</li> <li>Docker</li> <li>unattended-upgrades per aggiornamenti di sicurezza automatici all'avvio e alle 6 del mattino ogni giorno</li> <li>rkhunter come antivirus</li> <li>apache</li> <li>Cloudwatch Agent</li> </ul>"},{"location":"customers/epipoli-C2456/#prd","title":"Prd","text":""},{"location":"customers/eurovita-C2298/","title":"Eurovita SPA","text":""},{"location":"customers/eurovita-C2298/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR210729: DEVOPS JOURNEY + POC</li> </ul>"},{"location":"customers/eurovita-C2298/#eurovita","title":"Eurovita","text":"<p>Referente del progetto: Massimo Pezzuto massimo.pezzuto@eurovita.it Per il codice applicativo: Francesco Stillo francesco.stillo@eurovita.it Per le risorse nella VPN di Eurovita: Radeglia Pietro pietro.radeglia@beta80group.it</p>"},{"location":"customers/eurovita-C2298/#schema","title":"Schema","text":""},{"location":"customers/eurovita-C2298/#appunti-progetto","title":"Appunti progetto","text":"<ul> <li>Region da usare: westeurope</li> <li>Doxibase:</li> <li>EvaReport:</li> <li>EvaJasper: Template JasperReport che vengono sviluppati. Il deploy va assieme all'applicazione java</li> <li>java oracle 1.8.077</li> </ul> <p>Configurazione: <code>src/resources/base.properties</code></p> <ul> <li>URL SVN: https://172.24.18.15/svn/workspace_evareport/trunk</li> <li>UI SVN: https://emiilmt10.ergoitalia.it/!/#workspace_evareport</li> <li>Accesso remote desktop: https://rdweb.wvd.microsoft.com/arm/webclient/index.html</li> </ul> <p>User: andrea.sosso.cons@eurovita.it SVNUser: BYUT4291</p> <p>Issues:</p> <ul> <li>l'url di swagger nell'index.html va cambiato manualmente in base all'ambiente: swagger.json viene generato a runtime.</li> </ul>"},{"location":"customers/franz-kraler-C2217/","title":"Franz Kraler S.r.l. @ C2217","text":""},{"location":"customers/franz-kraler-C2217/#ito-h24","title":"ITO H24","text":"<ul> <li>CR210268: e-commerce B2B e B2C basato su Mulesoft franzkraler.force.com</li> <li>CR200600: PIM Core pim.franzkraler.it</li> </ul> <p>Internal data:</p> <ul> <li>Start date: 10/02/2021</li> <li>Sales: Iryna</li> <li>Phone: +39 0474 972 328</li> <li>Email: i.zharik@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Giacomo Silvestri</li> <li>Email: digital@franzkraler.com</li> <li>Phone: +39 0474 972 328</li> </ul>"},{"location":"customers/franz-kraler-C2217/#data-center","title":"Data Center","text":"<ul> <li> Critical Case RZ2</li> </ul>"},{"location":"customers/franz-kraler-C2217/#cr210268-descrizione-ambito-funzionale-e-tecnologico","title":"CR210268 Descrizione ambito funzionale e tecnologico","text":"<p>Ambiente stage e produzione B2B e B2C basato su Mulesoft (Java). Ogni macchina \u00e8 a se col suo ip pubblico, no lan privata. Sistema operativo Debian 10.8 Vdc: https://vc2-is3.rz1.vdc.ccws.it. Backup gira su Nakivo (https://10.154.67.24:4443/c/mt) ore 1:25 tutti i giorni con ritenzione 4 settimanali e 2 mensili.</p> <p>Monitoraggio su Zabbix e Pingdom Cliente attivo su Jira Cloud.</p>"},{"location":"customers/franz-kraler-C2217/#elenco-server-e-servizi","title":"Elenco Server e servizi","text":"<ul> <li>VPS 1: 4 vCPU, 8 GB RAM, 125 GB Storage Gold, OS Debian quantity: 1</li> <li>VPS 2: 4 vCPU, 8 GB RAM, 100 GB Storage Gold, OS Debian quantity: 1</li> <li>VPS 3: 4 vCPU, 8 GB RAM, 100 GB Storage Gold, OS Debian quantity: 1</li> <li> <p>VPS 4: 4 vCPU, 8 GB RAM, 100 GB Storage Gold, OS Debian quantity: 1</p> </li> <li> <p>Spazio storage backup quantity: 800</p> </li> <li> <p>Licenza Nakivo Backup quantity: 4</p> </li> <li> <p>IP Server production</p> </li> <li><code>212.112.91.28</code> fkb2cesbprod</li> <li><code>212.112.91.29</code> fkb2besbprod</li> <li>IP Server development/staging:</li> <li><code>212.112.91.30</code> fkb2cesbstage</li> <li><code>212.112.91.31</code> fkb2besbstage</li> </ul>"},{"location":"customers/franz-kraler-C2217/#accesso","title":"Accesso","text":"<p>Via Telport:</p> <ul> <li>https://cc-ds1.criticalcasecloud.com</li> <li>https://cc-ds2.criticalcasecloud.com</li> </ul>"},{"location":"customers/franz-kraler-C2217/#how-to-access-to-vms-without-teleport-ssh","title":"How to access to VMs without teleport (ssh)","text":"<p>the IPs, user and password are provided in the following link, in case of teleport failure, you can access to VMs by ssh: Via ssh su ip pubblico (credentials)</p> Bash<pre><code>sshpass -p\"PASSWORD_HERE\" ssh -o StrictHostKeyChecking=no hfarm@212.112.91.28\n</code></pre> <p>Or you can use putty (https://www.putty.org/) Or mobaxterm (https://mobaxterm.mobatek.net/)</p>"},{"location":"customers/franz-kraler-C2217/ecommerce/","title":"e-commerce B2B e B2C","text":""},{"location":"customers/franz-kraler-C2217/ecommerce/#cr210268-descrizione-ambito-funzionale-e-tecnologico","title":"CR210268 Descrizione ambito funzionale e tecnologico","text":"<p>Ambiente stage e produzione B2B e B2C basato su Mulesoft (Java). Ogni macchina \u00e8 a se col suo ip pubblico, no lan privata. Sistema operativo Debian 10.8 Vdc: https://vc2-is3.rz1.vdc.ccws.it. Backup gira su Nakivo (https://10.154.67.24:4443/c/mt) ore 1:25 tutti i giorni con ritenzione 4 settimanali e 2 mensili.</p> <p>Monitoraggio su Zabbix e Pingdom Cliente attivo su Jira Cloud.</p>"},{"location":"customers/franz-kraler-C2217/ecommerce/#elenco-server-e-servizi","title":"Elenco Server e servizi","text":"<ul> <li>VPS 1: 4 vCPU, 8 GB RAM, 125 GB Storage Gold, OS Debian quantity: 1</li> <li>VPS 2: 4 vCPU, 8 GB RAM, 100 GB Storage Gold, OS Debian quantity: 1</li> <li>VPS 3: 4 vCPU, 8 GB RAM, 100 GB Storage Gold, OS Debian quantity: 1</li> <li> <p>VPS 4: 4 vCPU, 8 GB RAM, 100 GB Storage Gold, OS Debian quantity: 1</p> </li> <li> <p>Spazio storage backup quantity: 800</p> </li> <li> <p>Licenza Nakivo Backup quantity: 4</p> </li> <li> <p>IP Server production</p> </li> <li><code>212.112.91.28</code> fkb2cesbprod</li> <li><code>212.112.91.29</code> fkb2besbprod</li> <li>IP Server development/staging:</li> <li><code>212.112.91.30</code> fkb2cesbstage</li> <li><code>212.112.91.31</code> fkb2besbstage</li> </ul>"},{"location":"customers/franz-kraler-C2217/ecommerce/#accesso","title":"Accesso","text":"<p>Via Telport:</p> <ul> <li>https://cc-ds1.criticalcasecloud.com</li> <li>https://cc-ds2.criticalcasecloud.com</li> </ul>"},{"location":"customers/franz-kraler-C2217/ecommerce/#how-to-access-to-vms-without-teleport-ssh","title":"How to access to VMs without teleport (ssh)","text":"<p>the IPs, user and password are provided in the following link, in case of teleport failure, you can access to VMs by ssh: Via ssh su ip pubblico (credentials)</p> Bash<pre><code>sshpass -p\"PASSWORD_HERE\" ssh -o StrictHostKeyChecking=no hfarm@212.112.91.28\n</code></pre> <p>Or you can use putty (https://www.putty.org/) Or mobaxterm (https://mobaxterm.mobatek.net/)</p>"},{"location":"customers/franz-kraler-C2217/pimcore/","title":"PIM Core","text":""},{"location":"customers/franz-kraler-C2217/pimcore/#cr200600-pimcore","title":"CR200600 PIMCORE","text":"<ul> <li>VDC: https://vc2-is3.rz1.vdc.ccws.it/</li> <li>franzkraler-prod: 176.221.55.100 (Docker con 4 container)</li> <li>franzkraler-stag: 176.221.55.101 (Docker con 4 container)</li> </ul> Text Only<pre><code>Stage 4Cpu; 8GB RAM; 100GB storage iron\nProd 4Cpu; 8GB RAM; 500GB storage gold\n2IP\nBackUp\nManaged\n</code></pre>"},{"location":"customers/franz-kraler-C2217/pimcore/#analisi-dellindisponibilita-del-sito","title":"Analisi dell'indisponibilit\u00e0 del sito","text":"<p>Quando abbiamo ricevuto un avviso di indisponibilit\u00e0 del sito web, lo abbiamo analizzato come segue:</p> <ul> <li> <p>Accesso al server franzkraler-prod tramite teleport https://cc-ds1.criticalcasecloud.com/, cluster tp.cc-on-premise.criticalcasecloud.com</p> </li> <li> <p>Passato all'utente hfarm</p> </li> </ul> Text Only<pre><code>su - hfarm\n</code></pre> <ul> <li>Verificare che i containers siano in esecuzione. A questo punto ci siamo resi conto che il conteiner dell'applicazione PIM non era in esecuzione:</li> </ul> Text Only<pre><code>docker ps\n</code></pre> <ul> <li>Abbiamo valutato il history dei comandi per concludere che erano stati fermati dal cliente stesso,tramite il comando 'docker stop pimcore ' , probabilmente per qualche manutenzione. Pertanto, non \u00e8 stata necessaria alcuna azione da parte nostra.</li> </ul>"},{"location":"customers/goglobalecommerce-C2379/","title":"GO GLOBAL ECOMMERCE S.R.L. @ C2269","text":""},{"location":"customers/goglobalecommerce-C2379/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR230149: account AWS di Kooomo goglobalecommerce.com</li> </ul>"},{"location":"customers/goglobalecommerce-C2379/#data-center","title":"Data center","text":"<ul> <li> AWS eu-central-1: kooomo</li> </ul> Text Only<pre><code>Account: 661193417511\nAlias: kooomo\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/goglobalecommerce-C2379/#teleport","title":"Teleport","text":"<p>Access to the VMs is possible via federated teleport cluster:</p> <p>Select the cluster:</p> <ul> <li><code>tp.cc-on-premise.criticalcasecloud.com</code></li> </ul> <p>Select the server:</p> <ul> <li><code>goglobalecommerce</code></li> </ul>"},{"location":"customers/goglobalecommerce-C2379/#architettura","title":"Architettura","text":""},{"location":"customers/goglobalecommerce-C2379/#note","title":"Note","text":"<ul> <li>I backup sono automatizzati da AWS Backup</li> <li>Le credenziali sono sotto Sherlock</li> <li>Il WAF \u00e8 deployato tramite Terraform Cloud Prd</li> <li>Essendo in ITO, l'account \u00e8 controllato sia dalle Lambda in cc-digital</li> <li>Le risorse sono sotto monitoring tramite Cloudwatch Alarms e Zabbix</li> </ul>"},{"location":"customers/goglobalecommerce-C2379/#prd","title":"Prd","text":"<p>Esiste un Cloudfront che fa da cache per i contenuti statici. Al Cloufront \u00e8 associato un WAF al quale sono associate regole di rate-limit, ip permessi e ip malevoli (aggiornati ogni ora dalle nostre lambda in cc-digital). Al Waf \u00e8 associato un Kinesis che ha il compito di inviare i logs al Bucket S3. On-Premise Logstash recupera questi logs e li invia al nostro Kibana (Ricordarsi di selezionare l'host corretto). E' presente una all-in-one EC2.</p> <p>Sulla macchina \u00e8 installato un apache2 sulla porta 80/443. Il codice si trova sulla macchina stessa, con mysql.</p>"},{"location":"customers/gsped-C2234/","title":"GSPED @ C2234","text":""},{"location":"customers/gsped-C2234/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR210475: Consulenza AWS</li> </ul> <p>Internal data:</p> <ul> <li>Referente Commerciale: Silvano Griot</li> <li>Telefono referente: +39 392 912 2831</li> <li>Email: s.griot@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Nome e cognome: Gianluca Nigro</li> <li>Email: nigro@gsped.com</li> <li>Telefono: +39 328 2172860</li> </ul>"},{"location":"customers/gsped-C2234/#aws","title":"Aws","text":"Text Only<pre><code>Account: 262636229443\nAlias: gsped\nType: CriticalCaseAccountReadOnly\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/gsped-C2234/#note-condivise-dal-cliente","title":"Note condivise dal cliente","text":""},{"location":"customers/gsped-C2234/#vpn","title":"VPN","text":"<ul> <li>server openvpn.aws.gsped.it (credentials)</li> </ul>"},{"location":"customers/gsped-C2234/#elasticsearch","title":"Elasticsearch","text":"<ul> <li>Kibana: https://vpc-gsped-loadbalancers-7pv6fstiug3kjsayo2iszwyd6a.eu-west-1.es.amazonaws.com/_plugin/kibana/</li> <li>Elasticsearch: https://vpc-gsped-loadbalancers-7pv6fstiug3kjsayo2iszwyd6a.eu-west-1.es.amazonaws.com</li> </ul>"},{"location":"customers/gsped-C2234/#sophos","title":"Sophos","text":"<p>Il cliente utilizza un firewall di Sophos per controllare il traffico; \u00e8 installato su tre macchine, uno \u00e8 un Master e due Slave. I bilanciatori di solito sono privati, Sophos a redirige il traffico verso di loro.</p>"},{"location":"customers/gsped-C2234/#operazioni-manuali","title":"Operazioni manuali","text":"<p>Le operazioni manuali dovrebbero essere limitate, tuttavia ci sono queste eccezioni date dalle risorse esistenti e non create da Terraform:</p> <ul> <li>VPC c'\u00e8 un nuovo CIDR creato da noi: 192.168.112.0/20 con queste tre nuove Subnets. Al momento il cliente ha voluto creare solo 3 subnets private da 500 ip per i nuovi deploy degli autoscaling creati in Terraform. Ci sono ancora molti ip disponibili</li> <li>Security Groups: le istanze dietro un bilanciatore hanno un sg dedicato per accettare il traffico dal bilanciatore corrispondente. Di solito per\u00f2 altre istanze private devono contattare questi servers. Per fare questo esiste un sg webports-from-ssh-sg. Per accedere in ssh dall'interno esiste invece questo sg ssh-sg. Questi sg sono condivisi spesso tra le istanze dei vari progetti</li> <li>Gli EBS delle risorse attuali sono tutti criptati manualmente</li> <li>Attualmente alcuni launch template sono creati manualmente per risparmiare soldi e non con Terraform, si sta concordando con il cliente la migrazione su Terraform poich\u00e9 il deploy deve essere manualmente e non tramite Codepipeline</li> <li>Il cliente ha avviato un Saving Plan riservandosi un costo orario col fine di risparmiare soldi: https://console.aws.amazon.com/cost-management/home?region=eu-west-1#/savings-plans/overview</li> </ul>"},{"location":"customers/gsped-C2234/#documenti-condivisi","title":"Documenti condivisi:","text":"<ul> <li>Stato dei lavori: https://docs.google.com/spreadsheets/d/1zG2Ha6bJlmHPFFqvjLGhFi5e3V3cOum-fBqTTeAB-9w/edit#gid=828647350</li> <li>Criptazione degli EBS: https://docs.google.com/spreadsheets/d/1ForogL8YiTxaHxp7_3gMR1ZKT9b0Q0eK/edit?rtpof=true</li> <li>Certificati sui bilanciatori: https://docs.google.com/spreadsheets/d/1EORoxFz3822E1y3Sd3-RJY_HwfDbrSMNd5K4oRO6KVk/edit#gid=0</li> </ul>"},{"location":"customers/gsped-C2234/#architetture","title":"Architetture","text":""},{"location":"customers/gsped-C2234/#terraform","title":"Terraform","text":"<p>Tutte le nuove risorse sono gestite da Terraform, il repository \u00e8 sul GitHub del cliente: https://github.com/omnilogsc/terraform-aws Una macchina EC2 con Atlantis si occupa di effettuare il deploy nell'infrastruttura del cliente </p>"},{"location":"customers/gsped-C2234/#spot-istance-con-alb-e-pipeline-per-deploy-nuovo-codice","title":"Spot istance con ALB e pipeline per deploy nuovo codice","text":"<p>I deploy vengono gestiti dal cliente, noi gestiamo solo la parte di infrastruttura</p> <p>Con il fine di ridurre i costi legati alle EC2, si \u00e8 deciso di applicare un'architettura composta da un mix On demand ed Spot istance in Autoscaling dietro ad un bilanciatore. Le nuove architetture create con Terraform utilizzano una unica Codepipeline per il rilascio del codice, il quale \u00e8 memorizzato sul GitHub del cliente. I servers utilizzano una unica AMI. Il deploy avviene tramite CodeDeploy direttamente sulle EC2. Al termine del deploy, c'\u00e8 una fase di Build con lo scopo di creare una nuova AMI e associarla creando una nuova versione del launch template dell'autoscaling. Il codice si trova sul Github del cliente nel file buildspec.yml</p> <p>Le risorse coinvolte in questa architettura sono:</p> <ul> <li>Autoscaling:</li> <li>Gsped-frontend-tf</li> <li>Gsped-labeling-tf</li> <li> <p>Bilanciatori:</p> </li> <li> <p>Gsped-frontend-tf</p> </li> <li> <p>Gsped-labeling-tf</p> </li> <li> <p>Codepipeline:</p> </li> <li>Gsped-frontend-tf</li> </ul> <p>Terraform Code: https://github.com/omnilogsc/terraform-aws/tree/main/prd/loadbalancer/gsped_tf_frontend</p> <p>Al fine di dare meno disservizio possibile, nel caso in cui AWS voglia ripredersi la spot instance, esiste un EventBridge che avvia una Lambda il cui scopo \u00e8 fare il deregister dell'istanza dall'autoscaling e il draining dal bilanciatore; una volta terminata questa procedura, termina l'istanza non utilizzata. Il codice si trova su Codecommit Non esiste un automatismo per l'aggiornamento del codice della lambda, prima si scrive sulla lambda, poi si aggiorna il Codecommit per mantenere il codice.</p> <p>Terraform Code: https://github.com/omnilogsc/terraform-aws/tree/main/prd/lambda/lb_spot_detach_ec2</p> <p></p>"},{"location":"customers/gsped-C2234/#lambda-stream-logs-da-s3-a-elasticsearch","title":"Lambda stream logs da S3 a ElasticSearch","text":"<p>I bilanciatori scrivono i logs delle richieste su dei buckets. Questo evento permette di eseguire una lambda in VPC che prende i logs da S3, li modifica secondo le esigenze del cliente e li carica su un ElasticSearch privato. Il codice della Lambda si trova su Codecommit Non esiste un automatismo per l'aggiornamento del codice della lambda, prima si scrive sulla lambda, poi si aggiorna il Codecommit per mantenere il codice. Esiste un SNS che ci notifica in caso di fallimento della Lambda</p> <p>Terraform Code: https://github.com/omnilogsc/terraform-aws/tree/main/prd/lambda/stream_logs_s3_to_elasticsearch</p> <p></p>"},{"location":"customers/gsped-C2234/#lambda-invio-email-s3-lifecycle","title":"Lambda invio email S3 lifecycle","text":"<p>Una Lambda ogni Luned\u00ec si occupa di andare a verificare la configurazione Lifecycle dei buckets S3, se non \u00e8 presente, invia una email al cliente. Le email sono presenti nell'EventBridge. Il codice \u00e8 su Codecommit Non esiste un automatismo per l'aggiornamento del codice della lambda, prima si scrive sulla lambda, poi si aggiorna il Codecommit per mantenere il codice.</p> <p>Terraform Code: https://github.com/omnilogsc/terraform-aws/tree/main/prd/lambda/s3_lifecycle_notifications</p> <p></p>"},{"location":"customers/hippocrates-C2377/","title":"Hippocrates @ C2377","text":""},{"location":"customers/hippocrates-C2377/#ito-h24","title":"ITO H24","text":"<ul> <li>CR240362: Migrazione Linode TopFarmacia e LaFarmacia</li> </ul>"},{"location":"customers/hippocrates-C2377/#note-h24","title":"Note H24","text":"<ul> <li>ITO attivo solo per il sito LaFarmacia</li> </ul>"},{"location":"customers/hippocrates-C2377/#descrizione-ambito-funzionale-e-tecnologico","title":"Descrizione ambito funzionale e tecnologico","text":"<p>Il progetto prevede la migrazione in ambiente Linode dei siti web TopFarmacia e LaFarmacia. Gli ambienti sono ospitati su cluster Kubernetes LKE.</p>"},{"location":"customers/hippocrates-C2377/#accessi","title":"Accessi","text":"<p>L'accesso agli applicativi quali ArgoCD, Jenkins, Harbor, Vault, Monitoring e al control plane dei cluster K8S, \u00e8 permesso solamente dall'ip 109.233.125.62/32 (ufficio) e alla VPN Ipsec 176.221.51.59/32. Per ulteriori informazioni sulla vpn IPSEC si veda qua.</p> <p>Per accedere ai repository GitHub e ai tool che utilizzano GitHub come metodologia di autenticazione, \u00e8 necessario utilizzare l'account GitHub condiviso <code>criticalcase-digital-solutions</code>, le cui credenziali sono disponibili su sherlock.</p>"},{"location":"customers/hippocrates-C2377/#architettura","title":"Architettura","text":"<p> Il cluster di Management si occupa di governare gli altri due cluster attraverso Argo. Argo infatti, pubblicher\u00e0 e terr\u00e0 sincronizzate le risorse kubernetes col repository git di CD di entrambi i siti lafarmacia-cd e topfarmacia-cd. La parte di CI \u00e8 gestita dalle pipeline, suddivise per sito, configurate su Jenkins e i jenkinsfile, suddivisi per sito: lafarmacia-jenkinsfile e topfarmacia-jenkinsfile. Tutti i secrets usati nelle pipeline sono gestiti nel vault interno di Jenkins. Tutti gli env file necessari all'applicativo, sono memorizzati per sito e ambiente su Vault. Le immagini degli applicativi che vengono generate attraverso Jenkins, vengono pushate sul container registry Harbor, il quale ha configurato come storage l'Harbor Object Storage per entrambi i siti e per tutti gli ambienti esistenti. Ogni cluster ha installato e configurato la parte di monitoring e alerting tramite Prometheus, Grafana e Alertmanager. La parte di backup \u00e8 eseguita tramite cron nei rispettivi namespace per Jenkins e Vault; tramite backup nativi con cnpg per Harbor. Tutti i backup risultanti vengono memorizzati sull'Backup Object Storage. Non sono presenti altri Backup in quanto non ci sono ulteriori dati che necessitano di backup.</p>"},{"location":"customers/hippocrates-C2377/#note","title":"Note","text":"<ul> <li>DNS e domini sono gestiti dal cliente, esistono i seguenti puntamenti:</li> </ul> Text Only<pre><code>*.dev.topfarmacia.it A 172.232.201.224\n*.stg.topfarmacia.it A 172.232.201.224\n*.mng.topfarmacia.it A 172.232.200.230\n*.prd.topfarmacia.it 172.232.201.226\n\n*.dev.lafarmacia.it A 172.232.201.224\n*.stg.lafarmacia.it A 172.232.201.224\n*.prd.lafarmacia.it 172.232.201.226\n</code></pre> <ul> <li>Le credenziali si trovano su Sherlock</li> <li>Il progetto \u00e8 deployato via Terraform (Digger) e Flux</li> <li>Il codice si trova su GitHub</li> </ul>"},{"location":"customers/hippocrates-C2377/#cluster-kubernetes","title":"Cluster Kubernetes","text":"<p>Su Sherlock sono presenti i kubeconfig necessari per accedere agli ambienti. L'ambiente di Management dispone di 3 nodi Linode Shared con 6 CPU e 16GB di Ram L'ambiente di Dev/Stage dispone di 3 nodi Linode Shared con 2 CPU e 4GB di Ram L'ambiente di Produzione dispone di 3 nodi Linode Dedicated con 4 CPU e 8GB di Ram</p>"},{"location":"customers/hippocrates-C2377/#repository","title":"Repository","text":"<p>I seguenti repository nell'organizzazione condivisa:</p> <ul> <li>lafarmacia-web-app: contiene il codice applicativo di LaFarmacia</li> <li>lafarmacia-cd: contiene i file manifest delle risorse LaFarmacia</li> <li>lafarmacia-jenkinsfile: contiene i jenkinsfile usati per le pipeline di LaFarmacia</li> </ul>"},{"location":"customers/hippocrates-C2377/#tool-devops","title":"Tool DevOps","text":"<p>I seguenti tool vengono utilizzati per effettuare i rilasci:</p> <ul> <li>Argo: autenticazione tramite GitHub</li> <li>Jenkins: autenticazione tramite GitHub</li> <li>Harbor: autenticazione tramite utenza personale</li> <li>Vault: autenticazione tramite utenza personale</li> </ul>"},{"location":"customers/hippocrates-C2377/argo/","title":"Argo","text":"<p>Argo \u00e8 il tool di CD utilizzato per effettuare il deploy delle risorse Kubernetes sui cluster di dev e prod.</p> <p>\u00c8 installato nel namespace <code>argo</code> del cluster Management.</p>"},{"location":"customers/hippocrates-C2377/argo/#installazione","title":"Installazione","text":"<p>Argo e i componenti ad esso legati sono deployati sul cluster attraverso FluxCD. I file manifest sono stati caricati sul repository GitHub c-hippocrates-c2377-topfarmacia.</p> <p>Argo \u00e8 deployato attraverso una risorsa HelmRelease.</p> <p>Tutti i componenti necessari al funzionamento di Argo, come Redis e Dex, sono installati attraverso l'Helm chart di Argo.</p>"},{"location":"customers/hippocrates-C2377/argo/#accessi","title":"Accessi","text":""},{"location":"customers/hippocrates-C2377/argo/#networking","title":"Networking","text":"<p>Argo \u00e8 raggiungibile dal seguente link.</p> <p>Tramite annotation \u00e8 configurata una lista di IP dai quali \u00e8 possibile raggiungere Argo. Gli IP autorizzati sono i seguenti:</p> <ul> <li><code>109.233.125.62/32</code> : IP ufficio Criticalcase</li> <li><code>176.221.51.59/32</code> : IP VPN IPsec Criticalcase</li> <li><code>134.255.171.24/32,134.255.171.20/32</code> : IP del cliente</li> </ul>"},{"location":"customers/hippocrates-C2377/argo/#autenticazione-e-autorizzazione","title":"Autenticazione e autorizzazione","text":"<p>Sfruttando il connettore GitHub per Dex, configurato tramite l'helm chart, viene utilizzato la GitHub OAuth App Argo per effettuare l'autenticazione tramite le utenze GitHub. Il clientSecret necessario per utilizzare la OAuth App \u00e8 memorizzato nel secret <code>github-auth-credentials</code>.</p> <p>La gestione dei permessi \u00e8 legata ai team GitHub. Tutti gli utenti appartenenti al team <code>criticalcase</code> o <code>hippocrates-holding</code> nell'organizzazione <code>hippocrates-holding-critical-case</code> possono accedere ad Argo come admin.</p>"},{"location":"customers/hippocrates-C2377/argo/#risorse","title":"Risorse","text":"<p>Argo permette di apportare le sue configurazioni (repository, cluster, progetti ecc.) tramite CRD e secret. Di conseguenza sono stati creati i file manifest che vengono deployati tramite FluxCD e interpretati dai controller di Argo.</p>"},{"location":"customers/hippocrates-C2377/argo/#cluster","title":"Cluster","text":"<p>Argo utilizza un service account per poter interagire (quindi effettuare il deploy di risorse, cancellarle e modificarle) con il cluster sul quale \u00e8 installato, in questo caso il cluster di Management. Per poter effettuare operazioni sui cluster di dev e prod, Argo utilizza autenticazione tramite bearerToken, che viene memorizzato in un secret.</p> <ul> <li>dev-lke</li> <li>prd-lke</li> </ul>"},{"location":"customers/hippocrates-C2377/argo/#project","title":"Project","text":"<p>I progetti Argo permettono di raggruppare le applicazioni, definendo quali risorse possono essere deployate e in quale destinazione. Oltre alla gestione delle permission, i progetti permettono di filtrare le applicazioni tramite la dashboard di Argo.</p> <p>Sono stati creati 4 progetti:</p> <ul> <li>lafarmacia-dev</li> <li>lafarmacia-prd</li> <li>topfarmacia-dev</li> <li>topfarmacia-prd</li> </ul>"},{"location":"customers/hippocrates-C2377/argo/#repository","title":"Repository","text":"<p>I repository ai quali Argo dovr\u00e0 avere accesso per recuperare i manifest delle risorse da sincronizzare nelle applicazioni sono configurati tramite secret. In particolare i secret contengono le credenziali delle GitHub App che Argo utilizza per autenticarsi e accedere ai repository e il progetto al quale collegare il repository stesso.</p> <p>Sono stati collegati due repository, attraverso due GitHub App, ma essendo collegati a 2 progetti diversi, ogni repo \u00e8 stato aggiunto due volte:</p> <ul> <li>lafarmacia-cd per progetto dev</li> <li>topfarmacia-cd per progetto dev</li> <li>lafarmacia-cd per progetto prd</li> <li>topfarmacia-cd per progetto prd</li> </ul>"},{"location":"customers/hippocrates-C2377/argo/#applications","title":"Applications","text":"<p>Un'application in Argo \u00e8 un gruppo di risorse Kubernetes. Tramite le application vengono definite le risorse contenute in un determinato path di un repository e vengono specificate una serie di configurazioni, ad esempio la sincronizzazione automatica.</p> <p>Per ogni sito (lafarmacia e topfarmacia) sono presenti degli ambienti sempre up (main, staging e prd) e degli ambienti temporanei.</p>"},{"location":"customers/hippocrates-C2377/argo/#ambienti-always-running","title":"Ambienti always running","text":"<p>Le seguenti application sincronizzano automaticamente le risorse contenute nel percorso definito in <code>path</code> del corrispondente repository verso il cluster di dev:</p> <ul> <li>lafarmacia-main</li> <li>lafarmacia-staging</li> <li>topfarmacia-main</li> <li>topfarmacia-staging</li> </ul> <p>Le seguenti application invece richiedono l'intervento manuale da parte dell'utente per effettuare la sincronizzazione delle risorse con il cluster di produzione:</p> <ul> <li>lafarmacia-prd</li> <li>topfarmacia-prd</li> </ul>"},{"location":"customers/hippocrates-C2377/argo/#ambienti-temporanei","title":"Ambienti temporanei","text":"<p>Poich\u00e9 gli ambienti di sviluppo sono dinamici, \u00e8 stato utilizzato l'approccio app-of-apps. Tramite tale approccio, nel momento in cui vengono generati i file manifest relativi ai nuovi ambienti dalle pipeline Jenkins, vengono aggiunte dinamicamente nuove application.</p> <p>In particolare, tramite le seguenti application, viene effettuata la sincronizzazione di due path dei repository <code>lafarmacia-cd</code> e <code>topfarmacia-cd</code>, i quali conterranno a loro volta le application relative ai nuovi ambienti creati tramite pipeline. Quindi:</p> <ul> <li>tramite dev-application-of-applications-lafarmacia e dev-application-of-applications-topfarmacia si istruisce Argo a mantenere sincronizzato il contenuto dei path <code>argo/dev/applications</code> dei due repository</li> <li>tramite le pipeline di <code>pr-open</code> viene generato un file manifest con la descrizione di una custom resource di tipo <code>Application</code> nel path <code>argo/dev/applications</code>, la quale porter\u00e0 Argo a sincronizzare sul cluster di dev i file manifest del nuovo ambiente generati dalla stessa pipeline</li> <li>grazie al parametro <code>prune: true</code> l'ambiente temporaneo verr\u00e0 eliminato dal cluster quando tramite la pipeline di <code>pr-close</code> verranno cancellati i file manifest e l'application.</li> </ul>"},{"location":"customers/hippocrates-C2377/argo/#monitoring","title":"Monitoring","text":"<p>Le metriche di tutti i componenti di Argo vengono esportate tramite ServiceMonitor. \u00c8 possibile visualizzare le metriche attuali tramite Grafana.</p> <p>\u00c8 stato creato un alert:</p> <ul> <li>ArgoComponentNotRunning: notifica se uno o pi\u00f9 pod nel namespace <code>argo</code> si trovano in stato non Ready</li> </ul>"},{"location":"customers/hippocrates-C2377/argo/#backup","title":"Backup","text":"<p>Poich\u00e9 tutte le configurazioni di Argo sono effettuate tramite i values dell'Helm Release e tutte le risorse sono create tramite file manifest versionati su GitHub e sincronizzati tramite FluxCD, non viene fatto ulteriore backup.</p>"},{"location":"customers/hippocrates-C2377/cloud-firewall-controller/","title":"Cloud Firewall Controller","text":"<p>Questo controller permette di configurare le regole di sicurezza sui Firewall Linode che vengono associati ai nodi dei cluster LKE.</p> <p>\u00c8 possibile trovare la documentazione relativa al progetto su GitHub.</p> <p>I tool sono installati nel namespace <code>kube-system</code> di ogni cluster.</p>"},{"location":"customers/hippocrates-C2377/cloud-firewall-controller/#installazione","title":"Installazione","text":"<p>I componenti necessari sono deployati sul cluster attraverso FluxCD. I file manifest sono stati caricati sul repository GitHub c-hippocrates-c2377-topfarmacia.</p> <p>La configurazione prevede due componenti, entrambi installati tramite risorse di tipo HelmRelease:</p> <ul> <li>cloud-firewall-crd</li> <li>cloud-firewall-controller</li> </ul> <p>Entrambe le installazioni non hanno personalizzazioni, e le regole create sui firewall sono quelle di default previste dal controller.</p> <p></p>"},{"location":"customers/hippocrates-C2377/harbor/","title":"Harbor","text":"<p>Harbor \u00e8 l'image registry utilizzato per memorizzare le immagini dell'applicativo.</p> <p>\u00c8 installato nel namespace <code>harbor</code> del cluster Management.</p>"},{"location":"customers/hippocrates-C2377/harbor/#installazione","title":"Installazione","text":"<p>Harbor e i componenti ad esso legati sono deployati sul cluster attraverso FluxCD. I file manifest sono stati caricati sul repository GitHub c-hippocrates-c2377-topfarmacia.</p> <p>Harbor e i componenti necessari per il suo funzionamento sono deployati tramite risorse HelmRelease.</p>"},{"location":"customers/hippocrates-C2377/harbor/#redis","title":"Redis","text":"<p>Redis \u00e8 configurato in HA con 3 repliche usando sentinel.</p> <p>\u00c8 dotato di persistenza, usando un block storage da 10 GB.</p>"},{"location":"customers/hippocrates-C2377/harbor/#postgresql","title":"PostgreSQL","text":"<p>Il database PostgreSQL \u00e8 stato deployato utilizzando CloudNativePG.</p> <p>\u00c8 configurato in modalit\u00e0 HA con 3 repliche.</p> <p>\u00c8 dotato di persistenza, usando un block storage da 10 GB.</p> <p>\u00c8 inizializzato creando il database <code>harbor</code> e l'utente <code>harbor</code>, le cui credenziali di accesso sono memorizzate nel secret <code>cnpg-harbor-credentials</code>.</p>"},{"location":"customers/hippocrates-C2377/harbor/#harbor_1","title":"Harbor","text":"<p>Harbor \u00e8 installato in modalit\u00e0 HA per quanto riguarda due componenti: core e registry.</p> <p>Le immagini dell'applicativo sono memorizzate sull'object storage di Linode, in particolare sul bucket chiamato <code>hippocrates-holding-management-bucket</code>. L'autenticazione al bucket avviene attraverso le credenziali memorizzate nel secret <code>harbor-credentials</code>.</p> <p>La configurazione di utenze e progetti \u00e8 effettuata manualmente.</p> <p>\u00c8 stato configurato manualmente il job di clean up, il quale si occupa giornalmente di cancellare dall'object storage le immagini eliminate tramite Harbor.</p>"},{"location":"customers/hippocrates-C2377/harbor/#accessi","title":"Accessi","text":""},{"location":"customers/hippocrates-C2377/harbor/#networking","title":"Networking","text":"<p>Harbor \u00e8 raggiungibile dal seguente link.</p> <p>Tramite annotation \u00e8 configurata una lista di IP dai quali \u00e8 possibile raggiungere Harbor. Gli IP autorizzati sono i seguenti:</p> <ul> <li><code>109.233.125.62/32</code> : IP ufficio Criticalcase</li> <li><code>176.221.51.59/32</code> : IP VPN IPsec Criticalcase</li> <li><code>134.255.171.24/32,134.255.171.20/32</code> : IP del cliente</li> <li><code>10.2.0.0/16</code> : subnet del cluster LKE, necessario affinch\u00e9 i pod di Jenkins possano effettuare il push delle immagini</li> <li>tramite GitHub Action vengono aggiunti e mantenuti aggiornati gli IP di Linode della regione di Milano, necessari affinch\u00e9 i cluster di dev e produzione possano scaricare le immagini</li> </ul>"},{"location":"customers/hippocrates-C2377/harbor/#autenticazione-e-autorizzazione","title":"Autenticazione e autorizzazione","text":"<p>Su Harbor sono state configurate delle utenze locali, che permettono agli utenti umani di effettuare l'accesso tramite le credenziali personali.</p> <p>Gli utenti Criticalcase sono amministratori. Gli utenti Hippocrates non sono amministratori di Harbor, ma solo amministratori dei singoli progetti.</p> <p>\u00c8 possibile fare accesso anche tramite utenza di admin, le cui credenziali sono su Sherlock e nel secret <code>harbor-credentials</code>.</p> <p>L'autenticazione machine-to-machine avviene attraverso i robot account. In particolare i robot account sono di due tipi:</p> <ul> <li>livello Harbor: garantiscono accesso a pi\u00f9 progetti e sono utilizzati per autorizzare Jenkins ad effettuare lettura e scrittura delle immagini</li> <li>livello progetto (ad esempio per lafarmacia-dev): garantiscono accesso a un singolo progetto e sono utilizzati per effettuare il download delle immagini dai cluster di dev e prod.</li> </ul>"},{"location":"customers/hippocrates-C2377/harbor/#progetti","title":"Progetti","text":"<p>I progetti sono divisi per sito e per ambiente.</p>"},{"location":"customers/hippocrates-C2377/harbor/#progetti-di-sviluppo","title":"Progetti di sviluppo","text":"<p>I seguenti progetti contengono 3 repository, per gli ambienti main, dev e staging.</p> <ul> <li>lafarmacia-dev</li> <li>topfarmacia-dev</li> </ul>"},{"location":"customers/hippocrates-C2377/harbor/#progetti-di-produzione","title":"Progetti di produzione","text":"<p>I seguenti progetti contengono un solo repository, contenente le immagini usate in produzione:</p> <ul> <li>lafarmacia-prd</li> <li>topfarmacia-prd</li> </ul>"},{"location":"customers/hippocrates-C2377/harbor/#monitoring","title":"Monitoring","text":"<p>Le metriche di Harbor vengono esportate tramite ServiceMonitor, le metriche di CNPG vengono esportate tramite PodMonitor.</p> <p>Sono stati configurati due alert:</p> <ul> <li>HarborDatabaseBackupFailed: notifica se l'ultimo backup effettuato con successo \u00e8 antecedente a 25h prima (25h per garantire il tempo necessario al completamento del backup, effettuato ogni 24h)</li> <li>HarborUnhealthy: notifica se Harbor \u00e8 in stato unhealthy, verificando tramite la custom metric esportata da Harbor.</li> </ul>"},{"location":"customers/hippocrates-C2377/harbor/#backup","title":"Backup","text":"<p>L'unico componente che necessita di backup per Harbor \u00e8 il database.</p> <p>Il backup viene gestito tramite CNPG nella sezione corrispondente della configurazione.</p> <p>I backup integrali vengono effettuati giornalmente alle 2.00 UTC, mentre vengono automaticamente generati dei WAL per garantire i Point in Time Recovery (PITR) quando vengono effettuate delle transazioni.</p> <p>I backup vengono memorizzati sull'object storage <code>hippocrates-holding-management-backup-bucket</code> sotto il path <code>/harbor-backup</code>. Le credenziali per accedere al bucket sono memorizzate nel secret <code>bucket-credentials</code>.</p>"},{"location":"customers/hippocrates-C2377/ingress-controller/","title":"Nginx Ingress Controller","text":"<p>Poich\u00e9 i node balancer disponibili su Linode sono di tipo L4, \u00e8 stato utilizzando Nginx Ingress Controller per distribuire il traffico ai servizi deployati sui cluster LKE.</p> <p>Sono presenti due ingress controller, entrambi deployati nel namespace <code>kube-ingress</code> di ogni cluster.</p>"},{"location":"customers/hippocrates-C2377/ingress-controller/#installazione","title":"Installazione","text":"<p>I due ingress controller sono deployati sul cluster attraverso FluxCD. I file manifest sono stati caricati sul repository GitHub c-hippocrates-c2377-topfarmacia.</p> <p>Le risorse HelmRelease con cui sono deployati i controller sono presenti nella cartella <code>base</code>. L'ingress controller che si interfaccia con i node balancer di Linode \u00e8 personalizzato nei path relativi a ogni cluster.</p>"},{"location":"customers/hippocrates-C2377/ingress-controller/#ingress-controller-esterno","title":"Ingress controller esterno","text":"<p>Il primo Nginx ingress controller, definito nei file ingress-controller.yaml, si interfaccia direttamente con il node balancer di Linode attraverso un service di tipo Load Balancer.</p> <p>Sul cluster prd \u00e8 in HA con 3 repliche.</p> <p>Le annotation permettono di gestire le configurazioni del Node Balancer. In particolare:</p> <ul> <li>si associa l'ingress controller a un node balancer gi\u00e0 esistente</li> <li>si configura il Node Balancer per non venir eliminato in caso di cancellazione dell'ingress controller</li> <li>si abilita il proxy protocol sul Node Balancer</li> </ul> <p>Il proxy protocol, abilitato tramite config anche su Nginx, permette a Nginx di ricavare l'IP originario per ogni richiesta invece di ricavare solo l'IP del Node Balancer. Questa configurazione \u00e8 necessaria per poter utilizzare l'ip whitelist sugli ingress.</p>"},{"location":"customers/hippocrates-C2377/ingress-controller/#ingress-controller-interno","title":"Ingress controller interno","text":"<p>Essendo abilitato il proxy protocol sul primo ingress controller, tutte le richieste in ingresso devono attraversare il Node Balancer per poter avere gli header validi.</p> <p>Poich\u00e9 la comunicazione tra alcuni servizi intercluster (ad esempio tra Jenkins e Harbor) deve avvenire in HTTPS e tramite l'host *.topfarmacia.it \u00e8 stato necessario un secondo ingress controller con proxy protocol abilitato.</p> <p>L'internal-ingress-controller espone solo un service di tipo ClusterIP, e non ha il proxy protocol abilitato.</p> <p>Tramite una configurazione coredns presente per ogni cluster, il traffico verso i domini <code>*.topfarmacia.it</code> e <code>*.lafarmacia.it</code> viene risolto con il dns del service dell'internal ingress controller.</p> <p>Tutti gli ingress vengono creati senza una ingress class, e entrambi gli ingress controller sono configurati per accettare automaticamente tutti gli ingress senza una ingress class specificata, cos\u00ec che in seguito alla creazione di un ingress entrambi gli nginx ricevano la stessa configurazione.</p>"},{"location":"customers/hippocrates-C2377/ingress-controller/#monitoring","title":"Monitoring","text":"<p>Le metriche degli ingress vengono esportate tramite ServiceMonitor.</p> <p>Sui cluster di management e produzione sono presenti due dashboard grafana che permettono di visualizzare le metriche:</p> <ul> <li>ingress controller</li> <li>mng</li> <li>prd</li> <li>request handling performance</li> <li>mng</li> <li>prd</li> </ul>"},{"location":"customers/hippocrates-C2377/jenkins/","title":"Jenkins","text":"<p>Jenkins \u00e8 il software di CI utilizzato per eseguire la build delle immagini degli applicativi e la loro pubblicazione sul container registry. E' inoltre responsabile della configurazione delle risorse Kubernetes che verrano pushate sui repository di CD.</p> <p>\u00c8 installato nel namespace <code>jenkins</code> del cluster Management.</p>"},{"location":"customers/hippocrates-C2377/jenkins/#installazione","title":"Installazione","text":"<p>Jenkins e i componenti ad esso legati sono deployati sul cluster attraverso FluxCD. I file manifest sono stati caricati sul repository GitHub c-hippocrates-c2377-topfarmacia.</p> <p>Jenkins e i componenti necessari per il suo funzionamento sono deployati tramite risorse HelmRelease.</p> <p>Jenkins \u00e8 dotato di persistenza e usa un block storage da 10 GB.</p>"},{"location":"customers/hippocrates-C2377/jenkins/#applicativo","title":"Applicativo","text":"<p>Jenkins comprende una serie di plugins necessari all'autenticazione utente, accesso ai repository GitHub, trigger e utility per le pipeline. In particolare, nella parte Jenkins Configuration as Code (JCasC), \u00e8 presente la parte relativa all'autenticazione utente tramite GitHub. Le pipeline vengono eseguite attraverso l'uso degli agent. Ogni agent \u00e8 un pod che esegue la pipeline. Data la configurazione del cluster a 3 nodi con 16GB di Ram per nodo, \u00e8 stata impostata una anti affinity rule per fare in modo che solo un agent per nodo possa essere esguito. Eventuali altri agent verranno messi in coda e fatti partire appena uno dei tre agent in escuzione termina.</p> <p>La configurazione dei progetti \u00e8 effettuata manualmente.</p>"},{"location":"customers/hippocrates-C2377/jenkins/#accessi","title":"Accessi","text":""},{"location":"customers/hippocrates-C2377/jenkins/#networking","title":"Networking","text":"<p>Jenkins \u00e8 raggiungibile dal seguente link.</p> <p>Tramite annotation, \u00e8 configurata una lista di IP dai quali \u00e8 possibile raggiungere Jenkins. Gli IP autorizzati sono i seguenti:</p> <ul> <li><code>109.233.125.62/32</code> : IP ufficio Criticalcase</li> <li><code>176.221.51.59/32</code> : IP VPN IPsec Criticalcase</li> <li><code>134.255.171.24/32,134.255.171.20/32</code> : IP del cliente</li> <li>tramite GitHub Action vengono aggiunti e mantenuti aggiornati gli IP di Github, necessari affinch\u00e9 Jenkins possa ricevere i webhook al verificarsi di eventi sui repository monitorati.</li> </ul>"},{"location":"customers/hippocrates-C2377/jenkins/#autenticazione-e-autorizzazione","title":"Autenticazione e autorizzazione","text":"<p>L'accesso avviene tramite autenticazione GitHub, l'accesso \u00e8 consentito ad utenti che partecipano all'organizzazione condivisa hippocrates-holding-critical-case.</p> <p>Viene utilizzata la GitHub OAuth App Jenkins per effettuare l'autenticazione tramite le utenze GitHub. Il clientSecret necessario per utilizzare la OAuth App \u00e8 memorizzato nel secret <code>github-auth-credentials</code>.</p> <p>La gestione dei permessi \u00e8 legata ai team GitHub. Tutti gli utenti appartenenti al team <code>criticalcase</code> o <code>hippocrates-holding</code> nell'organizzazione <code>hippocrates-holding-critical-case</code> possono accedere come admin.</p> <p>Jenkins dispone di un proprio gestore di credenziali dove \u00e8 possibile trovare le credenziali utilizzate dalle pipeline per l'autenticazione a GitHub (repository), Harbor e Vault.</p>"},{"location":"customers/hippocrates-C2377/jenkins/#suddivisione-siti-e-ambienti","title":"Suddivisione Siti e ambienti","text":"<p>I progetti Jenkins legati ai siti lafarmacia.it e topfarmacia.it sono suddivisi attraverso l'uso di folder, quindi le pipeline in esse contenute sono ulteriormente suddivise per:</p> <ul> <li>tipo di evento github ricevuto (pr opened/closed) per gli ambienti temporanei che saranno generati a fronte dell'apertura di una Pull Request o distrutti a fronte della close della PR</li> <li>ambiente e tipo di evento github ricevuto (push) per quanto riguarda gli ambienti sempre attivi: main, staging e production</li> </ul>"},{"location":"customers/hippocrates-C2377/jenkins/#pipeline","title":"Pipeline","text":"<p>La configurazione delle pipeline in Jenkins \u00e8 ottenuta manualmente utlizzando in prima istanza la GUI e in seguito il relativo Jenkins file. La parte configurata attraverso la GUI si occupa di definire il trigger che far\u00e0 scattare la pipeline, in particolare utilizziamo il plugin \"Generic Webhook Trigger\" che ci permette un maggior controllo nella configurazione. Da GitHub riceviamo un json per ogni repository applicativo opportunamente configurato con i webhooks, a fronte di eventi specifici che sono <code>Pushes</code> e <code>Pull requests</code>. Questo plugin permette di attivare la pipeline in maniera puntuale e di parsare velocemente il Json ricevuto. Esempio lafarmacia-web-app webhook. Inoltre il plugin ci permette di configurare e valorizzare delle variabili con il contenuto del json ricevuto, che saranno utilizzate nella pipeline.</p> <p>Ad esempio analizzando la pipeline feature-bugfix-pr-open con i log della PR numero 23 abbiamo:</p> variabile path json valore action $.action opened pr_from_ref $.pull_request.head.ref bugfix/LFIT-965-Error-in-handling-trailing-slashes pr_number $.number 23 pr_branch_prefix $.pull_request.head.ref bugfix/LFIT-965-Error-in-handling-trailing-slashes <p>Inoltre ci permette di definirne il token che verr\u00e0 usato da Github per richiamare la pipeline</p> <p></p> <p>e di scrivere delle regex per attivare la pipeline solo e quando effettivamente \u00e8 stata aperta un Pull Request o eseguita una Merge su uno specifico branch del repository. Per esempio prendendo a riferimento la stessa pipeline e PR analizzata sopra, abbiamo nel Filter:</p> <p></p> <p>che significa che quando la stringa <code>$action $pr_from_ref</code> fa il match con <code>^opened (feature|bugfix)\\/[\\w-]+$</code>, allora la pipeline deve essere eseguita e proseguire.</p> <p>Infine ci permette di definire il Jenkins File corretto che sar\u00e0 eseguito per quel tipo di pipeline.</p> <p></p>"},{"location":"customers/hippocrates-C2377/jenkins/#pipeline-per-ambienti-temporanei","title":"Pipeline per ambienti temporanei","text":"<p>Il flusso di sviluppo del cliente, prevede che dal branch main che corrisponde all'ambiente dev su kubernetes, gli sviluppatori creino dei nuovi branch temporanei che sono indicati dai prefissi:</p> <ul> <li><code>feature-*</code></li> <li><code>bugfix-*</code></li> <li><code>hotfix-*</code></li> <li><code>release-*</code></li> </ul> <p>Lo sviluppo avviene in locale fino a quando non viene aperta una Pull Request da uno di questi branch. In seguito a questo evento, grazie alle pipeline di Jenkins di tipo pr open, verranno create le definizioni delle risorse kubernetes relative all'ambiente di sviluppo sul repository di CD, le quali, tramite ArgoCD, verranno automaticamente sincronizzate sul cluster di dev e rese disponibili allo sviluppatore per i test all'indirizzo <code>[nomebranch]-[numeroPR].dev.[top|la]farmacia.it</code>. L'ambiente creato sar\u00e0 disponibile fino a quando la PR non verr\u00e0 chiusa. A seguito di questo evento, grazie alle pipeline di Jenkins di tipo pr close, la definizione delle risorse kubernetes per l'ambiente temporaneo verranno eliminate sul repository di CD. Tramite ArgoCD le risorse kubernetes verranno eliminate dal cluster di dev grazie alla sincronizzazione automatica col repository di CD.</p>"},{"location":"customers/hippocrates-C2377/jenkins/#pipeline-per-ambienti-permanenti","title":"Pipeline per ambienti permanenti","text":"<p>I branch per gli ambienti permanenti sono:</p> <ul> <li><code>main</code> (dev.[top|la]farmacia.it)</li> <li><code>stagin</code> (staging.[top|la]farmacia.it)</li> <li><code>production</code> ([top|la]farmacia.it, www.[top|la]farmacia.it)</li> </ul> <p>Quando viene eseguita una Merge di una Pull Request verso questi rami o una Push, grazie alle pipeline di Jenkins di tipo push verranno aggiornati i tag dell'applicazione nella definizione delle risorse kubernetes per l'ambiente permanente nel repository di CD. Tramite ArgoCD, le risorse kubernetes verranno sincronizzate automaticamente sul cluster di dev per gli ambienti di dev e staging, mentre rimangono in attesa di sincronizzazione manuale sul cluster di produzione. Per questo ambiente sar\u00e0 il cliente a decidere quando lanciare la sincronizzazione tramite GUI di ArgoCD.</p>"},{"location":"customers/hippocrates-C2377/jenkins/#jenkins-files-per-ambienti-temporanei","title":"Jenkins files per ambienti temporanei","text":""},{"location":"customers/hippocrates-C2377/jenkins/#apertura-pr","title":"Apertura PR","text":"<p>Il jenkins file che viene chiamato dalle pipeline di tipo pr open, all'apertura di una PR su di un branch monitorato \u00e8:</p> <ul> <li>jenkinsfile-pr-open-dev (lafarmacia|topfarmacia)</li> </ul> <p>La struttura \u00e8 composta da blocchi:</p> <ul> <li>enviroment: dove sono definite le variabili da utilizzare successivamente</li> <li>stages: dove sono definiti gli stage ovvero dei blocchi di codice che verranno eseguiti. In particolare:</li> <li>tramite api di GitHub, viene pubblicato un commento nella PR aperta che informa sullo stato della pipeline di Jenkins</li> <li>viene controllato che il nome del branch rispetti la naming convention richiesta per le risorse kubernetes, in quanto esse possono contenere solo caratteri alfanumerici e trattino e possono essere lunghe al massimo 63 caratteri. Il nome del branch viene trasformato in minuscolo</li> <li>utilizzando le credenziali inserite in Jenkins, viene eseguito il <code>git clone</code> del repository dell'applicativo, quindi il <code>git checkout</code> del branch corretto.</li> <li>Viene eseguito il login a Vault tramite credenziali memorizzate in Jenkins. Viene scaricato da Vault il secret env di default necessario all'ambiente <code>[top|la]farmacia/[feature|bugfix|hotfix|release]/env</code>. Se presente in Vault il secret opzionale <code>[top|la]farmacia/[feature|bugfix|hotfix|release]/[nomebranch]-[prnumber]/env</code> (es. <code>/lafarmacia/bugfix/LFIT-965-Error-in-handling-trailing-slashes-12/env</code>) viene scaricato e mergiato al default. Sempre utilizzando le credenziali presenti in Jenkins, viene seguita la build dell'immagine docker e la sua pubblicazione verso Harbor</li> <li>utilizzando le credenziali inserite in Jenkins, viene eseguito <code>git pull</code> del repository di CD (topfarmacia|lafarmacia). Viene copiato il template dell'application di ArgoCD sotto <code>argo/dev/applications/[nomebranch]-[prnumber].yaml</code>; su questo yaml viene aggiornato il tag dell'immagine attraverso <code>sed</code>. Viene copiato il contenuto della cartella argo/templates/dev/ nella cartella <code>argo/dev/[nomebranch]-[prnumber]</code>. Il contenuto sono i file yaml delle definizioni delle risorse kubernetes. Su queste risorse viene aggiornato il tag e il nome dell'imagine attraverso <code>sed</code>. Viene infine eseguito <code>git add, git commit e git push</code>.</li> <li>ArgoCd vedr\u00e0 le modifiche e proceder\u00e0 alla sincronizzazione automatica delle risorse kubernetes sul cluster di dev</li> <li>post: codice che verr\u00e0 eseguito al termine della pipeline in caso di eventi di successo, fallimento o interruzione della pipeline. In particolare viene aggiornato il messaggio precedentemente pubblicato nei commenti della PR con il risultato dell'esecuzione della pipeline di Jenkins e relativo link all'ambiente pubblicato in caso si successo.</li> </ul>"},{"location":"customers/hippocrates-C2377/jenkins/#chiusura-pr","title":"Chiusura PR","text":"<p>Il jenkins file che viene eseguito dalle pipeline di tipo pr close, alla chiusura di una PR su di un branch monitorato \u00e8:</p> <ul> <li>jenkinsfile-pr-close-dev (lafarmacia|topfarmacia)</li> </ul> <p>La struttura \u00e8 composta da blocchi:</p> <ul> <li>enviroment: dove sono definite le variabili da utilizzare successivamente</li> <li>stages: dove sono definite gli stage ovvero dei blocchi di codice che verranno eseguiti, in particolare:</li> <li>il nome del branch viene trasformato in minuscolo</li> <li>utilizzando le credenziali inserite in Jenkins, viene eseguito <code>git pull</code> del repository di CD (topfarmacia|lafarmacia). Vengono cancellati il file dell'application di ArgoCD <code>argo/dev/applications/[nomebranch]-[prnumber].yaml</code> e la cartella contenente le risorse kubernetes <code>argo/dev/[nomebranch]-[prnumber]</code>. Viene infine eseguito <code>git add, git commit e git push</code>.</li> <li>ArgoCd vedr\u00e0 le modifiche e proceder\u00e0 alla sincronizzazione automatica delle risorse kubernetes sul cluster di dev</li> </ul>"},{"location":"customers/hippocrates-C2377/jenkins/#jenkins-files-per-ambienti-permanenti","title":"Jenkins files per ambienti permanenti","text":"<p>Il jenkins file che viene chiamato dalle pipeline di tipo push a seconda dell'ambiente \u00e8:</p> <ul> <li>jenkinsfile-push-main (lafarmacia|topfarmacia)</li> <li>jenkinsfile-push-staging (lafarmacia|topfarmacia)</li> <li>jenkinsfile-push-production (lafarmacia|topfarmacia)</li> </ul> <p>La struttura \u00e8 comune a tutti e tre i files tranne per qualche piccola differenza legata al branch e ai nomi utilizzati nelle risorse kubernetes ed \u00e8 composta da blocchi:</p> <ul> <li>enviroment: dove sono definite le variabili da utilizzare successivamente</li> <li>stages: dove sono definiti gli stage ovvero dei blocchi di codice che verranno eseguiti, in particolare:</li> <li>viene valorizzata la variabile <code>BUILD_IDENT_SHORT</code> utilizzando lo sha del commit come valore, sar\u00e0 il tag dell'immagine</li> <li>utilizzando le credenziali inserite in Jenkins, viene eseguito il <code>git clone</code> del repository dell'applicativo, quindi il <code>git checkout</code> del branch corretto. Viene eseguito il login a Vault tramite credenziali memorizzate in Jenkins. Viene scaricato da Vault il secret env di default necessario all'ambiente <code>[top|la]farmacia/[main|staging|prd]</code>. Sempre utilizzando le credenziali presenti in Jenkins, viene eseguita la build dell'immagine docker e la sua pubblicazione verso Harbor</li> <li>utilizzando le credenziali inserite in Jenkins, viene eseguito <code>git pull</code> del repository di CD (topfarmacia|lafarmacia). Attraverso l'uso di <code>sed</code>, viene aggiornato il tag dell'immagine nella definizione delle risorse kubernetes per l'ambiente permanente. Viene infine eseguito <code>git add, git commit e git push</code></li> <li>ArgoCD vedr\u00e0 le modifiche e proceder\u00e0 alla sincronizzazione automatica delle risorse kubernetes sul cluster di dev per gli ambienti di dev e staging. Per l'ambiente di produzione il cliente dovr\u00e0 procedere alla sincronizzazione manuale tramite GUI di ArgoCD quando lo riterr\u00e0 opportuno</li> </ul>"},{"location":"customers/hippocrates-C2377/jenkins/#monitoring","title":"Monitoring","text":"<p>Le metriche di Jenkins vengono esportate tramite ServiceMonitor.</p> <p>Sono stati configurati due alert:</p> <ul> <li>JenkinsOffline: notifica se l'applicativo Jenkins risulta offline utilizzando le metriche custom esportate da Jenkins</li> <li>JenkinsNotRunning: notifica se non sono presenti pod di Jenkins in stato Running.</li> </ul>"},{"location":"customers/hippocrates-C2377/jenkins/#backup","title":"Backup","text":"<p>Il backup di Jenkins \u00e8 eseguito attraverso un cron che lancia successivamente uno script di backup.</p> <p>Lo script si occupa di configurare il pod con i tools necessari, quindi tramite <code>kubectl</code> lancia un comando tar e gz sul pod di Jenkins per la directory <code>/var/jenkins_home</code> (la home di Jenkins contiene tutte le configurazioni) escludendo la master.key salvata su Sherlock. Infine copia localmente l'archivio ottenuto e ne fa upload sull'object storage di backup.</p> <p>I backup vengono memorizzati sull'object storage <code>hippocrates-holding-management-backup-bucket</code> sotto il path <code>/jenkins-backup</code>. Le credenziali per accedere al bucket sono memorizzate nel secret <code>bucket-credentials</code>.</p>"},{"location":"customers/hippocrates-C2377/jenkins/#restore","title":"Restore","text":"<p>Per poter restorare un Backup di Jenkins \u00e8 necessario:</p> <ul> <li>scaricare l'archivio di backup in locale dal bucket <code>hippocrates-holding-management-backup-bucket/jenkins-backup</code> tramite la console di Linode.</li> <li>copiare l'archivio scaricato che si vuole restorare nel pod di jenkins sotto la directory <code>/tmp</code>. Per copiare il backup sul pod di jenkins eseguire ad esempio <code>kubectl cp jenkins-backup-12.tar.tar jenkins-0:/tmp</code></li> <li>collegarsi al pod di Jenkins sul cluster di management con <code>k exec -it jenkins-0 -- bash</code></li> <li>cancellare tutto il contenuto sotto <code>/var/jenkins_home</code> nel pod di Jenkins tramite il comando <code>cd /var/jenkins_home &amp;&amp; rm -rf *</code></li> <li>dalla cartella <code>/tmp</code> restorare l'archivio con <code>tar -xzvf jenkins_backup.tar.gz -C /var/jenkins_home</code></li> <li>copiare la <code>master.key</code> da sherlock e sostituire o creare la <code>master.key</code> corretta nel pod di Jenkins con: <code>echo -n \"the long master key from sherlock\"&gt;/var/jenkins_home/secrets/master.key</code></li> <li>eseguire un restart del pod di Jenkins tramite <code>kubectl delete pod jenkins-0</code></li> </ul> <p>Attenzione: \u00e8 possibile restorare solo in parte l'archivio rimuovendo le singole directory o files nel pod di Jenkins e copiandole dall'archivio di backup come ad esempio la directory plugins. In questo caso non \u00e8 necessario restorare la master.key.</p>"},{"location":"customers/hippocrates-C2377/monitoring/","title":"Monitoring","text":"<p>Per effettuare il monitoraggio dei cluster LKE e degli applicativi su di essi installati, vengono utilizzati Prometheus, Grafana e AlertManager.</p> <p>I tool sono installati nel namespace <code>kube-monitoring</code> di ogni cluster.</p>"},{"location":"customers/hippocrates-C2377/monitoring/#installazione","title":"Installazione","text":"<p>I tool di monitoraggio sono deployati sul cluster attraverso FluxCD. I file manifest sono stati caricati sul repository GitHub c-hippocrates-c2377-topfarmacia.</p> <p>I tool di monitoraggio sono deployati tramite risorse HelmRelease, utilizzando l'helm chart kube-prometheus-stack.</p> <p>Le configurazioni comuni a tutti i cluster sono presenti nella cartella base.</p> <p>Per ogni cluster \u00e8 presente un file per le personalizzazioni:</p> <ul> <li>management</li> <li>dev</li> <li>prd</li> </ul> <p>Ogni tool \u00e8 dotato di persistenza e usa un block storage di 10 GB.</p>"},{"location":"customers/hippocrates-C2377/monitoring/#accessi","title":"Accessi","text":""},{"location":"customers/hippocrates-C2377/monitoring/#networking","title":"Networking","text":"<p>I tool sono raggiungibile dai seguenti url:</p> <ul> <li>Prometheus</li> <li>management</li> <li>dev</li> <li>prd</li> <li>Grafana</li> <li>management</li> <li>dev</li> <li>prd</li> <li>AlertManager</li> <li>management</li> <li>dev</li> <li>prd</li> </ul> <p>Tramite annotation, \u00e8 configurata una lista di IP dai quali \u00e8 possibile raggiungere i tool. Gli IP autorizzati sono i seguenti:</p> <ul> <li><code>109.233.125.62/32</code> : IP ufficio Criticalcase</li> <li><code>176.221.51.59/32</code> : IP VPN IPsec Criticalcase</li> <li><code>134.255.171.24/32,134.255.171.20/32</code> : IP del cliente</li> </ul>"},{"location":"customers/hippocrates-C2377/monitoring/#autenticazione-e-autorizzazione","title":"Autenticazione e autorizzazione","text":"<p>L'accesso a Prometheus e AlertManager non richiede autenticazione.</p> <p>L'accesso a Grafana avviene tramite autenticazione GitHub, l'accesso \u00e8 consentito ad utenti che partecipano all'organizzazione condivisa hippocrates-holding-critical-case. Per ogni installazione di Grafana \u00e8 stata creata una GitHub OAuth App per effettuare l'autenticazione tramite le utenze GitHub.</p> <ul> <li>management</li> <li>dev</li> <li>prd</li> </ul> <p>Il clientSecret necessario per utilizzare la OAuth App \u00e8 memorizzato nel secret <code>grafana-oauth</code>.</p> <p>La gestione dei permessi \u00e8 legata ai team GitHub. Tutti gli utenti appartenenti al team <code>criticalcase</code> nell'organizzazione <code>hippocrates-holding-critical-case</code> possono accedere come admin. Gli altri utenti accedono con ruolo <code>Viewer</code>.</p>"},{"location":"customers/hippocrates-C2377/monitoring/#prometheus","title":"Prometheus","text":"<p>Prometheus \u00e8 configurato con retention di 10 giorni.</p> <p>Con la seguente configurazione, vengono automaticamente rilevati tutti i serviceMonitor installati nei diversi namespace. Questa configurazione \u00e8 necessaria in quanto i serviceMonitor degli applicativi come Jenkins e Argo sono deployati all'interno dei rispettivi namespace.</p> YAML<pre><code>serviceMonitorSelectorNilUsesHelmValues: false\nserviceMonitorNamespaceSelector:\n  matchLabels: {}\npodMonitorSelectorNilUsesHelmValues: false\npodMonitorNamespaceSelector:\n  matchLabels: {}\n</code></pre>"},{"location":"customers/hippocrates-C2377/monitoring/#grafana","title":"Grafana","text":"<p>Le configurazioni di Grafana sono quelle di default, ad eccezione della persistenza e della configurazione dell'autenticazione. In particolare, il secret <code>grafana-oauth</code> viene montato tramite la sezione extraSecretMounts e la modalit\u00e0 di autenticazione viene abilitata tramite il file grafana.ini</p>"},{"location":"customers/hippocrates-C2377/monitoring/#alertmanager","title":"AlertManager","text":"<p>L'invio degli alert avviene tramite SMTP, usando un'utenza fornita dal cliente. La password dell'utenza \u00e8 memorizzata nel secret <code>alertmanager-smtp-password</code>, mentre l'username \u00e8 contenuta nella configurazione, ad esempio qui.</p> <p>Gli alert che si desidera vengano inviati tramite mail devono contenere la label <code>ITO = \"true\"</code>.</p> <p>Sono stati configurati due receiver (la stessa configurazione \u00e8 riportata su ogni cluster):</p> <ul> <li>ITO: invia gli alert alla mail digitalerts@criticalcase.com di Criticalcase</li> <li>hippocrates: invia gli alert alla mail omnichannel@hippocratesholding.com del cliente</li> </ul>"},{"location":"customers/hippocrates-C2377/vault/","title":"Vault","text":"<p>Vault \u00e8 il secret manager utilizzato per memorizzare i secrets necessari all'applicativo usato dal cliente.</p> <p>\u00c8 configurato in HA con 3 repliche.</p> <p>\u00c8 installato nel namespace <code>vault</code> del cluster Management.</p> <p>\u00c8 dotato di persistenza, usando un block storage da 10 GB per ogni replica.</p>"},{"location":"customers/hippocrates-C2377/vault/#installazione","title":"Installazione","text":"<p>Vault e i componenti ad esso legati sono deployati sul cluster attraverso FluxCD. I file manifest sono stati caricati sul repository GitHub c-hippocrates-c2377-topfarmacia.</p> <p>Vault e i componenti necessari per il suo funzionamento sono deployati tramite risorse HelmRelease.</p> <p>Per poter installare i componenti di Vault e Vault autounseal \u00e8 stata seguita la seguente procedura:</p> <ul> <li>effettuare deploy Helm release di vault, aspettando che tutti i pod siano running ma in stato 0/1 ready</li> <li>effettuare deploy helm release di vault autounseal, il quale in automatico far\u00e0 init e unseal di vault, generando i secret che conterranno root token e chiavi di unseal</li> </ul>"},{"location":"customers/hippocrates-C2377/vault/#autounseal","title":"Autounseal","text":"<p>Autounseal. Viene usato per eseguire l'autounseal di Vault tramite secret al riavvio o creazione di una o pi\u00f9 repliche di Vault. Per l'iterazione con Vault e l'unseal di Vault utilizza i secrets <code>vault-root-token</code> e <code>vault-keys</code></p>"},{"location":"customers/hippocrates-C2377/vault/#vault_1","title":"Vault","text":"<p>Vault \u00e8 installato in modalit\u00e0 HA con 3 repliche. \u00c8 dotato di persistenza, usando un block storage da 10 GB.</p> <p>Vault viene utilizzato dal cliente per la memorizzazione dei secrets che andranno a formare il file env necessario all'applicativo per interagire con i suoi backend. Questi secrets vengono recuperati da Jenkins, il quale durante l'esecuzione delle pipeline crea ill file <code>.env</code> e lo inserisce successivamente nell'immagine dell'applicativo. La configurazione dei secrets prevede dei path ben specifici suddivisi per lafarmacia e topfarmnacia che verranno utilizzati da Jenkins per la costruzione del file <code>.env</code>, pi\u00f9 precisamente:</p> Vault path Utilizzo bugfix/env env di default dell'applicazione per ambiente temporaneo bugfix/[nomebranch]-[prnumber]/env env opzionale dell'applicazione per ambiente temporaneo feature/env env di default dell'applicazione per ambiente temporaneo feature/[nomebranch]-[prnumber]/env env opzionale dell'applicazione per ambiente temporaneo hotfix/env env di default dell'applicazione per ambiente temporaneo hotfix/[nomebranch]-[prnumber]/env env opzionale dell'applicazione per ambiente temporaneo release/env env di default dell'applicazione per ambiente temporaneo release/[nomebranch]-[prnumber]/env env opzionale dell'applicazione per ambiente temporaneo main/env env di default dell'applicazione di dev staging/env env di default dell'applicazione di staging prd/env env di default dell'applicazione di prod <p>La configurazione di utenze, policy e secrets \u00e8 effettuata manualmente.</p>"},{"location":"customers/hippocrates-C2377/vault/#accessi","title":"Accessi","text":""},{"location":"customers/hippocrates-C2377/vault/#networking","title":"Networking","text":"<p>Vault \u00e8 raggiungibile dal seguente link.</p> <p>Tramite annotation, \u00e8 configurata una lista di IP dai quali \u00e8 possibile raggiungere Vault. Gli IP autorizzati sono i seguenti:</p> <ul> <li><code>109.233.125.62/32</code> : IP ufficio Criticalcase</li> <li><code>176.221.51.59/32</code> : IP VPN IPsec Criticalcase</li> <li><code>134.255.171.24/32,134.255.171.20/32</code> : IP del cliente</li> </ul>"},{"location":"customers/hippocrates-C2377/vault/#autenticazione-e-autorizzazione","title":"Autenticazione e autorizzazione","text":"<p>Su Vault sono state configurate delle utenze locali, che permettono agli utenti umani di effettuare l'accesso tramite le credenziali personali. Sono stati creati i gruppi Criticalcase e Hippocrates. Criticalcase ha collegata la policy di admin, mentre Hippocrates ha collegata la policy di hippocrates che permette read write solo sui path <code>topfarmacia/*</code> e <code>lafarmacia/*</code> oltre al permesso di cambio password.</p> <p>\u00c8 possibile fare accesso anche tramite root token che si trova su Sherlock e nel secret <code>vault-root-token</code>.</p> <p>L'autenticazione machine-to-machine avviene attraverso i token jenkins-topfarmacia e jenkins-lafarmacia. Due policy garantiscono accesso sola lettura ai token che sono utilizzata da Jenkins:</p> <ul> <li>jenkins-topfarmacia permette l'accesso in sola lettura al path <code>topfarmacia/*</code></li> <li>jenkins-lafarmacia permette l'accesso in sola lettura al path <code>lafarmacia/*</code></li> </ul>"},{"location":"customers/hippocrates-C2377/vault/#monitoring","title":"Monitoring","text":"<p>Le metriche di Vault vengono esportate tramite ServiceMonitor.</p> <p>Tramite la configurazione \u00e8 stato abilitato l'accesso senza autenticazione alle metriche ed \u00e8 stata abilitata la generazione delle metriche stesse.</p> <p>Sono stati configurati due alert:</p> <ul> <li>VaultDown: notifica se l'applicativo Vault risulta offline utilizzando le metriche custom esportate da Vault</li> <li>VaultNotRunning: notifica se uno o pi\u00f9 pod di Vault non si trovano in stato Running.</li> </ul>"},{"location":"customers/hippocrates-C2377/vault/#backup","title":"Backup","text":"<p>Il backup di Vault \u00e8 eseguito attraverso un cron che lancia successivamente uno script di backup.</p> <p>Il cron di backup per Vault inoltre monta come env var i seguenti secrets: | ENV | Secrets | | :---: | :---: | |AWS_ACCESS_KEY_ID|bucket-credentials| |AWS_SECRET_ACCESS_KEY|bucket-credentials| |AWS_DEFAULT_REGION|bucket-credentials| |VAULT_TOKEN|vault-root-token|</p> <p>Lo script esegue il backup con i seguenti steps:</p> <ul> <li>scarica https://cdn.criticalcase.com/vault/vault_1.18.0_linux_amd64.zip, lo decomprime e lo rende eseguibile</li> <li>esegue la login su Vault tramite l'uso della variabile <code>${VAULT_TOKEN}</code></li> <li>esegue il backup di Vault tramite <code>./vault operator raft snapshot save backup.snap</code></li> <li>infine copia localmente l'archivio ottenuto e ne fa upload sull'object storage di backup</li> </ul> <p>I backup vengono memorizzati sull'object storage <code>hippocrates-holding-management-backup-bucket</code> sotto il path <code>/vault-backup</code>. Le credenziali per accedere al bucket sono memorizzate nelle variabili di ambiente descritte sopra. I backup hanno una ritenzione di 30 giorni. Vengono eseguiti ogni giorno alle 2:00AM.</p>"},{"location":"customers/hippocrates-C2377/vault/#restore","title":"Restore","text":"<p>Per poter restorare un Backup di Vault \u00e8 necessario:</p> <ol> <li>creare un nuovo Vault seguendo la procedura definita nella sezione Installazione</li> <li>copiare dal bucket di backup <code>hippocrates-holding-management-backup-bucket</code> sotto il path <code>/vault-backup</code>, l'archivio che si vuole restorare nel pod di vault sotto la directory /tmp o in locale sul proprio pc (in questo caso assicurarsi di avere una versione di vault installata localmente pari o superiore alla 1.18)</li> <li>valorizzare la variabile <code>VAULT_ADDR</code> con l'url <code>http://vault-active.vault.svc.cluster.local:8200</code> se nel pod di vault, se in locale con <code>https://vault.mng.topfarmacia.it</code></li> <li>effettuare login con root token all'https url del vault di destinazione <code>vault login [root_token]</code></li> <li>dalla cartella <code>tmp</code> o dal path locale, restorare il backup con <code>vault operator raft snapshot restore -force backup.snap</code></li> <li>nel namespace di destinazione sostituire nel secret <code>vault-root-token</code> il root-token del vecchio cluster da restorare e nel secret <code>vault-keys</code> le chiavi del vecchio cluster da restorare</li> <li>eseguire un restart del pod di autounseal</li> </ol> <p>Nel caso si volesse solamente restorare il db di Vault in un Vault esistente seguire solamente i punti 2,3,4 e 5 del precedente paragrafo.</p>"},{"location":"customers/humanativa-C2369/","title":"Humanativa Group S.r.l. @ C2369","text":""},{"location":"customers/humanativa-C2369/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR220484: MaticaNet_Consulenza</li> </ul>"},{"location":"customers/humanativa-C2369/#documentazione","title":"Documentazione","text":"<ul> <li>Word condiviso con il cliente: matica_lld_architecture_eng.docx</li> <li>Template Cloudformation:   dms_roles.yaml dms.yaml elasticsearch.yaml lambda.yaml</li> </ul>"},{"location":"customers/islm-C2215/ISLM/","title":"ISLM - Innovazione e Sistemi per il Lavoro e il Mercato s.r.l. @ C2215","text":"<p>Stiamo facendo l'analisi dell'account</p>"},{"location":"customers/islm-C2215/ISLM/#aws","title":"Aws","text":"Text Only<pre><code>Account: 888085151158\nAlias: cosvifor-lispa\n</code></pre> <ul> <li>Login to AWS console</li> </ul> Text Only<pre><code>Account: 984069941891\nAlias: islm\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/italiaonline-C2287/","title":"Italiaonline SpA @\u00a0C2287","text":""},{"location":"customers/italiaonline-C2287/#progetti-senza-ito","title":"Progetti Senza ITO","text":"<ul> <li>CR240263: Assessment Kubernetes</li> </ul>"},{"location":"customers/italiaonline-C2287/#vpn","title":"VPN","text":"<ul> <li>Download client: https://www.fortinet.com/it/support/product-downloads#vpn</li> <li>SSL-VPN: Remote Gateway: ssl-pratnern.iol.it</li> </ul>"},{"location":"customers/italiaonline-C2287/#url","title":"URL","text":"<ul> <li>Cambio password</li> <li>Confluence</li> <li>GitLab</li> <li>Teleport</li> <li>Grafana</li> <li>Prometheus</li> <li>Alertamanger</li> <li>Harbor</li> </ul>"},{"location":"customers/italiaonline-C2287/#bastion-host","title":"Bastion Host","text":"<ul> <li>10.1.93.21/28</li> </ul> Bash<pre><code>ssh asosso:@sshgw1.iol.sys\nmkdir -p .ssh; echo \"ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOrQ+BmDshUGsaOZ9NHYr0hrsfeUUiXaJUYwwUbcAdhK a.sosso@criticalcase.com\" &gt;&gt; .ssh/authorized_keys\n</code></pre> <ul> <li>Get root access</li> </ul> Bash<pre><code>sudo su -\n</code></pre>"},{"location":"customers/italiaonline-C2287/#sshconfig","title":".ssh/config","text":"INI<pre><code># IOL\nHost iol\n        HostName sshgw1.iol.sys\n        User asosso\n        IdentityFile ~/.ssh/id_criticalcase\n\nHost *.iol.sys\n        User asosso\n        IdentityFile ~/.ssh/id_criticalcase\n        ProxyJump iol\n</code></pre>"},{"location":"customers/italiaonline-C2287/#dns","title":"DNS","text":"Bash<pre><code>ns1.iol.nw.   3600    IN    A 10.2.74.21\nns2.iol.nw.   3600    IN    A 10.2.74.22\n</code></pre>"},{"location":"customers/italiaonline-C2287/#staging","title":"Staging","text":"<ul> <li>Network interna: 10.1.42.0/24 SSH / Control Plane</li> <li>Network pubblica: 10.1.75.0/24 in/out network</li> <li>Firewall pubblico: 10.1.75.0 (fortigate)</li> </ul>"},{"location":"customers/italiaonline-C2287/#control-plane","title":"Control plane","text":"<ul> <li><code>k8s-maildev-cp-01.iol.sys</code> 10.1.42.101/24 ens18, 10.1.75.101/24 ens19</li> <li><code>k8s-maildev-cp-02.iol.sys</code> 10.1.42.102/24 ens18, 10.1.75.102/24 ens19</li> <li><code>k8s-maildev-cp-03.iol.sys</code> 10.1.42.103/24 ens18, 10.1.75.103/24 ens19</li> </ul>"},{"location":"customers/italiaonline-C2287/#worker-nodes","title":"Worker nodes","text":"<ul> <li><code>k8s-maildev-wk-01.iol.sys</code> 10.1.42.11/24 ens18, 10.1.75.11/24 ens19</li> <li><code>k8s-maildev-wk-02.iol.sys</code> 10.1.42.12/24 ens18, 10.1.75.12/24 ens19</li> <li><code>k8s-maildev-wk-03.iol.sys</code> 10.1.42.13/24 ens18, 10.1.75.13/24 ens19</li> </ul>"},{"location":"customers/italiaonline-C2287/#other-nodes","title":"Other nodes","text":"<ul> <li>Bastion Host: <code>k8s-maildev-bh-01.iol.sys</code></li> <li>Harbor Registry: <code>k8s-maildev-rg-01.iol.sys</code></li> </ul>"},{"location":"customers/italiaonline-C2287/#gw","title":"GW","text":"<ul> <li>out ip: 213.209.6.247</li> </ul>"},{"location":"customers/italiaonline-C2287/#kubespray","title":"Kubespray","text":"<p>Here's a guide to set up a Kubernetes cluster on AWS using Kubespray and Ansible. This configuration involves three control nodes, one worker node on EC2 instances, and a fifth instance for managing the installation with Ansible.</p>"},{"location":"customers/italiaonline-C2287/#prerequisites","title":"Prerequisites","text":"<p>Create 5 EC2 instances on AWS:</p> <ul> <li>3 x t2.medium instances for the control plane nodes, named cp01 (RHEL-9.4.0), cp02 (Rocky-9), and cp03 (Rocky-9)</li> <li>1 x t2.small instance for the worker node, named worker01 (RHEL-9.4.0)</li> <li>1 x t2.small instance for Ansible named kubespray-installer (Ubuntu-24.04)</li> </ul> <p>SSH Key (cp01-kubespray.pem): Ensure it's accessible and added to all instances for seamless connectivity.</p>"},{"location":"customers/italiaonline-C2287/#installation-steps","title":"Installation Steps","text":"<p>Configure the Ansible Instance (installer-kubespray). Log into the installer-kubespray instance and perform the following steps to set up the environment:</p>"},{"location":"customers/italiaonline-C2287/#set-selinux-to-permissive","title":"Set SELinux to permissive","text":"<p>To set SELinux to permissive mode on RHEL servers, run:</p> Bash<pre><code>setenforce 0\n</code></pre> <p>Setting SELinux to permissive mode allows it to log warnings rather than enforcing policies, which is often enough to avoid permission-related issues during the installation.</p> <p>This command temporarily sets SELinux to permissive mode for the current session. You may also want to ensure this setting persists across reboots. To do this, modify the SELinux configuration file:</p> <p>Open the SELinux configuration file:</p> Bash<pre><code>sudo vi /etc/selinux/config\n</code></pre> <p>Set the following line to ensure SELinux starts in permissive mode:</p> Bash<pre><code>SELINUX=permissive\n</code></pre> <p>This way, SELinux is not entirely disabled but will avoid enforcing strict policies that could interfere with the Kubernetes installation.</p>"},{"location":"customers/italiaonline-C2287/#clone-the-kubespray-repository","title":"Clone the Kubespray repository","text":"Bash<pre><code>git clone https://github.com/kubernetes-sigs/kubespray.git\ncd kubespray\n</code></pre>"},{"location":"customers/italiaonline-C2287/#set-up-a-python-virtual-environment-and-activate-it","title":"Set up a Python virtual environment and activate it","text":"<p>In this setup, setting up a Python virtual environment for Kubespray's Ansible dependencies is recommended if you\u2019re installing Ansible and its requirements on a local machine where Python and other dependencies might already exist. This prevents potential conflicts with other Python versions or packages.</p> <p>However, because we're using a dedicated instance, installer-kubespray, which doesn\u2019t have any pre-existing Python versions or other conflicting dependencies, setting up a virtual environment is not strictly necessary. Instead, we can directly install Ansible and Kubespray dependencies in the system's default Python environment.</p> Bash<pre><code>python3 -m venv $VENVDIR\nsource $VENVDIR/bin/activate\n</code></pre>"},{"location":"customers/italiaonline-C2287/#install-required-dependencies-for-kubespray","title":"Install required dependencies for Kubespray","text":"Bash<pre><code>pip install -U -r requirements.txt\n</code></pre>"},{"location":"customers/italiaonline-C2287/#configure-the-inventory","title":"Configure the Inventory","text":"<p>Create a Custom Inventory: Copy the sample inventory directory and customize it.</p> Bash<pre><code>cp -rfp inventory/sample inventory/kubespray-inventory\n</code></pre> <p>Add IP Addresses: Define the IPs of your EC2 instances for the control and worker nodes.</p> Bash<pre><code>declare -a IPS=(172.31.45.15 172.31.11.81 172.31.4.128 172.31.47.47)\nCONFIG_FILE=inventory/kubespray-inventory/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}\n</code></pre> <p>Set Ansible User: Add ansible_user: ec2-user to each node in inventory/kubespray-inventory/hosts.yaml.</p> <p>Edit Inventory Configuration: Here is a sample hosts.yaml structure:</p> YAML<pre><code>all:\n  hosts:\n    cp01:\n      ansible_host: 172.31.45.15\n      ansible_user: ec2-user\n      ip: 172.31.45.15\n      access_ip: 172.31.45.15\n    cp02:\n      ansible_host: 172.31.11.81\n      ansible_user: ec2-user\n      ip: 172.31.11.81\n      access_ip: 172.31.11.81\n    cp03:\n      ansible_host: 172.31.4.128\n      ansible_user: ec2-user\n      ip: 172.31.4.128\n      access_ip: 172.31.4.128\n    worker01:\n      ansible_host: 172.31.47.47\n      ansible_user: ec2-user\n      ip: 172.31.47.47\n      access_ip: 172.31.47.47\n  children:\n    kube_control_plane:\n      hosts:\n        cp01:\n        cp02:\n        cp03:\n    kube_node:\n      hosts:\n        worker01:\n    etcd:\n      hosts:\n        cp01:\n        cp02:\n        cp03:\n    k8s_cluster:\n      children:\n        kube_control_plane:\n        kube_node:\n</code></pre>"},{"location":"customers/italiaonline-C2287/#initial-installation-with-kubernetes","title":"Initial Installation with Kubernetes","text":"<p>Add Additional Cluster Configuration: Create an extra_vars.yml file for custom settings in kubespray directory:</p> YAML<pre><code>cluster_name: k8s-maildev.local\ndns_domain: cluster.local\nsupplementary_addresses_in_ssl_keys:\n  [\n    \"10.1.42.101\",\n    \"10.1.42.102\",\n    \"10.1.42.103\",\n    \"k8s-maildev-cp-01.iol.sys\",\n    \"k8s-maildev-cp-02.iol.sys\",\n    \"k8s-maildev-cp-03.iol.sys\",\n  ]\nkubeconfig_localhost: true\nkube_version: \"v1.30.6\"\nmetrics_server_enabled: false\nlocal_path_provisioner_enabled: false\napiserver_loadbalancer_domain_name: \"k8s-maildev-cp-ha.iol.sys\"\nloadbalancer_apiserver:\n  address: \"10.1.64.247\"\n  port: 8383\nkube_network_plugin: \"cilium\"\nping_access_ip: false\nupstream_dns_servers: [\"10.2.74.21\", \"10.2.74.22\"]\n</code></pre> <p>Specify the Initial Version: In the extra_vars.yml file (or directly in inventory/kubespray-inventory/group_vars/k8s_cluster/k8s-cluster.yml)</p> YAML<pre><code>kube_version: \"v1.30.6\"\n</code></pre> <p>Once the initial cluster was up and running, we upgraded to Kubernetes version modifying the Version Variable: In the same configuration file, we updated the var:</p> YAML<pre><code>kube_version: \"v1.31.1\"\n</code></pre> <p>Running the playbook with the updated version automatically handles the upgrade, updating all nodes and cluster components to the specified version. This approach allows seamless upgrades and keeps configurations consistent across the cluster.</p>"},{"location":"customers/italiaonline-C2287/#deploy-the-kubernetes-cluster-with-ansible","title":"Deploy the Kubernetes Cluster with Ansible","text":"<p>Run the Ansible Playbook: Execute the following command to deploy Kubernetes. This requires --become to gain necessary privileges and --private-key for SSH.</p> Bash<pre><code>ansible-playbook -i inventory/kubespray-inventory/hosts.yaml --become --become-user=root --private-key /root/.ssh/cp01-kubespray.pem --extra-vars '@extra_vars.yml' cluster.yml\n</code></pre> <p>Monitoring and Validation: Once the playbook completes, validate the cluster by connecting to one of the control plane nodes and running:</p> Bash<pre><code>kubectl get nodes\n</code></pre> <p>to ensure all nodes are active and part of the cluster.</p>"},{"location":"customers/italiaonline-C2287/#additional-notes","title":"Additional Notes","text":"<p>Cilium Network Plugin: This setup uses Cilium as the network plugin for advanced networking capabilities. You can modify this setting in extra_vars.yml. DNS and SSL Configuration: The supplementary addresses in the SSL keys and DNS settings are specified to ensure reliable connections and avoid potential DNS issues.</p>"},{"location":"customers/italiaonline-C2287/#nfs-mount","title":"NFS Mount","text":"<p>Registry:</p> Bash<pre><code>mount -t nfs -o vers=3,tcp,hard,rsize=262144,wsize=262144 f4304t-01-pod1.iol.sys:/K8S/registry &lt;mount_point&gt;\n</code></pre>"},{"location":"customers/italiaonline-C2287/#documentazione-ox","title":"Documentazione OX","text":"<ul> <li>https://documentation.open-xchange.com/appsuite/operation-guides/requirements.html</li> <li>https://documentation.open-xchange.com/appsuite/operation-guides/kubernetes.html</li> </ul>"},{"location":"customers/jakala-C2249/","title":"Jakala @ C2249","text":""},{"location":"customers/jakala-C2249/#ito-h24","title":"ITO H24","text":"<ul> <li>CR210361: TIM Party su GCP timparty.tim.it</li> </ul>"},{"location":"customers/jakala-C2249/#note-h24","title":"Note H24","text":"<ul> <li>Il progetto \u00e8 per Jakala ma il customer finale \u00e8 TIM</li> </ul>"},{"location":"customers/jakala-C2249/#cloud-sql-scale-up-scale-down","title":"Cloud SQL scale up / scale down","text":"<p>Sul database di produzione \u00e8 stato implementato un meccanismo di autoscaling basato su Cloud Scheduler e Cloud Function. Ogni sabato alle 23:30 il database viene scalato a 8CPU e 32GB di Ram, il luned\u00ec viene riportato a 4 CPU e 16GB di Ram. Questo avviene con un minimo downtime di circa 30 secondi nel momento in cui il database esegue il failover. Il tempo totale dell'operazione \u00e8 di circa 5 minuti. Si \u00e8 reso necessario in quanto TIM Party ha il maggior traffico la domenica. E' possibile che il cliente, per eventi infrasettimanali quali X-Factor ad esempio, richieda ulteriori diverse schedulazioni extra week end attraverso un ticket Jira. In questo caso \u00e8 sufficiente copiare i due job gi\u00e0 schedulati (mysql-scale-up e mysql-scale-down) e modificare la frequenza con le date e orari richiesti dal cliente attraverso la notazione cron. Inoltre \u00e8 necessario nel job clonato reinserire i dati della post nel campo Body alla voce \"Configure the execution\" per i due job aggiuntivi che saranno inviati alla function:</p> <ul> <li>Scale UP</li> </ul> Text Only<pre><code>{\n  \"project\": \"timparty-prod\",\n  \"instanceId\": \"mysql-prod-01-f8af844e\",\n  \"tier\": \"db-custom-8-32768\"\n}\n</code></pre> <ul> <li>Scale Down</li> </ul> Text Only<pre><code>{\n  \"project\": \"timparty-prod\",\n  \"instanceId\": \"mysql-prod-01-f8af844e\",\n  \"tier\": \"db-custom-4-16384\"\n}\n</code></pre>"},{"location":"customers/jakala-C2249/#note-aggiornamento-teleport","title":"Note Aggiornamento Teleport","text":"<ul> <li> <p>Attenzione, per quanto riguarda l'aggiornamento di Teleport per le vm in produzione sotto ASG, seguire la seguente procedura:</p> </li> <li> <p>Scaricare il pacchetto della versione aggiornata di teleport (deb o rpm 64bit) e uploadarlo sul nostro s3 dedicato:    https://s3.console.aws.amazon.com/s3/buckets/cdn-criticalcase-com?region=eu-south-1&amp;prefix=teleport-tim-jakala/&amp;showversions=false    rinominandolo a seconda della distro teleport_latest_amd64.deb o teleport_latest_x86_64.rpm.</p> </li> <li>Spostarsi su ogni front end di produzione ed eliminare il file :</li> </ul> Text Only<pre><code>rm -f  /usr/local/etc/no_gc_startup_script_prod\n</code></pre> <ol> <li>su ogni front end di produzione lanciare lo script:</li> </ol> Text Only<pre><code>bash /usr/local/bin/startup_script.sh\n</code></pre> <p>dopo qualche minuto il front end avr\u00e0 installato e aggiornato la versione di teleport. Per le nuove istanze questo non \u00e8 necessario, in quanto lo script viene lannciato tramite cron @reboot e il file no_gc_startup_script_prod non \u00e8 presente nell'immagine delle vm.</p> <p>Per quanto riguarda gli aggiornamento degli ambienti quality e dev, sono gestiti da ansible, per il bastion da flux.</p>"},{"location":"customers/jakala-C2249/#jakala","title":"Jakala","text":"<p>Internal data:</p> <ul> <li>Start date: 20/04/2021</li> <li>Sales: Alessandro Zoncu</li> <li>Phone: +39 3357899388</li> <li>Email: a.zoncu@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Andrea Pedrazzini</li> <li>Email: noc@jakala.com</li> <li>Phone: +39 3423416700</li> <li>Severity 1 notification: [noc@jakala.com ] Email, [+39 3423416700 ] Telefono</li> <li>Severity 2 notification: [noc@jakala.com ] Email, [ +39 3423416700] Telefono</li> <li>Il report di pingdom dev\u2019essere mandato ai seguenti indirizzi:   noc@jakala.com alberto.rossetti@jakala.com</li> <li>Mail a cui mandare l\u2019alerting di gcp   noc@jakala.com</li> </ul>"},{"location":"customers/jakala-C2249/#data-center","title":"Data Center","text":"<ul> <li> AWS</li> <li> Azure</li> <li> GCP</li> <li> Tencent</li> <li> Critical Case RZ1</li> <li> Critical Case RZ2</li> </ul>"},{"location":"customers/jakala-C2249/#descrizione-ambito-funzionale-e-tecnologico","title":"Descrizione ambito funzionale e tecnologico","text":"<p>Sono disponibili 3 differenti ambienti su Google Cloud Platform.</p> <p>Il servizio ITO \u00e8 attivo solo per gli ambienti di Quality e Produzione non per Dev</p>"},{"location":"customers/jakala-C2249/#accessi","title":"Accessi","text":""},{"location":"customers/jakala-C2249/#teleport","title":"Teleport","text":"<p>Tutte le vm sono accessibili tramite Teleport con interfaccia web o client tsh.</p> <ul> <li>Web interface: https://cc-ds1.criticalcasecloud.com/ selezionare cluster tim-access.criticalcasecloud.com</li> <li>Telport cluster accesso diretto https://tim-access.criticalcasecloud.com:3080/ utilizzare nel caso di problemi sull'accesso centralizzato</li> <li>Download Teleport from https://goteleport.com/teleport/download/</li> </ul>"},{"location":"customers/jakala-C2249/#vault-keys","title":"VAULT keys","text":"<ul> <li>https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/show/ds/customers/jakala-C2249/gcp</li> </ul>"},{"location":"customers/jakala-C2249/#google-cloud-access","title":"Google Cloud Access","text":"<p>Login with Google account Credentials (for Critical Service available here https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/show/ds/customers/jakala-C2249/criticalserviceaccess )</p> <p>PRODUCTION PROJECT - https://console.cloud.google.com/home/dashboard?authuser=2&amp;folder=&amp;organizationId=&amp;orgonly=true&amp;project=timparty-prod&amp;supportedpurview=project,organizationId,folder</p> <p>QUALITY PROJECT - https://console.cloud.google.com/home/dashboard?authuser=2&amp;folder=&amp;organizationId=&amp;orgonly=true&amp;project=timparty-qa&amp;supportedpurview=project,organizationId,folder</p>"},{"location":"customers/jakala-C2249/#production","title":"Production","text":""},{"location":"customers/jakala-C2249/#architecture","title":"Architecture","text":""},{"location":"customers/jakala-C2249/#components","title":"Components","text":"<p>VPC-Prod La piattaforma \u00e8 interamente implementata all'interno di una Network privata.</p> <p>Drupal Server E' la sola componente IaaS ed \u00e8 accessibile tramite Teleport Fa parte di un autoscaling group che pu\u00f2 scalare tra 2 e 20 VM</p> <p>Clod Filestore PaaS GCP per share NFS Endpoint: nfs.svczone.local Montato su: <code>mount -t nfs nfs.svczone.local:/share /var/www/html</code></p> <p>Load Balancer GCP load Balancer configurato per https://timparty.it e https://concorsotimparty.it, usati certificati forniti dal cliente (jakala e TIM) Public IP: 34.117.220.69</p> <p>Memorystore for Memcached PaaS di Memcache per gestione delle sessioni PHP-Drupal Endpoint: memcache.svczone.local</p> <p>Cloud SQL GCP PaaS di MySQL, gi\u00e0 in HA, backup periodico giornaliero e point-in-time recovery a 3 giorni Endpoint: mysql.svczone.local</p>"},{"location":"customers/jakala-C2249/#elenco-midlleware-installati-sui-server-drupal","title":"Elenco Midlleware Installati Sui Server Drupal","text":"<ul> <li>Apache 2.4 (con catch-all sui virtual host)</li> <li>PHP Ver 7.4</li> <li>Drupal NON GESTITO DA CRITICALCASE</li> </ul>"},{"location":"customers/jakala-C2249/#quality","title":"Quality","text":""},{"location":"customers/jakala-C2249/#components_1","title":"Components","text":"<p>VPC La piattaforma \u00e8 interamente implementata all'interno di una Network privata.</p> <p>Drupal Server E' la sola componente IaaS ed \u00e8 accessibile tramite Teleport Fa parte di un autoscaling group che pu\u00f2 scalare tra 1 e 2 VM (solo per rolling update)</p> <p>Clod Filestore PaaS GCP per share NFS Endpoint: nfs.svczone.local Montato su: <code>mount -t nfs nfs.svczone.local:/share /var/www/html</code></p> <p>Load Balancer GCP load Balancer configurato per https://timparty.it e https://concorsotimparty.it, usati certificati forniti dal cliente (jakala) Public IP: 34.120.61.72</p> <p>Memorystore for Memcached PaaS di Memcache per gestione delle sessioni PHP-Drupal Endpoint: memcache.svczone.local</p> <p>Cloud SQL GCP PaaS di MySQL, gi\u00e0 in HA, backup periodico giornaliero e point-in-time recovery a 3 giorni Endpoint: mysql.svczone.local</p>"},{"location":"customers/jakala-C2249/#elenco-midlleware-installati-sui-server-drupal_1","title":"Elenco Midlleware Installati Sui Server Drupal","text":"<ul> <li>Apache 2.4 (con catch-all sui virtual host)</li> <li>PHP Ver 7.4</li> <li>Drupal NON GESTITO DA CRITICALCASE</li> </ul>"},{"location":"customers/jakala-C2249/#dev","title":"DEV","text":"<p> Dev \u00e8 un ambiente molto semplice che consiste in un'installazione di una VM all-in-one sul progetto dedicato timparty-dev e su un VPC dedicato. La VM ha un IP fisso riservato: 35.210.206.58 Questo ambiente non \u00e8 sotto ITO e non \u00e8 coinvolto nelle pipeline DevOps. Anche la VM di DEV \u00e8 accessibile tramite Teleport</p>"},{"location":"customers/jakala-C2249/#elenco-midlleware-installati-sui-server-drupal_2","title":"Elenco Midlleware Installati Sui Server Drupal","text":"<ul> <li>Apache 2.4 (con catch-all sui virtual host)</li> <li>PHP Ver 7.4</li> <li>MySQL 5.7</li> <li>Memcache</li> <li>Drupal NON GESTITO DA CRITICALCASE</li> </ul>"},{"location":"customers/jakala-C2249/#bastion-vm","title":"Bastion VM","text":"<p>La macchina Bastion \u00e8 una VM hardened usata per permettere un accesso SSH al VPC privato. Solo sshuser pu\u00f2 accedere al server usando la chiave ssh. I permessi di sshuser sono solo quelli necessari a permettere un ulteriore ssh verso altre macchine, tutti gli altri comandi sono bloccati. L'accesso alla rete privata virtuale pu\u00f2 passare attraverso un tunnel SSH Bastion nel caso in cui Teleport non funzionasse correttamente. sshuser \u00e8 usato anche dalle pipeline di CloudBuild per creare un tunnel ssh verso le VM all'interno dei gruppi di autoscaling in ambiente quality e prod.</p> <p>Utilizzare sempre Teleport per gli accessi alle VM, sshuser \u00e8 solo per emergenze dovute a malfunzionamenti di Teleport</p> <p>Note:</p> <ul> <li>Vedere la sezione Pipeline di DevOps per eventuali problemi</li> </ul> <p>Terraform Code: https://github.com/criticalcase/terraform-jakala-gcp</p> <p>DevOps Code: https://github.com/criticalcase/terraform-jakala-cloudbuild</p> <p>Documentazione di Progetto (accesso riservato al gruppo Digita Solutions) https://criticalcaseazure.sharepoint.com/:w:/s/DigitalSolution/EVkk5Zdng2ZHiXwlZRaLalcBeZDTCuTbvk40yEKCfsZ2lg?e=Rursfr</p>"},{"location":"customers/jakala-C2249/TIMPARTY-Code-Pipeline/","title":"Code Pipeline","text":""},{"location":"customers/jakala-C2249/TIMPARTY-Code-Pipeline/#cloudbuild-code-pipelines","title":"CloudBuild code pipelines","text":""},{"location":"customers/jakala-C2249/TIMPARTY-Code-Pipeline/#quality","title":"Quality","text":"<p>At time zero Jakala CI pipelines or a manual operation makes two actions:</p> <ul> <li>a) Copies the Drupal application code on Cloud Storage. This code MUST be already formatted with the correct tree structure, otherwise the whole procedure is not working.</li> <li>b) Triggers the CloudBuild Pipeline</li> </ul> <p>Once triggered the pipeline:</p> <ul> <li> <ol> <li>Open a sshtunnel on Bastion host using sshuser (limited user)</li> </ol> </li> <li> <ol> <li>Exploiting the network peering, it connects via ssh tunnel to one of the VMs active in the cluster.</li> </ol> </li> <li> <ol> <li>A Storage Read Only role is configured for the VMs, this allows the CloudBuild ssh session to connect to the Cloud Storage and access the code copied at point \u201ca\u201d.</li> </ol> </li> <li> <ol> <li>Copy the code on the mounted Cloud Filestore with a rsync approach, i.e. just copying new files and avoiding to overwrite older files already presents on Filestore.</li> </ol> </li> <li> <ol> <li>As Filestore is NFS mounted on all VMs, changes are available for the whole cluster.</li> </ol> </li> </ul>"},{"location":"customers/jakala-C2249/TIMPARTY-Code-Pipeline/#prod","title":"Prod","text":"<p>At time zero Jakala CI pipelines or a manual operation makes two actions:</p> <ul> <li>a) Copies the Drupal application code on Cloud Storage (the code that has passed the Quality testing phase). This code MUST be already formatted with the correct tree structure, otherwise the whole procedure is not working.</li> <li>b) Triggers the CloudBuild Pipeline</li> </ul> <p>Once triggered the pipeline:</p> <ul> <li> <ol> <li>Open a sshtunnel on Bastion host using sshuser (limited user)</li> </ol> </li> <li> <ol> <li>Exploiting the network peering between different Projects, it connects via ssh tunnel to one of the VMs active in the cluster.</li> </ol> </li> <li> <ol> <li>A Storage Read Only role is configured for the VMs, this allows the CloudBuild ssh session to connect to the Cloud Storage and access the code copied at point \u201ca\u201d.</li> </ol> </li> <li> <ol> <li>Copy the code on the mounted Cloud Filestore with a rsync approach, i.e. just copying new files and avoiding to overwrite older files already presents on Filestore.</li> </ol> </li> <li> <ol> <li>As Filestore is NFS mounted on all VMs, changes are available for the whole cluster.</li> </ol> </li> </ul>"},{"location":"customers/jakala-C2249/TIMPARTY-System-Pipeline/","title":"DevOps and Infrastructure as a Code","text":"<p>The whole infrastructure is deployed via Terraform (an open source and widely used software for the Infrastructure as a Code), with the only exception of Dev environment middleware that is installed via start-up script during the start-up procedure. This approach permits an easy deployment of other Environment (in this case the Production one) and an easy rebuild of the whole infra. All the automation is inside a Criticalcase private git repo and opened to the Criticalcase Timparty project team only.</p>"},{"location":"customers/jakala-C2249/TIMPARTY-System-Pipeline/#cloudbuild-system-pipelines","title":"CloudBuild system pipelines","text":""},{"location":"customers/jakala-C2249/TIMPARTY-System-Pipeline/#quality","title":"Quality","text":"<p>At time zero:</p> <ul> <li>a) configuration/middleware/system update activity is made by Jakala or Criticalcase on the only server of an Autoscaling group set to 1.</li> <li>b) Jakala CI pipelines or a manual operation. Triggers the CloudBuild Pipeline</li> </ul> <p>Once triggered the pipeline:</p> <ul> <li> <ol> <li>Creates a snapshot of the VMs that has been changed</li> </ol> </li> <li> <ol> <li>Creates an Image from the snapshot</li> </ol> </li> <li> <ol> <li>Creates a VM Template from the Image</li> </ol> </li> <li> <ol> <li>The autoscaling group via Rolling update creates new instances of VMs starting by the new template</li> </ol> </li> </ul>"},{"location":"customers/jakala-C2249/TIMPARTY-System-Pipeline/#prod","title":"Prod","text":"<p>At time zero:</p> <ul> <li>a) Jakala CI pipelines or a manual operation. Triggers the CloudBuild Pipeline</li> </ul> <p>Once triggered the pipeline:</p> <ul> <li> <ol> <li>The CloudBuild service user has a policy set up to access to the vm snapshot created during Quality system pipeline inside quality project.</li> </ol> </li> <li> <ol> <li>Creates an Image on production project using the snapshot on quality.</li> </ol> </li> <li> <ol> <li>Creates a VM Template from the Image</li> </ol> </li> <li> <ol> <li>The autoscaling group via Rolling update creates new instances of VMs starting by the new template.</li> </ol> </li> </ul>"},{"location":"customers/kooomo-C1362/","title":"Kooomo SAAS Limited @ C1362 - S103","text":""},{"location":"customers/kooomo-C1362/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR200591: Gestione infrastruttura on premise a Dublino + AWS account 661193417511 (English customer)</li> </ul>"},{"location":"customers/kooomo-C1362/#note-h24","title":"Note H24","text":"<p>~~Se il cliente chiama, tutte le VM (anche dev/test/stag/sit) sono da considerare produzione. Il cliente ha la sensibilit\u00e0 di segnalarvi cosa \u00e8 urgente o meno. {.is-warning}~~</p> <ul> <li>Il cliente \u00e8 formato sull'infrastruttura, se chiama per un problema vi aiuter\u00e0 a risolvere il problema nel pi\u00f9 breve tempo possibile.</li> <li>Se la VM non \u00e8 un master, mettere in maintenance consul prima di procedere ad un riavvio.</li> <li>Controllare la documentazione del sito www.goglobalecommerce.com</li> </ul>"},{"location":"customers/kooomo-C1362/#kooomo","title":"Kooomo","text":"<p>Internal data:</p> <ul> <li>Referente Commerciale: Silvano Griot</li> <li>Telefono referente: +39 335 142 6087</li> <li>Email: s.griot@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Nome e cognome: Joaquin Casas</li> <li>Email: jcasas@kooomo.com</li> <li>Telefono: +353 894045158</li> </ul>"},{"location":"customers/kooomo-C1362/#aws","title":"Aws","text":"Text Only<pre><code>Account: 661193417511\nAlias: kooomo\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/kooomo-C1362/#vmware-ie1-dublin-dc","title":"VMWare IE1 Dublin DC","text":"<ul> <li> <p>VC0-IS1-IE1 https://10.155.67.10 (credentials)</p> </li> <li> <p>10.155.65.3</p> </li> <li>10.155.65.110</li> <li> <p>10.155.65.113</p> </li> <li> <p>VC1-IS1-IE1 https://10.155.67.51 (credentials)</p> </li> <li> <p>10.155.65.4</p> </li> <li>10.155.65.5</li> <li> <p>10.155.65.112</p> </li> <li> <p>VC2-IS1-IE1 https://10.155.67.52 (credentials)</p> </li> <li> <p>10.155.65.1</p> </li> <li>10.155.65.2</li> <li> <p>10.155.65.111</p> </li> <li> <p>vCenter OLD RDP ie1.ccws.it:43389 (credentials)</p> </li> <li> <p>Backup RDP backup.ie1.ccws.it:43389 (credentials) internal ip: 192.168.100.254</p> </li> </ul>"},{"location":"customers/kooomo-C1362/#licenses","title":"Licenses","text":""},{"location":"customers/kooomo-C1362/#essential","title":"Essential","text":"<ul> <li>VCenter: 0563K-FA0E4-X8K9A-033U0-31L1N VMware vCenter Server 6 Essentials (Instances)</li> <li>ESXi: J0630-6NHE3-E8V8X-011K4-AH9HJ VMware vSphere 6 Essentials Plus (CPUs)</li> </ul>"},{"location":"customers/kooomo-C1362/#enterprise","title":"Enterprise","text":"<ul> <li>VCenter: 602AH-8CL04-K819J-0H8R0-3XQKM VMware vCenter Server 6 Standard (Instances)</li> <li>ESXi: CJ2AJ-FYL5L-6888D-029AK-05WKJ VMware vSphere 6 Enterprise Plus (CPUs)</li> </ul>"},{"location":"customers/kooomo-C1362/#vmotion-network","title":"vMotion network","text":"<ul> <li>vSwitch1 vmotion_2001 10.155.73. net 255.255.248.0</li> <li>vSwitch2 vmotion_2002 10.155.81. net 255.255.248.0</li> </ul>"},{"location":"customers/kooomo-C1362/#services","title":"Services","text":"<ul> <li>Foreman https://provision.kooomo.com</li> <li>SSH root@login.kooomo.com (VPN Criticalcase)</li> </ul> Bash<pre><code># Networks\nLegacy: 192.168.100.0/24 kooomo.local\nNFS: 192.168.200.0/24\nDMZ: 192.168.106.0/24 dmz.kooomo\nK8S: 192.168.107.0/24 k8s.kooomo\nNET: 192.168.108.0/24 dmz.kooomo\nK8S cust: 192.168.109.0/24 cust-k8s.kooomo\nMNG: 10.155.64.0/21\nILO: 10.155.32.0/20\n</code></pre>"},{"location":"customers/kooomo-C1362/#firewall","title":"Firewall","text":"<p>External:</p> <ul> <li>https://firewall-secure.kooomo.com/</li> </ul> <p>Internal:</p> <ul> <li>https://192.168.100.2</li> <li>https://192.168.100.3</li> </ul>"},{"location":"customers/kooomo-C1362/#vpn","title":"VPN","text":"<p>Lo spazio di indirizzamento di zerogrey collide con quello di TO2, le 2 vpn sono incompatibili.</p> <ul> <li>Download client https://gp-vpn.kooomo.com/</li> <li>Connect to: gp-vpn.kooomo.com</li> <li>Setting Up The Vpn</li> </ul>"},{"location":"customers/kooomo-C1362/#known-issues","title":"Known issues","text":""},{"location":"customers/kooomo-C1362/#windows","title":"Windows","text":"<p>Su alcuni sistemi si sono verificati problemi di permessi sulla cartella che fa capo alla variabile di sistema <code>%TEMP%</code>, in cui gli installer andranno a scompattare i propri setup files.</p> <p>Es. VM Windows 10: la variabile punta a <code>C:\\Windows\\Temp</code></p> <p>Creare la cartella e impostare i permessi come segue: https://answers.microsoft.com/en-us/windows/forum/windows_7-windows_programs/error-code-2203-when-trying-to-install-a-program/a1461ca8-f0fd-406d-ae91-c3afc4cf22ce</p>"},{"location":"customers/kooomo-C1362/#monitoring","title":"Monitoring","text":""},{"location":"customers/kooomo-C1362/#observium","title":"Observium","text":"<ul> <li>https://prodmonit02.kooomo.local/overview/ (solo da vpn)</li> <li>https://network.kooomo.com</li> </ul>"},{"location":"customers/kooomo-C1362/#zabbix","title":"Zabbix","text":"<ul> <li> <p>https://monitoring-int.kooomo.com</p> </li> <li> <p>http://prodmonit06.kooomo.local/</p> </li> </ul>"},{"location":"customers/kooomo-C1362/#prometheus","title":"Prometheus","text":"<ul> <li>https://prom-mon-int.kooomo.com/</li> </ul>"},{"location":"customers/kooomo-C1362/#grafana","title":"Grafana","text":"<ul> <li>https://mon-grafana.kooomo.com/</li> </ul>"},{"location":"customers/kooomo-C1362/#pmm-mysql","title":"PMM MySQL","text":"<ul> <li>https://prodmonit04.kooomo.local (solo da vpn)</li> </ul>"},{"location":"customers/kooomo-C1362/#datadog","title":"Datadog","text":"<ul> <li>https://app.datadoghq.eu/account/login?redirect=f</li> </ul>"},{"location":"customers/kooomo-C1362/#autenticazione","title":"Autenticazione","text":"<ul> <li>https://zgpipa01.kooomo.local/ (solo da vpn)</li> <li>https://zgpipa02.kooomo.local/ (solo da vpn)</li> <li>https://ipa.kooomo.com</li> </ul>"},{"location":"customers/kooomo-C1362/#dns-recursor","title":"DNS recursor","text":"Bash<pre><code>192.168.100.4 proddnsre01.kooomo.local\n192.168.100.8 proddnsre01.kooomo.local\n</code></pre> <p>Internal authoritative:</p> Bash<pre><code>allow-from: \"192.168.100.0/24, 10.33.33.0/24\"\nforward-zones: kooomo.local=192.168.100.241;192.168.100.242, 100.168.192.in-addr.arpa=192.168.100.241;192.168.100.242\n</code></pre> <p>Managed by https://git.criticalcase.com/ops/zg-ansible/blob/master/group_vars/pdns-recursor.yml</p>"},{"location":"customers/kooomo-C1362/#domain-and-dns","title":"Domain and DNS","text":"<p>The customers uses NS1 to manage his Domain: (credentials)</p>"},{"location":"customers/kooomo-C1362/#certificati","title":"Certificati","text":""},{"location":"customers/kooomo-C1362/#gestione-certificati-redirect","title":"Gestione certificati / redirect","text":"<p>I certificati e i redirect si gestiscono da: prodmanag02.kooomo.local Leggere il file /root/README.md per istruzioni</p> <p>you need to copy the \".crt\" , \".key\" and \"intermediate-ca.crt\" to one file with \".pem\" format.</p> <p>After upgarde the certificate do a validation on one of hapxyxx.dmz.kooomo VMs:</p> <ul> <li>ssh prd-hapxy02.dmz.kooomo</li> <li>haproxy -c -f /etc/haproxy/haproxy.cfg</li> </ul>"},{"location":"customers/kooomo-C1362/#generazione-cname-ipa","title":"Generazione CNAME IPA","text":"Bash<pre><code>ipa host-add prom.kooomo.local --force\nipa service-add prom/prodmonit03.kooomo.local\nipa service-add prom/prom.kooomo.local --force\nipa service-add-host --host=prom.kooomo.local prom/prom.kooomo.local\nipa service-add-host --host=prodmonit03.kooomo.local prom/prom.kooomo.local\nipa-getcert request -r -f /etc/ssl/certs/prom.kooomo.local.crt -k /etc/ssl/private/prom.kooomo.local.key -N CN=prom.kooomo.local -D prom.kooomo.local -K prom/prom.kooomo.local\n</code></pre>"},{"location":"customers/kooomo-C1362/#observium-zerogrey","title":"Observium Zerogrey","text":"<p>Per integrare Observium con ipa la configurazione \u00e8 stata modificata in questo modo:</p> Bash<pre><code>$config['auth_mechanism'] = \"ldap\"; // default, other options: ldap, http-auth, please see documentation for config help\n$config['auth_ldap_version'] = 3; # v2 or v3\n$config['auth_ldap_server'] = \"zgpipa02.kooomo.local\";\n$config['auth_ldap_server'] = \"zgpipa01.kooomo.local\";\n$config['auth_ldap_port'] = 389;\n$config['auth_ldap_starttls'] = OPTIONAL;\n$config['auth_ldap_prefix'] = \"uid=\";\n$config['auth_ldap_suffix'] = \",cn=users,cn=accounts,dc=kooomo,dc=local\";\n$config['auth_ldap_groupbase'] = \"cn=groups,cn=accounts,dc=kooomo,dc=local\";\n$config['auth_ldap_group'] = array(\"cn=ipausers,cn=groups,cn=accounts,dc=kooomo,dc=local\");\n$config['auth_ldap_groupmemberattr'] = 'member';\n$config['auth_ldap_groupmembertype'] = \"fulldn\";\n$config['auth_ldap_groups']['admins']['level'] = 10;\n$config['auth_ldap_groups']['ipausers']['level'] = 1;\n</code></pre> <p>In questo modo gli utenti appartententi al gruppo admins possono modificare la configurazione mentre quelli appartenenti al gruppo ipausers possono solo visualizzare i dati.</p>"},{"location":"customers/kooomo-C1362/#schema-fisico","title":"Schema fisico","text":"<p>Rack1</p> Text Only<pre><code>Router backup\nNausica 100.254\nNodo dismesso\nNodo 114-115-116-117\nMicrocloud\nEquallogic SAS\n\nSwitch 3750\n</code></pre> <p>Rack2</p> Text Only<pre><code>Nodo 110-111-112-113\nPoseidon 192.168.100.20\nDiscone 192.168.100.16\nNodo 122  --  Nodo 123\nRaskolnikov 100.106\nNodo 99 (off)\nNodi 118-119-120-121\nEqual SATA\n</code></pre> <p>Seriali</p> Text Only<pre><code>192.168.100.254 (rete backup) Nausica\n3750 ttyUSB0\n3750 ttyUSB1\n2960 ttyUSB2\n2960 ttyUSB3\nFisica su EQL SAS\n\n192.168.100.20 Nodo fisico r1\n\nFisica 2960 Rack2 Iscsi\n\n192.168.100.16 Nodo fisico Discone\n\nFisica 2960 Rack2 Iscsi\n\n192.168.100.106 Nodo Fisico Raskolnikov\n\n2 Seriali Equaligic SATA\n</code></pre>"},{"location":"customers/kooomo-C1362/#customer-environments","title":"Customer environments","text":"<ul> <li>prd: production</li> <li>sit: staging for nutmeg (should be considered production)</li> <li>stag: staging for all other customers (should be considered production)</li> <li>test: old development</li> <li>dev: new development</li> </ul>"},{"location":"customers/kooomo-C1362/#load-balancers","title":"Load Balancers","text":"<p>Main loadbalancers are prd-hapxy01 and prd-hapxy02. There is a virtual IP on both machine. If someting goes wrong to one VM, the other one will take the VIP ip address. Public IPs are: 217.173.100.66 (prd-hapxy01) and 80.169.138.214,80.169.138.210 (prd-hapxy02).</p> <p>Health checks for prd-* environment are managed by consul.</p> <p>Prod:</p> <ul> <li>https://prd-hapxy01.dmz.kooomo:1936/prd-hapxy01/</li> <li>https://prd-hapxy02.dmz.kooomo:1936/prd-hapxy02/</li> </ul> <p>Test:</p> <ul> <li>https://prd-hapxy03.dmz.kooomo:1936/prd-hapxy03/</li> <li>https://prd-hapxy04.dmz.kooomo:1936/prd-hapxy04/</li> </ul>"},{"location":"customers/kooomo-C1362/#local-haproxy","title":"Local HAProxy","text":"<p>Each -front, -admin, *-workr have a local HAProxy to handle connections to:</p> <ul> <li>MySQL masters</li> <li>MySQL slaves</li> <li>MySQL slaves for batch operations</li> <li>Redis masters</li> <li> <p>Redis slaves</p> </li> <li> <p>UI Test: https://prd-front00.kooomo.local:1936/</p> </li> <li>Es. UI Prod: https://prd-front01.kooomo.local:1936/</li> </ul>"},{"location":"customers/kooomo-C1362/#consul","title":"Consul","text":"<p>Masters are: prd-consul[01:03]</p> <p>To change maintenance mode for a node use:</p> Bash<pre><code>consul maint -enable\nconsul maint -disable\n</code></pre> <p>UI Master: http://prd-consul01.kooomo.local:8500/ui/ie1/nodes</p>"},{"location":"customers/kooomo-C1362/#sql-injection","title":"SQL Injection","text":"Bash<pre><code>git clone https://github.com/sqlmapproject/sqlmap.git\ncd sqlmap\npython sqlmap.py --headers=\"x-requested-with: XMLHttpRequest\" -u \"https://www.slam.com/eshop/eshop/action/addProductToCart/?product_id=38805&amp;sku=118887&amp;quantity=1\" -f -b -dbs\n</code></pre>"},{"location":"customers/kooomo-C1362/#database-sync","title":"Database sync","text":"<p>Check sync mode</p> Bash<pre><code>pt-table-checksum h=prodsqlfr00,P=3306,u=zabbix,p=SUPER_SECURE_PASSWORD --databases=eshop_production_rec --no-check-binlog-format --no-check-replication-filters 2&gt;&amp;1 &gt; table-check2.log\npt-table-checksum h=prodsqlba00,P=3306,u=zabbix,p=SUPER_SECURE_PASSWORD --databases=admin_production --no-check-binlog-format --no-check-replication-filters 2&gt;&amp;1 &gt; table-check.log\n</code></pre> <p>Noop mode</p> Bash<pre><code>pt-table-sync --print --verbose --no-bin-log --tables eshop_production_rec.es_usr_merchant h=prodsqlfr00,u=zabbix,p=SUPER_SECURE_PASSWORD h=192.168.100.202\n</code></pre> <p>Execute</p> Bash<pre><code>pt-table-sync --execute --verbose --no-bin-log --tables eshop_production_rec.es_usr_merchant h=prodsqlfr00,u=zabbix,p=SUPER_SECURE_PASSWORD h=192.168.100.202\n</code></pre>"},{"location":"customers/kooomo-C1362/#redis","title":"Redis","text":"<p>Environments:</p> <ul> <li>prodredca: cache</li> <li>prodredse: session</li> <li>prodredpr: products</li> <li>prodredle: legacy (ex memcache)</li> </ul> <p>Check redis status</p> Bash<pre><code>&gt; CONFIG set appendonly yes\n</code></pre> <p>On slaves</p> Bash<pre><code>&gt; CONFIG set appendonly yes\n</code></pre> <p>On masters</p> Bash<pre><code>&gt; CONFIG set appendonly no\n</code></pre> <p>Failover from sentinel. prodredha01</p> Bash<pre><code>docker ps\ndocker exec -it redis-se-NAME bash\nredis-cli -a $REDIS_MASTER_PASSWORD -p 26379\n&gt; sentinel failover mymaster\n</code></pre>"},{"location":"customers/kooomo-C1362/#planned-maintenances","title":"Planned maintenances","text":"<ul> <li>Kooomo MySQL Masters maintenance 16/01/2020</li> <li>Colt maintenance - 11/02/2020</li> </ul>"},{"location":"customers/kooomo-C1362/#spof","title":"Spof","text":"<ul> <li>prodsqlfr00: MySQL eshop master</li> <li>prodsqlba03: MySQL admin master</li> <li>prd-fsnfs01: Production NFS</li> </ul>"},{"location":"customers/kooomo-C1362/#known-issues_1","title":"Known issues","text":"<ul> <li>vSphere client search isn't working on win-control1. Is working in win-control2</li> </ul>"},{"location":"customers/kooomo-C1362/#files","title":"Files","text":"<ul> <li>Hardware IP Address allocation: datacenter_ie1_kooomo_network.xlsx</li> </ul>"},{"location":"customers/kooomo-C1362/HAProxy/","title":"HAProxy LB in Kooomo","text":"<p>Characteristics:</p> <ul> <li>HAProxy is deployed using Vbernat PPA via ansible.</li> <li>Production HAProxy servers are Highly Available in Active/Active mode.</li> <li>Every request is checked against a nginx + modsecurity pool.</li> <li>Sensitive public endpoint are protected by Authelia.</li> <li>Health checks via consul agents (port 8500).</li> </ul>"},{"location":"customers/kooomo-C1362/HAProxy/#ansible","title":"Ansible","text":"<ul> <li>Ansible Playbook: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/haproxy-lb.yml</li> <li>Ansible Module <code>cc.haproxy</code> to setup haproxy: https://git.criticalcase.com/ansible/cc.haproxy</li> <li>Ansible module <code>cc.haproxy-manager</code> to setup letsencrypt certificates, redirects and custom configurations: https://git.criticalcase.com/ansible/cc.haproxy-manager</li> <li>Ansible module <code>cc.nginx</code> to install custom TLS certificates and validate apple webook for iOS notifications: https://git.criticalcase.com/ansible/cc.nginx</li> <li>Ansible module <code>hadret.restic</code> to backup TLS certificates and configurations: https://github.com/hadret/ansible-role-restic</li> <li>Ansible Module <code>oefenweb.keepalived</code> to setup the VRRP High Availability: https://github.com/Oefenweb/ansible-keepalived</li> </ul>"},{"location":"customers/kooomo-C1362/HAProxy/#env","title":"Env","text":"<p>HAProxy configuration is managed by ansible groups:</p> <ul> <li> <p>Global configuration: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/haproxy_lb.yml</p> </li> <li> <p>Intranet configuration: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/haproxy_lb_int.yaml</p> </li> <li> <p>Intranet VIP: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/haproxy_lb_int_g1.yaml</p> </li> <li> <p>Public configurations: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/haproxy_lb_prod.yml</p> </li> <li> <p>Public VIP: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/haproxy_lb_prod_g1.yaml</p> </li> <li> <p>Manager configuration: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/manager.yml</p> </li> </ul> <p>The full list is avaialable in <code>hosts</code> file: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/hosts</p> <p>VIP configuration is in the hosts file</p> YAML<pre><code># Es.\n[haproxy_lb_prod_g1]\nprd-hapxy01.dmz.kooomo keepalived_vrrp_vi_1_priority=101 keepalived_vrrp_vi_2_priority=100\nprd-hapxy02.dmz.kooomo keepalived_vrrp_vi_1_priority=100 keepalived_vrrp_vi_2_priority=101\n</code></pre>"},{"location":"customers/kooomo-C1362/HAProxy/#consul","title":"Consul","text":"<p>We can check the backend availability throught consul, i.e.:</p> <ul> <li>modsecurity-nginx: http://prd-consul01.kooomo.local:8500/ui/ie1/services/modsecurity-nginx</li> <li>prd-front: http://prd-consul01.kooomo.local:8500/ui/ie1/services/prd-front</li> <li>prd-admin: http://prd-consul01.kooomo.local:8500/ui/ie1/services/prd-front</li> </ul>"},{"location":"customers/kooomo-C1362/HAProxy/#monitoring","title":"Monitoring","text":"<ul> <li>Grafana: https://grafana-mon-int.kooomo.com/d/y9w1bsJWz/haproxy-2-0?orgId=1&amp;refresh=10s&amp;var-host=prd-hapxy01.dmz.kooomo&amp;var-port=1936&amp;var-backend=All&amp;var-frontend=All&amp;var-server=All&amp;var-code=All&amp;var-interval=30s&amp;from=now-3h&amp;to=now</li> <li>Prometheus jobs: https://prom-mon-int.kooomo.com/classic/service-discovery#job-haproxy-https</li> <li>Prometheus exporter: https://prd-hapxy01.dmz.kooomo:1936/metrics</li> <li>HAProxy stats: https://prd-hapxy01.dmz.kooomo:1936/prd-hapxy01/</li> </ul>"},{"location":"customers/kooomo-C1362/HAProxy/#keepalived","title":"Keepalived","text":"<p>The generated configuration is <code>/etc/keepalived/keepalived.conf</code></p> <p>Health check is every second:</p> Text Only<pre><code>vrrp_script chk_haproxy {\n  script \"killall -0 haproxy\"\n  weight 2\n  interval 1\n}\n\ntrack_script {\n  chk_haproxy\n}\n</code></pre>"},{"location":"customers/kooomo-C1362/HAProxy/#authelia","title":"Authelia","text":"<p>Protect applications with Single Sign-On using FreeIPA backend: https://www.authelia.com/</p> <ul> <li>Authelia allows users stored in a LDAP to provide their username and password as first factor.</li> <li>Authelia offers a login portal to allow your users to login once and access everything.</li> </ul> <p>Authelia is installed in <code>prd-hspoa01.dmz.kooomo</code> as docker container.</p>"},{"location":"customers/kooomo-C1362/HAProxy/#acl","title":"acl","text":"YAML<pre><code># Hostname used for authentication\n- string: acl-authelia hdr(host) -i auth.kooomo.com\n\n# Domains protected by authelia\n- string: acl-authelia-protected-fr hdr(host) -i {{ cc_haproxy_acl_protected_domains }}\n</code></pre> <ul> <li>Authelia service: https://auth.kooomo.com</li> <li>Protected service: https://kibana.kooomo.com</li> </ul>"},{"location":"customers/kooomo-C1362/HAProxy/#how-it-works-http_request","title":"How it works? http_request","text":"YAML<pre><code># Save all GET params if present\n- action: set-var(req.questionmark)\n  param: str(?)\n  cond: if { query -m found }\n\n# Save the requested hostname\n- action: set-header\n  param: \"X-Forwarded-Host %[req.hdr(Host)]\"\n\n# Save the requested url to redirect the user after the successful login\n- action: set-header\n  param: \"X-Forwarded-Uri %[path]%[var(req.questionmark)]%[query]\"\n\n# Verify if the users is already authenticated\n- action: lua.auth-request\n  param: bk-authelia /api/verify\n  cond: if acl-authelia-protected-fr\n\n# Do the redirect if not authenticated\n- action: redirect\n  param: location https://auth.kooomo.com/?rd=https://%[base]%[var(req.questionmark)]%[query]\n  cond: if acl-authelia-protected-fr !{ var(txn.auth_response_successful) -m bool }\n</code></pre> <p>Detailed HAProxy integration: https://www.authelia.com/docs/deployment/supported-proxies/haproxy.html</p>"},{"location":"customers/kooomo-C1362/HAProxy/#existing-basic-authentication-in-backend","title":"Existing basic authentication in backend","text":"<p>A single Basic Authentication could be set directly in HAProxy to avoid double authentication to some services like prometheus or alertmanager.</p> YAML<pre><code>- name: bk-kooomo-c1-https\n  mode: http\n  balance: roundrobin\n  acl:\n    - string: acl-prometheus hdr(host) -i alertmanager-c1-int.kooomo.com prometheus-c1-int.kooomo.com\n  http_request:\n    - action: set-header\n      param: Authorization Basic\\ ZGVmYXVsdDpwYXNzd29yZA==\n      cond: if acl-prometheus\n</code></pre>"},{"location":"customers/kooomo-C1362/HAProxy/#manager","title":"Manager","text":"<p>prodmanag02.kooomo.local manage this services:</p> <ul> <li>Letsencrypt certificates requests and renewals</li> <li>Force https redirects</li> <li>Domain redirects</li> <li>URL redirects</li> <li>Custom header parameters: es. Content-Security-Policy</li> <li>Sync parameteres to all HAProxy servers</li> <li>Custom certificates csr/key/crt/pem</li> <li>Apple Pay validation</li> </ul> <p>Check the up-to-date README here: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/manager.yml</p>"},{"location":"customers/kooomo-C1362/HAProxy/#modesecurity","title":"Modesecurity","text":"<p>All requestes are checked by custom Modsecurity rules managed by Kooomo: https://github.com/Zerogrey/modsecurity-rules (credentials)</p> <p></p>"},{"location":"customers/kooomo-C1362/Matomo/","title":"Matomo","text":"<p>Matomo, open source web analytics application to track online visits to one or more websites and display reports on these visits for analysis.</p>"},{"location":"customers/kooomo-C1362/Matomo/#the-kooomo-analytics-website","title":"the kooomo analytics website:","text":"<p>https://analytics.kooomo.com/</p>"},{"location":"customers/kooomo-C1362/Matomo/#credientials","title":"credientials","text":"<p>https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/show/ds/customers/kooomo-C1362/matomo</p> <p>the NFS sotage is used for persistent shared folder (nfs) which will contain the source code and configuration (/var/www/html).</p>"},{"location":"customers/kooomo-C1362/Matomo/#the-nfs-server-vm","title":"the NFS server vm:","text":"<ul> <li>kooomo-c2-d01.k8s.kooomo</li> </ul>"},{"location":"customers/kooomo-C1362/Matomo/#the-storing-data-path-on-the-kooomo-c2-d01k8skooomo-vm","title":"the storing data path on the kooomo-c2-d01.k8s.kooomo vm","text":"<ul> <li>/opt/nfs/matomo</li> </ul> <p>the CronJob is used to run every hour for archiving the logs.</p> <ul> <li>/usr/local/bin/php /var/www/html/console core:archive --url=https://analytics.kooomo.com/</li> <li>the above command runs just against one of the pods.</li> </ul> <p>the other CronJob is used to take a backup from mysql database and push it on s3. the following helm chart is used for the mysql backup purpose:</p> <ul> <li>https://github.com/benjamin-maynard/kubernetes-cloud-mysql-backup</li> </ul> <p>the storing path on s3 :</p> <ul> <li>s3://matamo-mysql-analytics/mysql-matomo/</li> </ul> <p>the matomo custom helm chart is created in the following address:</p> <ul> <li>https://git.criticalcase.com/a.kolahdouzan/kooomo-matomo</li> </ul> <p>the flux configuration for Matomo and mysql is in this address:</p> <ul> <li>https://git.criticalcase.com/flux/kooomo-c2/-/blob/master/releases/matomo-mysql.yaml</li> </ul>"},{"location":"customers/kooomo-C1362/canary/","title":"OpenCanary installation","text":"<p>Installation on prd-hbackup01.kooomo.local (Ubuntu)</p> Bash<pre><code>$ sudo apt-get install python-dev python-pip python-virtualenv\n$ sudo apt-get install -y build-essential libssl-dev libffi-dev python-dev\n$ sudo apt-get install jq -y\n$ sudo apt install ftp\n</code></pre> <p>now, you should make a directory in root/opt/</p> Bash<pre><code> mkdir /opt/canaries\n cd /opt/canaries\n virtualenv env/\n . env/bin/activate\n pip install opencanary\n pip install scapy\n pip install pyasn1\n</code></pre>"},{"location":"customers/kooomo-C1362/canary/#configuring-opencanaryconf","title":"Configuring opencanary.conf","text":"<p>first of all, the sample config should be copied to the home path. Moreover, in case of configuring the services such as Linux web server, Email alret and mysql server this file should be modified.</p> Bash<pre><code>cp /etc/opencanary/opencanary.conf /opt/canaries/env/bin/\nnano /opt/canaries/env/bin/opencanary.conf\n</code></pre> <p>this is the configuration file that we configured according to our needs, please copy the below contents to the opencanary.conf.</p> Bash<pre><code>{\n    \"device.node_id\": \"hbackup-node1\",\n    \"ip.ignorelist\": [  ],\n    \"git.enabled\": false,\n    \"git.port\" : 9418,\n    \"ftp.enabled\": true,\n    \"ftp.port\": 21,\n    \"ftp.banner\": \"FTP server ready\",\n    \"http.banner\": \"Apache/2.2.22 (Ubuntu)\",\n    \"http.enabled\": true,\n    \"http.port\": 80,\n    \"http.skin\": \"nasLogin\",\n    \"http.skin.list\": [\n     {\n         \"desc\": \"Plain HTML Login\",\n         \"name\": \"basicLogin\"\n     },\n     {\n         \"desc\": \"Synology NAS Login\",\n         \"name\": \"nasLogin\"\n     }\n    ],\n    \"httpproxy.enabled\" : false,\n    \"httpproxy.port\": 8080,\n    \"httpproxy.skin\": \"squid\",\n    \"logger\": {\n        \"class\": \"PyLogger\",\n        \"kwargs\": {\n            \"formatters\": {\n                \"plain\": {\n                    \"format\": \"%(message)s\"\n                },\n                \"syslog_rfc\": {\n                    \"format\": \"opencanaryd[%(process)-5s:%(thread)d]: %(name)s %(levelname)-5s %(message)s\"\n                }\n            },\n            \"handlers\": {\n                \"console\": {\n                    \"class\": \"logging.StreamHandler\",\n                    \"stream\": \"ext://sys.stdout\"\n                },\n\n                \"SMTP\": {\n                    \"class\": \"logging.handlers.SMTPHandler\",\n                    \"mailhost\": [\"smtp.office365.com\", 587],\n                    \"fromaddr\": \"services@criticalcase.com\",\n                    \"toaddrs\" : [\"digitalerts@criticalcase.com\", \"developers@kooomo.com\"],\n                    \"subject\" : \"WARNING, OpenCanary abuse alert on internal kooomo network\",\n                    \"credentials\" : [\"services@criticalcase.com\", \"Lul94385\"],\n                    \"secure\" : []\n             },\n\n                \"file\": {\n                    \"class\": \"logging.FileHandler\",\n                    \"filename\": \"/var/tmp/opencanary.log\"\n                }\n            }\n        }\n    },\n    \"portscan.enabled\": false,\n    \"portscan.logfile\":\"/var/log/kern.log\",\n    \"portscan.synrate\": 5,\n    \"portscan.nmaposrate\": 5,\n    \"portscan.lorate\": 3,\n    \"smb.auditfile\": \"/var/log/samba-audit.log\",\n    \"smb.enabled\": false,\n    \"mysql.enabled\": true,\n    \"mysql.port\": 3306,\n    \"mysql.banner\": \"5.5.43-0ubuntu0.14.04.1\",\n    \"ssh.enabled\": false,\n    \"ssh.port\": 22,\n    \"ssh.version\": \"SSH-2.0-OpenSSH_5.1p1 Debian-4\",\n    \"redis.enabled\": false,\n    \"redis.port\": 6379,\n    \"rdp.enabled\": false,\n    \"rdp.port\": 3389,\n    \"sip.enabled\": false,\n    \"sip.port\": 5060,\n    \"snmp.enabled\": false,\n    \"snmp.port\": 161,\n    \"ntp.enabled\": false,\n    \"ntp.port\": 123,\n    \"tftp.enabled\": false,\n    \"tftp.port\": 69,\n    \"tcpbanner.maxnum\":10,\n    \"tcpbanner.enabled\": false,\n    \"tcpbanner_1.enabled\": false,\n    \"tcpbanner_1.port\": 8001,\n    \"tcpbanner_1.datareceivedbanner\": \"\",\n    \"tcpbanner_1.initbanner\": \"\",\n    \"tcpbanner_1.alertstring.enabled\": false,\n    \"tcpbanner_1.alertstring\": \"\",\n    \"tcpbanner_1.keep_alive.enabled\": false,\n    \"tcpbanner_1.keep_alive_secret\": \"\",\n    \"tcpbanner_1.keep_alive_probes\": 11,\n    \"tcpbanner_1.keep_alive_interval\":300,\n    \"tcpbanner_1.keep_alive_idle\": 300,\n    \"telnet.enabled\": false,\n    \"telnet.port\": 23,\n    \"telnet.banner\": \"\",\n    \"telnet.honeycreds\": [\n        {\n            \"username\": \"admin\",\n            \"password\": \"$pbkdf2-sha512$19000$bG1NaY3xvjdGyBlj7N37Xw$dGrmBqqWa1okTCpN3QEmeo9j5DuV2u1EuVFD8Di0GxNiM64To5O/Y66f7UASvnQr8.LCzqTm6awC8Kj/aGKvwA\"\n        },\n        {\n            \"username\": \"admin\",\n            \"password\": \"admin1\"\n        }\n    ],\n    \"mssql.enabled\": false,\n    \"mssql.version\": \"2012\",\n    \"mssql.port\":1433,\n    \"vnc.enabled\": false,\n    \"vnc.port\":5000\n}\n</code></pre>"},{"location":"customers/kooomo-C1362/canary/#running-the-opencanary","title":"Running the OpenCanary","text":"Bash<pre><code>$ sudo -i\ncd /opt/canaries/\n. env/bin/activate\ncd /env/bin\nopencanaryd --start\n</code></pre>"},{"location":"customers/kooomo-C1362/canary/#troubleshooting","title":"Troubleshooting","text":"<p>you can easily stop or restart the service</p> Bash<pre><code>$ opencanaryd --stop\n$ opencanaryd --restart\n</code></pre> <p>The tool JQ can be used to check that the config file is well-formed JSON.</p> Bash<pre><code>$ jq . ~/.opencanary.conf\n</code></pre> <p>Run opencanaryd in the foreground to see more error messages.</p> Bash<pre><code>$ opencanaryd --dev\n</code></pre>"},{"location":"customers/kooomo-C1362/canary/#testing","title":"Testing","text":"<p>Based on testing your target services, you can do some local tests:</p> Bash<pre><code>$ ftp Machine-IP\n$ curl Machine-IP\n</code></pre>"},{"location":"customers/kooomo-C1362/canary/#creating-a-service-in-etcsystemdsystem","title":"Creating a service in etc/systemd/system/","text":"<p>In case of booting up opencanary automatically when the machine is restarted :</p> Bash<pre><code>$ sudo nano /etc/systemd/system/opencanary.serivce\n</code></pre> <p>copying the following codes in the opencanary.service file and save it:</p> Bash<pre><code>[Unit]\nDescription=OpenCanary\nAfter=syslog.target\nAfter=network.target\n\n[Service]\nUser=root\nType=oneshot\nRemainAfterExit=yes\nEnvironment=VIRTUAL_ENV=/opt/canaries/env/\nEnvironment=PATH=$VIRTUAL_ENV/bin:/usr/bin:$PATH\nWorkingDirectory=/opt/canaries/env/bin\nExecStart=/opt/canaries/env/bin/opencanaryd --start\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Creating a link to systemd service and start the service :</p> Bash<pre><code>sudo systemctl enable opencanary.service\nsudo systemctl stop opencanary.service\nsudo systemctl start opencanary.service\n\nsudo systemctl status opencanary.service   #### for checking the service if active or inactive\n</code></pre>"},{"location":"customers/kooomo-C1362/canary/#receiving-alerts-instructions","title":"Receiving alerts instructions","text":"<p>Everytime the service restarts, we will receive four emails like below and you could easily ignore them.</p> <p>WARNING, OpenCanary abuse alert on internal kooomo network {\"logdata\": \"Canary running!!!\"}}, {\"logdata\": \"Added service from class CanaryHTTP in opencanary.modules.http to fake\"}}, {\"logdata\": \"Added service from class CanaryFTP in opencanary.modules.ftp to fake\"}}, {\"logdata\": \"Added service from class CanaryMySQL in opencanary.modules.mysql to fake\"}},</p> <p>Every malicious and supspect actions might be done on the IPs below, the Alret will be sent to your email by OpenCanary:</p> <p>192.168.100.189 192.168.109.34 192.168.108.19 192.168.106.35 192.168.107.34</p> <p>If it is not a false positive, it must be investigated the attack source host and source port. there is a sample attack which has been illustrated in below :</p> <p>WARNING, OpenCanary abuse alert on internal kooomo network</p> <p>{\"dst_host\": \"192.168.100.189\", \"dst_port\": 80, \"local_time\": \"2021-05-07 13:11:05.061727\", \"local_time_adjusted\": \"2021-05-07 13:11:05.061767\", \"logdata\": {\"HOSTNAME\": \"192.168.100.189\", \"PASSWORD\": \"sadf\", \"PATH\": \"/index.html\", \"SKIN\": \"nasLogin\", \"USERAGENT\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\", \"USERNAME\": \"sdf\"}, \"logtype\": 3001, \"node_id\": \"hbackup-node1\", \"src_host\": \"10.33.33.29\", \"src_port\": 48310, \"utc_time\": \"2021-05-07 13:11:05.061752\"}</p>"},{"location":"customers/kooomo-C1362/git-runner/","title":"Git runner","text":"<p>Use ansible to install docker in the VM.</p> <p>Manual install script:</p> Text Only<pre><code># as root\ncd /opt/\nmkdir actions-runner &amp;&amp; cd actions-runner\ncurl -O -L https://github.com/actions/runner/releases/download/v2.273.5/actions-runner-linux-x64-2.273.5.tar.gz\n// Extract the installer\ntar xzf ./actions-runner-linux-x64-2.273.5.tar.gz\n./config.sh --url https://github.com/Zerogrey --token XXXXXX\n\n# as github\nsu - github\ncd /opt/actions-runner\n./svc.sh install github\n\n# as root\nsystemctl enable actions.runner.Zerogrey.prd-gitru05\nsystemctl start actions.runner.Zerogrey.prd-gitru05\n\ndocker ps\n</code></pre>"},{"location":"customers/kooomo-C1362/mysql/","title":"MySQL","text":"<p>Characteristics:</p> <ul> <li><code>Master(1)</code>/<code>Slave(N)</code> configuration.</li> <li>Consul detects lagging slaves</li> <li>PMM: Percona Monitoring Tools</li> </ul>"},{"location":"customers/kooomo-C1362/mysql/#ansible","title":"Ansible","text":"<ul> <li>Ansible Playbook: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/percona.yml</li> <li>Ansible Module <code>kooomo.backup</code>: https://git.criticalcase.com/ops/zg-ansible/-/tree/master/roles/kooomo.backup</li> <li>Ansible Module <code>cc.percona-server</code>: https://git.criticalcase.com/ansible/cc.percona-server</li> <li>Ansible Module <code>cc.zabbix-mysql</code>: https://git.criticalcase.com/ansible/cc.zabbix-mysql</li> <li>Ansible Module <code>timorunge.pmm_client</code>: https://github.com/timorunge/ansible-pmm-client</li> </ul>"},{"location":"customers/kooomo-C1362/mysql/#env","title":"Env","text":"<ul> <li>Admin prodsqlbaXX https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/prodsqlba_slaves.yml</li> <li>Eshop prodsqlfrXX https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/prodsqlfr_slaves.yml</li> </ul>"},{"location":"customers/kooomo-C1362/mysql/#production-mysql-usage-scheme-by-the-kooomo-app","title":"Production MySQL usage scheme by the Kooomo APP","text":"<p>!</p> <ul> <li>eshop: 3318 master, 3308 slave</li> <li>admin: 3317 master, 3307 slave, 3310 batch</li> </ul>"},{"location":"customers/kooomo-C1362/mysql/#mysql-production","title":"MySQL Production","text":"<ul> <li>Main configuration: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/prodsql_sql.yml</li> </ul>"},{"location":"customers/kooomo-C1362/mysql/#consul","title":"Consul","text":"<p>We can check the MySQL availability throught cosul, i.e. we use eshop cluster:</p> <ul> <li>MySQL Master: http://prd-consul01.kooomo.local:8500/ui/ie1/services/prodsqlfr-master</li> <li>MySQL Slaves: http://prd-consul01.kooomo.local:8500/ui/ie1/services/prodsqlfr-slaves</li> <li>Non-lagging MySQL slaves: http://prd-consul01.kooomo.local:8500/ui/ie1/services/prodsqlfr-slaves-nolag</li> <li>Slaves view: http://prd-consul01.kooomo.local:8500/ui/ie1/nodes/prodsqlfr02</li> </ul>"},{"location":"customers/kooomo-C1362/mysql/#backup","title":"Backup","text":"<p>Backup is done by <code>prodsqlfr01</code> and <code>prodsqlba01</code>. This VM is not in the main loadbalancer pool.</p> <p>Is done by a very old script in <code>/opt/backupdb.sh</code></p>"},{"location":"customers/kooomo-C1362/mysql/#legacy-centralized-haproxy-prodsqlha01","title":"Legacy centralized HAProxy: prodsqlha01","text":"<p>Legacy HAProxy is manged by prodsqlha01.</p> <p>This VM is used as fallback for every environments if for some reason consul will not be available.</p> <p>ToDo: servers aren't updated since is less used. We should update this VM to ensure that the fallback is working well.</p> <p>Configuration details: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/host_vars/prodsqlha01.kooomo.local.yml</p>"},{"location":"customers/kooomo-C1362/mysql/#monitoring","title":"Monitoring","text":"<ul> <li>Grafana: https://pmm-int.kooomo.com</li> <li>Zabbix: https://monitoring-int.kooomo.com/zabbix/hosts.php?filter_host=prodsql</li> </ul>"},{"location":"customers/kooomo-C1362/mysql/#migration-queries","title":"Migration queries","text":"<ul> <li>prodsqlfr00.kooomo.local: <code>/root/move_old_data.sh</code></li> <li>prodsqlba03.kooomo.local: <code>/root/new-queries/move_old_data.sh</code></li> </ul>"},{"location":"customers/kooomo-C1362/mysql/#automate-staging-replacement","title":"Automate staging replacement:","text":"<p>Backup and restore tools: https://git.criticalcase.com/ops/zg-percona</p> <ul> <li>prodsqlfr01.kooomo.local: <code>/root/percona/remote/update_staging_eshop</code></li> <li>prodsqlba01.kooomo.local: <code>/root/percona/remote/update_staging_admin</code></li> </ul>"},{"location":"customers/kooomo-C1362/mysql/#useful-commands","title":"Useful commands","text":"Bash<pre><code># Replication\nmysql -e \"show master status \\G\"\nmysql -e \"show slave status \\G\"\n\n# Max connections\nmysql -e \"SHOW VARIABLES LIKE 'max_connections';\"\nmysql -e \"SET GLOBAL max_connections = 10000;\"\n\n# innodb_buffer_pool_size\nmysql -e \"SHOW VARIABLES LIKE 'innodb_buffer_pool_size'\"\nmysql -e \"SET GLOBAL innodb_buffer_pool_size=17179869184\"\n\n# query_cache_size\nmysql -e \"SHOW VARIABLES LIKE 'query_cache_size'\"\nmysql -e \"SET GLOBAL query_cache_size = 1073741824\"\n</code></pre>"},{"location":"customers/kooomo-C1362/redis/","title":"Redis in Kooomo","text":"<p>Characteristics:</p> <ul> <li>Redis is deployed inside a docker container: it's easier to update and rollback</li> <li><code>network_mode: host</code> guarantees high performance like directly installation on the host.</li> <li>A redis exporter is deployed in the same VM, useful to have metrics in prometheus.</li> <li>Consul checks the redis health and the primary/replica state</li> <li>Logs are sent to an elasticsearch cluster</li> </ul>"},{"location":"customers/kooomo-C1362/redis/#ansible","title":"Ansible","text":"<ul> <li>Ansible Playbook: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/redis-docker.yml</li> <li>Ansible Module <code>cc.docker</code> to setup docker and docker-compose: https://git.criticalcase.com/ansible/cc.docker</li> <li>Ansible Module <code>cc.thp</code> to setup the right transparent hugepage: https://git.criticalcase.com/ansible/cc.thp</li> <li>Ansible module <code>cc.haproxy</code> to setup load balancer: https://git.criticalcase.com/ansible/cc.haproxy</li> <li>Ansible module <code>elastic.beats</code> to send logs into elasticsearch: https://github.com/elastic/ansible-beats</li> </ul>"},{"location":"customers/kooomo-C1362/redis/#env","title":"Env","text":"<p>Redis clusters are managed by ansible groups:</p> <ul> <li>Redis cache: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/prodredca.yaml</li> <li>Redis legacy: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/prodredle.yaml</li> <li>Redis products: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/prodredpr.yaml</li> <li>Redis sessions: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/prodredse.yaml</li> <li>Redis bruteforce: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/host_vars/prodredbf01.kooomo.local.yml</li> </ul> <p>The full list is avaialable in <code>hosts</code> file: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/hosts</p>"},{"location":"customers/kooomo-C1362/redis/#production-redis-usage-scheme-by-the-kooomo-app","title":"Production redis usage scheme by the Kooomo APP","text":"<p>Recently we added a batch pool for:</p> <ul> <li>Cache: TCP Port 20022</li> <li>Product: TCP Port 20024</li> <li>Legacy: TCP Port 20026</li> </ul>"},{"location":"customers/kooomo-C1362/redis/#redis-production","title":"Redis production","text":"<p>Main configuration: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/redis_docker_prod.yml</p>"},{"location":"customers/kooomo-C1362/redis/#consul","title":"Consul","text":"<p>We can check the redis availability throught consul, i.e. we use the redis legacy servers:</p> <ul> <li>Primary: http://prd-consul01.kooomo.local:8500/ui/ie1/services/redis-legacy</li> <li>Replica: http://prd-consul01.kooomo.local:8500/ui/ie1/services/redis-legacy-slave</li> <li>Server view: http://prd-consul01.kooomo.local:8500/ui/ie1/nodes/prodredle01</li> </ul>"},{"location":"customers/kooomo-C1362/redis/#redis-sentinel-prodredha01","title":"Redis sentinel: prodredha01","text":"<p>Redis sentinel is manged by prodredha01. Sentinel is used to automatically failover a fault primary node using an available replica.</p> <p>Redis configured with <code>--replica-priority 0</code> can't be promoted to master.</p> <p>It's deployed in a docker container, so it's easy to separate sentinel for each environment.</p> Bash<pre><code>root@prodredha01.kooomo.local:~# docker ps\nCONTAINER ID        IMAGE                          COMMAND                  CREATED             STATUS              PORTS                        NAMES\n29d6021fb247        bitnami/redis-sentinel:5.0.3   \"/app-entrypoint.sh \u2026\"   12 months ago       Up 11 months        127.0.0.1:26382-&gt;26379/tcp   redis-se-product\n2cb4fb27ed29        bitnami/redis-sentinel:5.0.3   \"/app-entrypoint.sh \u2026\"   24 months ago       Up 11 months        127.0.0.1:26383-&gt;26379/tcp   redis-se-legacy\n6ebe7f0b2928        bitnami/redis-sentinel:5.0.3   \"/app-entrypoint.sh \u2026\"   24 months ago       Up 11 months        127.0.0.1:26381-&gt;26379/tcp   redis-se-cache\ne48774dc2aab        bitnami/redis-sentinel:5.0.3   \"/app-entrypoint.sh \u2026\"   24 months ago       Up 11 months        127.0.0.1:26380-&gt;26379/tcp   redis-se-session\n</code></pre> <p>Docker containers are managed by docker-compose in: <code>/opt/docker-compose.yaml</code>. This configuration is managed by Ansible in https://git.criticalcase.com/ops/zg-ansible/-/blob/master/host_vars/prodredha01.kooomo.local.yml</p> <p>If you need to upgreade or delete a container you must update <code>REDIS_MASTER_HOST</code> with the right master IP.</p>"},{"location":"customers/kooomo-C1362/redis/#legacy-centralized-haproxy-prodredha01","title":"Legacy centralized HAProxy: prodredha01","text":"<p>Legacy HAProxy is manged by prodredha01.</p> <p>This VM is used as fallback for every environments if for some reason consul will not be available.</p> <p>ToDo: servers aren't updated since is less used. We should update this VM to ensure that the fallback is working well.</p> <p>Configuration details: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/redis_docker_ha.yml</p>"},{"location":"customers/kooomo-C1362/redis/#redisaio-all-in-one-for-development","title":"RedisAIO: all in one for development","text":"<p>Configuration details: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/docker_aio.yml</p>"},{"location":"customers/kooomo-C1362/redis/#monitoring-and-logging","title":"Monitoring and logging","text":"<ul> <li>Grafana: https://grafana-mon-int.kooomo.com/d/qDDGAhdGz/new-redis-dashboard-for-prometheus-redis-exporter-1-x?orgId=1</li> <li>Prometheus: https://prom-mon-int.kooomo.com/classic/service-discovery#job-redis</li> <li>HAProxy (from a front) search by <code>bk-prodred</code>: https://prd-front01.kooomo.local:1936</li> <li>Elasticsearch discover: kibana-elastic1-c2-int.kooomo.com</li> </ul>"},{"location":"customers/kooomo-C1362/redis/#debug-into-the-redis-container","title":"Debug into the redis container","text":"<p>We could use for example a redis-legacy: <code>prodredle06.kooomo.local</code></p> Bash<pre><code>\u279c ssh prodredle06.kooomo.local\n\u279c sudo docker ps\n\nCONTAINER ID        IMAGE                             COMMAND                  CREATED             STATUS              PORTS               NAMES\n7b5cb3ec233f        bitnami/redis:5.0.9               \"/opt/bitnami/script\u2026\"   6 months ago        Up 5 months                             redis-legacy\n6d18e002bfbc        oliver006/redis_exporter:v1.9.0   \"/redis_exporter\"        6 months ago        Up 5 months                             redis-legacy-exporter\n\n# Show logs (last 100 lines)\n\u279c sudo docker logs -f --tail=100 redis-legacy\n\n\u279c sudo docker exec -it redis-legacy bash\nI have no name!@prodredle06:/$\n\n\u279c redis-cli -a $REDIS_PASSWORD info replication\n\n# Replication\nrole:slave\nmaster_host:192.168.100.98\nmaster_port:6379\nmaster_link_status:up\nmaster_last_io_seconds_ago:0\nmaster_sync_in_progress:0\nslave_repl_offset:1864089556436\nslave_priority:0\nslave_read_only:1\nconnected_slaves:0\n[...]\n</code></pre>"},{"location":"customers/kooomo-C1362/redis/#upgrade-procedure","title":"Upgrade procedure","text":""},{"location":"customers/kooomo-C1362/redis/#ansible_1","title":"Ansible","text":"<p>Use ansible to update image versions to the VM: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/redis_docker_prod.yml</p> YAML<pre><code>kooomo_redis_image: bitnami/redis:X.X.X\nkooomo_redis_exporter_image: oliver006/redis_exporter:vX.X.X\n</code></pre> <p>Double check the master hosts in group_vars and update if is outdated: https://git.criticalcase.com/ops/zg-ansible/-/blob/master/group_vars/prodredle.yaml</p> YAML<pre><code>kooomo_redis_master_host: 192.168.100.XXX\n</code></pre> <p>You could use the <code>info replication</code> command we have seen before</p> <p>Use ansible check mode before before execute apply the changes</p> Bash<pre><code>ansible-playbook -l redis_docker redis-docker.yml --limit prodredle06.kooomo.local --tags facts,docker-compose --check\n\n# When you're sure\nansible-playbook -l redis_docker redis-docker.yml --limit prodredle06.kooomo.local --tags facts,docker-compose --check\n</code></pre>"},{"location":"customers/kooomo-C1362/redis/#container-update","title":"Container update","text":"Bash<pre><code># Connect as root and put the VM in maintenance\n\u279c consul maint -enable\n\n# (optional) Do a manual BGSAVE to speedup the restor process\n\u279c docker exec -it redis-legacy bash\n\u279c redis-cli -a $REDIS_PASSWORD bgsave\nBackground saving started\n\n\u279c exit\n\u279c cd /opt/\n\n# AOF will trigger a full rsync from the master.\n# Delete the file to speedup the process\n\u279c rm -f /opt/redis-legacy/redis/data/appendonly.aof\n\n# Download new images\n\u279c docker-compose pull\n\n# Wait for bgsave ends...\n\u279c docker logs -f redis-legacy --tail=50 | grep \"Background saving terminated with success\"\n1:S 25 Feb 2021 11:34:41.598 * Background saving terminated with success\n\n# Do the upgrade\n\u279c docker-compose up -d\n\n# Check logs\n\u279c docker logs -f redis-legacy\n\n1:S 25 Feb 2021 11:42:25.553 # Server initialized\n1:S 25 Feb 2021 11:42:58.764 * DB loaded from disk: 33.211 seconds\n1:S 25 Feb 2021 11:42:58.764 * Before turning into a replica, using my master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.\n1:S 25 Feb 2021 11:42:58.764 * Ready to accept connections\n1:S 25 Feb 2021 11:42:58.764 * Connecting to MASTER 192.168.100.98:6379\n1:S 25 Feb 2021 11:42:58.765 * MASTER &lt;-&gt; REPLICA sync started\n1:S 25 Feb 2021 11:42:58.765 * Non blocking connect for SYNC fired the event.\n1:S 25 Feb 2021 11:42:58.766 * Master replied to PING, replication can continue...\n1:S 25 Feb 2021 11:42:58.767 * Trying a partial resynchronization (request 0a6d9a076d30c8058b668ff0e3d0de31508bf23a:1865705288040).\n1:S 25 Feb 2021 11:42:58.768 * Successful partial resynchronization with master.\n1:S 25 Feb 2021 11:42:58.768 * MASTER &lt;-&gt; REPLICA sync: Master accepted a Partial Resynchronization.\n\n# Remove Maintenance\n\u279c consul maint -disable\n</code></pre>"},{"location":"customers/kooomo-C1362/redis/#proposal-for-new-replication-strategy","title":"Proposal for new replication strategy","text":"<p>It's only a proposal, is not in production YET</p> <p>Initial situation</p> Text Only<pre><code>primary01:\n           &gt; replica02 &gt; replica05\n           &gt; replica03 &gt; replica06\n           &gt; replica04\n</code></pre> <p><code>primary01</code> is the <code>old-primary</code></p> <p>Select the <code>new-primary</code> (i.e. <code>replica02</code>) and move all the replica attached to the <code>old-primary</code> to the <code>new-primary</code>.</p> <p>Partial syncronization will be used during this change!</p> Bash<pre><code># ssh on replica03 and replica04\nredis-cli REPLICAOF new-primary-ip-address 6379\n</code></pre> <p><code>old-primary</code> has only one replica, the <code>new-primary</code></p> Text Only<pre><code>primary01:\n           &gt; replica02 &gt; replica05\n                                 &gt; replica03 &gt; replica06\n                                 &gt; replica04\n</code></pre> <p>Connect to the <code>new-primary</code> and lower the <code>slave-priority</code></p> Bash<pre><code># ssh on replica02\nredis-cli CONFIG SET slave-priority 10\n</code></pre> <p>Connect to the sentinel, check the slave-priority</p> <p>Each replica must have slave-priority = 0 or &gt; new-primary</p> Bash<pre><code># ssh on redis-sentinel\n#      redis-cli -p 26381 # redis-se-cache\n#      redis-cli -p 26382 # redis-se-product\n#      redis-cli -p 26383 # redis-se-legacy\n#      redis-cli -p 26380 # redis-se-session\n\nredis-cli -p XXXXX SENTINEL REPLICAS mymaster\n\n1)  1) \"name\"\n    2) \"replica3\"\n   37) \"slave-priority\"\n   38) \"100\"\n2)  1) \"name\"\n    2) \"replica4\"\n   37) \"slave-priority\"\n   38) \"0\"\n3)  1) \"name\"\n    2) \"replica2\"\n   37) \"slave-priority\"\n   38) \"10\"\n</code></pre> <p>To minimize downtime open two ssh console one in the old-primary and one in sentinel</p> <p>Connect to the <code>old-primary</code> and lock all operations for <code>30s</code> and put the node in maintenance.</p> Bash<pre><code># ssh on primary01\nredis-cli DEBUG sleep 30\n\nconsul maint -enable\n</code></pre> <p>Start the sentinel failover immediatly!</p> Bash<pre><code># Do the FAILOVER!\nredis-cli -p XXXXX SENTINEL FAILOVER mymaster\n</code></pre> <p>New topology after the failover (needs to be checked)</p> Text Only<pre><code>primary02: &gt; replica05\n           &gt; replica03 &gt; replica06\n           &gt; replica04\n           &gt; replica01\n</code></pre>"},{"location":"customers/kooomo-C1362/teleport/","title":"Teleport Kooomo","text":""},{"location":"customers/kooomo-C1362/teleport/#kubernetes","title":"Kubernetes","text":"YAML<pre><code>kind: role\nmetadata:\n  id: 1647942189705432963\n  name: k8s-limited-ns\nspec:\n  allow:\n    kubernetes_groups:\n      - developers\n    kubernetes_labels:\n      \"*\": \"*\"\n    rules:\n      - resources:\n          - pods/exec\n        verbs:\n          - list\n          - create\n          - read\n          - update\n          - delete\n      - resources:\n          - token\n        verbs:\n          - list\n          - create\n          - read\n          - update\n          - delete\n  deny: {}\n  options:\n    cert_format: standard\n    desktop_clipboard: true\n    enhanced_recording:\n      - command\n      - network\n    forward_agent: false\n    max_session_ttl: 30h0m0s\n    port_forwarding: true\n    record_session:\n      desktop: true\nversion: v4\n</code></pre> YAML<pre><code>kind: role\nmetadata:\n  id: 1646219743307399609\n  name: k8s-admin\nspec:\n  allow:\n    kubernetes_groups:\n      - \"{{internal.kubernetes_groups}}\"\n      - system:masters\n    kubernetes_labels:\n      \"*\": \"*\"\n    kubernetes_users:\n      - \"{{internal.kubernetes_users}}\"\n      - system:masters\n    logins:\n      - root\n    node_labels:\n      \"*\": \"*\"\n    rules:\n      - resources:\n          - role\n        verbs:\n          - list\n          - create\n          - read\n          - update\n          - delete\n      - resources:\n          - auth_connector\n        verbs:\n          - list\n          - create\n          - read\n          - update\n          - delete\n      - resources:\n          - session\n        verbs:\n          - list\n          - read\n      - resources:\n          - trusted_cluster\n        verbs:\n          - list\n          - create\n          - read\n          - update\n          - delete\n      - resources:\n          - event\n        verbs:\n          - list\n          - read\n      - resources:\n          - user\n        verbs:\n          - list\n          - create\n          - read\n          - update\n          - delete\n      - resources:\n          - pods\n        verbs:\n          - list\n          - create\n          - read\n          - update\n          - delete\n      - resources:\n          - token\n        verbs:\n          - list\n          - create\n          - read\n          - update\n          - delete\n  deny: {}\n  options:\n    cert_format: standard\n    desktop_clipboard: true\n    enhanced_recording:\n      - command\n      - network\n    forward_agent: false\n    max_session_ttl: 30h0m0s\n    port_forwarding: true\n    record_session:\n      desktop: true\nversion: v4\n</code></pre>"},{"location":"customers/kooomo-C1362/events/2020-01-16-mysql-master-failover/","title":"Kooomo MySQL Masters maintenance (Rev 1.5 - 16/01/2020)","text":"<p>Backups are configured to start after 3AM UTC.</p>"},{"location":"customers/kooomo-C1362/events/2020-01-16-mysql-master-failover/#prepare-kooomo","title":"Prepare - kooomo","text":"<ol> <li>Stop crons and workers</li> <li>Set a prorper maintenance page on index.php (to be done)</li> </ol>"},{"location":"customers/kooomo-C1362/events/2020-01-16-mysql-master-failover/#do-the-migration-criticalcase","title":"Do the migration - criticalcase","text":"<ol> <li>Disable new prd-frontXX from prodhapxy01 and prodhapxy02</li> <li> <p>Make the new master writable (prodsqlfr00 / prodsqlba03)</p> Text Only<pre><code>$ vi /etc/mysql/my.cnf\n#\u00a0remove line: `read_only`\n\n$ mysql -e 'set global read_only=0;'\n$ mysql -e 'show variables like \"read_only\";'\n</code></pre> </li> <li> <p>Read lock to the old master (192.168.100.30 / 192.168.100.41)     We need to do this command in a separate window - The downtime begin.</p> Text Only<pre><code>$ screen\n$ mysql\n\nmysql&gt; FLUSH TABLES WITH READ LOCK;\n</code></pre> </li> <li> <p>In the old master other window:</p> Text Only<pre><code>$ mysql -e 'set global read_only=1;'\n$ mysql -e 'show variables like \"read_only\";'\n</code></pre> </li> <li> <p>Wait until all slaves are aligned (prodsqlfr01-07 / prodsqlba00-02)</p> Text Only<pre><code>$ mysql -e \"SHOW SLAVE STATUS \\G\"\n</code></pre> </li> <li> <p>Stop the replication on all slaves</p> Text Only<pre><code>$ mysql -e \"STOP SLAVE;\"\n</code></pre> </li> <li> <p>Turn off old master</p> Text Only<pre><code>$ mysql -e \"SHOW MASTER STATUS \\G\"\n$ /etc/init.d/mysql stop\n</code></pre> </li> <li> <p>On new master</p> Text Only<pre><code>$ mysql -e \"RESET SLAVE ALL;\"\n$ mysql -e \"SHOW MASTER STATUS \\G\"\n$ mysql -e \"RESET MASTER;\"\n$ mysql -e \"SHOW MASTER STATUS \\G\"\n</code></pre> </li> </ol> <p>New master is now ready!</p> <ol> <li> <p>Connect to one slave (prodsqlfr02 / prodsqlba01)</p> Text Only<pre><code>#\u00a0WARNING Command for eshop\n$ mysql -e \"CHANGE MASTER TO MASTER_HOST = '192.168.100.206';\" #\u00a0prodsqlfr00\n\n#\u00a0WARNING Command for admin\n$ mysql -e \"CHANGE MASTER TO MASTER_HOST = '192.168.100.143';\" #\u00a0prodsqlba03\n\n$ mysql -e \"SHOW SLAVE STATUS \\G\"\n\n$ mysql -e \"START SLAVE;\"\n$ mysql -e \"SHOW SLAVE STATUS \\G\"\n</code></pre> </li> <li> <p>Repeat for all other slaves</p> </li> <li> <p>Change HAProxy master endpoints to prd-frontXX</p> </li> <li>Change HAProxy master endpoints to prodsqlha01</li> </ol>"},{"location":"customers/kooomo-C1362/events/2020-01-16-mysql-master-failover/#post-migration-kooomo-criticalcase","title":"Post migration - kooomo + criticalcase","text":"<ol> <li>Change HAProxy master endpoints in the code (front/admin/workers)</li> <li>Check</li> </ol>"},{"location":"customers/kooomo-C1362/events/2020-01-16-mysql-master-failover/#go-live-kooomo","title":"Go live - kooomo","text":"<ol> <li>Disable the maintenance page</li> <li>Enable crons and workers</li> <li>Double check</li> </ol>"},{"location":"customers/kooomo-C1362/events/2020-01-16-mysql-master-failover/#late-things","title":"Late things","text":"<ol> <li>Check ansible configuration for new masters (now are slaves in ansible)     20b. Migrate move old data script from masters</li> <li>Power off old masters</li> <li>Enjoy</li> </ol>"},{"location":"customers/kooomo-C1362/events/2020-01-16-mysql-master-failover/#useful-commands","title":"Useful commands","text":"<ul> <li> <p>https://mariadb.com/kb/en/change-master-to/</p> <p>mysql&gt; RESET SLAVE ALL;</p> <p># eshop auto</p> Text Only<pre><code>mysql&gt; CHANGE MASTER TO MASTER_HOST = '192.168.100.206', MASTER_USER = 'slaveuser', MASTER_PASSWORD = 'XXXXXXXXXXXX', MASTER_AUTO_POSITION = 1;\n</code></pre> <p># admin auto</p> Text Only<pre><code>mysql&gt; CHANGE MASTER TO MASTER_HOST = '192.168.100.41', MASTER_USER = 'slaveuser', MASTER_PASSWORD = 'XXXXXXXXXXXX', MASTER_AUTO_POSITION = 1;\n</code></pre> <p># eshop with pos</p> Text Only<pre><code>mysql&gt; CHANGE MASTER TO MASTER_HOST = '192.168.100.206', MASTER_USER = 'slaveuser', MASTER_PASSWORD = 'XXXXXXXXXXXX', MASTER_LOG_FILE='prodsqlfr00-master-bin.000001', MASTER_LOG_POS=154;\n</code></pre> <p>#\u00a0admin with pos</p> Text Only<pre><code>mysql&gt; CHANGE MASTER TO MASTER_HOST = '192.168.100.41', MASTER_USER = 'slaveuser', MASTER_PASSWORD = 'XXXXXXXXXXXX', MASTER_LOG_FILE='prodsqlba03-master-bin.000001', MASTER_LOG_POS=154;\n</code></pre> </li> </ul>"},{"location":"customers/kooomo-C1362/events/2020-02-11-colt-network-maintenance/","title":"colt-maintenance","text":""},{"location":"customers/kooomo-C1362/events/2020-02-11-colt-network-maintenance/#colt-maintenance-10-and-11022020","title":"Colt maintenance - 10 and 11/02/2020","text":"<ul> <li>22:00 Colt calls Criticalcase 1h before the scheduled maintenance. They want to start immediately</li> <li>22:30 Maintenance started 30m before</li> <li>22:40 Downtime started: Shutdown old port by Criticalcase</li> <li>22:57 Colt says: the switch is connected to the wrong router port (But they connected the cables to the router few weeks ago)</li> <li>23:01 Warning: Internal connectivity is down. All VMs can't reach the network.</li> <li>23:05 Colt says: their router is not responding</li> <li>23:38 Joaquin is going to the datacenter because colt is unable to restart their router</li> <li>00:26 Joaquin disconnected the cables from the router</li> <li> <p>00:27 Internal connectivity is up</p> </li> <li> <p>01:18 Firewall configuration was updated by Criticalcase following the colt specification give on the phone (not planned):</p> <p>Default gateway: from 80.169.237.45 -&gt; 80.169.138.209   Firewall IP Address: 80.169.237.46/30 -&gt; 80.169.138.212/29</p> </li> <li> <p>01:41 Our firewall can't reach colt router. The Port-Channel5 to colt router was configured in this way (As written by email months before):</p> <p>interface Port-channel5    description \"Router-Colt-New-2G\"    switchport access vlan 10    switchport mode access</p> </li> <li> <p>02:50 We also tried the vlan 11, but nothing is working.</p> </li> <li> <p>02:13 Rollback</p> <p>Default gateway: from 80.169.138.209 -&gt; 80.169.237.45   Firewall IP Address: 80.169.138.212/29 -&gt; 80.169.237.46/30</p> </li> <li> <p>02:40 RabbitMQ network partition detected by Joaquin</p> </li> <li>02:42 We discovered two redis master in the infrastructure: prodredpr01 and prodredpr02. During the internal network disruption Sentinel was unable to reach prodredpr01 and it elected prodredpr02 as master.</li> <li>02:53 prodredpr01 is slave with persistence enabled, prodredpr02 is master without persistence</li> <li>02:59 RabbitMQ is ready</li> </ul>"},{"location":"customers/kooomo-C1362/events/2020-09-96-redis-upgrade/","title":"Redis slave upgrade procedure","text":""},{"location":"customers/kooomo-C1362/events/2020-09-96-redis-upgrade/#from-redis5-to-redis6-18062021","title":"From redis5 to redis6 (18/06/2021)","text":""},{"location":"customers/kooomo-C1362/events/2020-09-96-redis-upgrade/#primary","title":"Primary","text":"<p>Never upgrade the primary node! Do not put consul maintenance on primary node!</p> <p>Check the replica on HAProxy page: https://prd-front00.kooomo.local:1936 <code>bk-prodredca-master</code></p>"},{"location":"customers/kooomo-C1362/events/2020-09-96-redis-upgrade/#replica","title":"Replica","text":"<p>The VM must be a replica.</p> <p>Check the replica on HAProxy page: https://prd-front00.kooomo.local:1936 <code>bk-prodredca-slaves</code></p> Bash<pre><code>docker ps | grep bitnami/redis\ndocker exec -it redis-cache bash\nredis-cli -a $REDIS_PASSWORD info replication\n# Replication\nrole:slave\n[...]\n</code></pre> <p>If role:slave we can proceed</p> <p>Enable maintenance to the target VM</p> Bash<pre><code>consul maint -enable\n</code></pre> <p>Update the docker-compose configuration file via ansible from your PC</p> Bash<pre><code>ansible-playbook -l redis_docker redis-docker.yml --tags facts,docker-compose --limit prodredca05.kooomo.local\n</code></pre> <p>Upgrade redis from the target VM</p> Bash<pre><code>cd /opt\ndocker-compose up -d\n\n# Check logs for successfully upgrade: MASTER &lt;-&gt; REPLICA sync: Master accepted a Partial Resynchronization.\ndocker logs -f redis-cache\n\n# Upgrade APT packages, if there are some question, please answer the default: N\napt-get update -y; apt-get autoremove -y; apt-get install -y containerd.io docker-ce docker-ce-cli; apt-get -y dist-upgrade; apt-get autoremove -y;\n</code></pre> <p>Double check the virtual machine name before reboot!</p> Bash<pre><code># Reboot the VM (add a space before execute reboot)\n reboot\n</code></pre> <p>Disable the maintenance when done</p> Bash<pre><code># Check logs for successfuly container restart: MASTER &lt;-&gt; REPLICA sync: Master accepted a Partial Resynchronization.\n# or\n# MASTER &lt;-&gt; REPLICA sync: Finished with success\n\ndocker logs -f redis-cache\n\n# When done disable the maintenance\nconsul maint -disable\n</code></pre> <p>Check if the replica back in the pool: https://prd-front00.kooomo.local:1936 <code>bk-prodredca-slaves</code></p> <p>Check the output traffic and hit from the cache: https://grafana-mon-int.kooomo.com/d/mAQws19Gz/redis-dashboard-for-prometheus-redis-exporter-1-x-may2020?orgId=1&amp;var-instance=prodredca05.kooomo.local:9121&amp;from=now-1h&amp;to=now&amp;refresh=10s</p>"},{"location":"customers/kooomo-C1362/events/2020-09-96-redis-upgrade/#from-redis4-to-redis5-2020","title":"From redis4 to redis5 (2020)","text":""},{"location":"customers/kooomo-C1362/events/2020-09-96-redis-upgrade/#on-master","title":"On master","text":"<p>Ensure repl-backlog-size &gt;= 256m</p> Bash<pre><code>docker exec -it redis-cache bash\nredis-cli -a $REDIS_PASSWORD config get repl-backlog-size\nexit\n</code></pre> <p>If not</p> Bash<pre><code>docker exec -it redis-cache bash\nredis-cli -a $REDIS_PASSWORD config set repl-backlog-size 256000000\nredis-cli -a $REDIS_PASSWORD config get repl-backlog-size\nexit\n</code></pre>"},{"location":"customers/kooomo-C1362/events/2020-09-96-redis-upgrade/#on-slave","title":"On slave","text":"Bash<pre><code>consul maint -enable;\ndocker exec -it redis-cache bash\n\n# Inside the container (Wait the console)\nredis-cli -a $REDIS_PASSWORD config set appendonly no\nredis-cli -a $REDIS_PASSWORD config set repl-backlog-size 256000000\nredis-cli -a $REDIS_PASSWORD config get appendonly\nredis-cli -a $REDIS_PASSWORD config rewrite\nredis-cli -a $REDIS_PASSWORD bgsave\nexit\n\n# Remove aof\ncd /opt/\nrm /opt/redis-cache/redis/data/appendonly.aof\n\ndocker-compose pull\ndocker logs -f redis-cache --tail=50\n\n# Wait bgsave\ndocker-compose up -d\n\napt-get update -y; apt-get autoremove -y; apt-get install -y containerd.io docker-ce docker-ce-cli; apt-get -y dist-upgrade; apt-get autoremove -y;\n\n reboot\n\n# After the boot\nconsul maint -disable\n</code></pre>"},{"location":"customers/kooomo-C1362/events/2021-03-02-mysql-nfs-migration-firewall/","title":"2021-03-02 MySQL + NFS migration, firewall reboot","text":"<p>Maintenance windows is 1h and starts at midnight CET</p> <p>Useful dashboards to keep open:</p> <ul> <li>Zabbix: https://monitoring.kooomo.com/zabbix/zabbix.php?action=dashboard.view</li> <li>HAProxy: https://grafana-mon-int.kooomo.com/d/y9w1bsJWz/haproxy-2-0?orgId=1&amp;refresh=10s&amp;var-host=prd-hapxy01.dmz.kooomo&amp;var-port=1936&amp;var-backend=All&amp;var-frontend=All&amp;var-server=All&amp;var-code=All&amp;var-interval=30s&amp;from=now-15m&amp;to=now</li> <li>Consul MySQL admin: http://prd-consul01.kooomo.local:8500/ui/ie1/services/prodsqlba-slaves</li> <li>Consul MySQL eshop: http://prd-consul01.kooomo.local:8500/ui/ie1/services/prodsqlfr-slaves</li> <li>Pingdom (Tag: kooomo): https://my.pingdom.com/app/newchecks/checks</li> </ul>"},{"location":"customers/kooomo-C1362/events/2021-03-02-mysql-nfs-migration-firewall/#section-a-prepare-the-maintenance","title":"Section A: Prepare the maintenance","text":""},{"location":"customers/kooomo-C1362/events/2021-03-02-mysql-nfs-migration-firewall/#criticalcase","title":"Criticalcase","text":"<ol> <li> Create maintenace windows on zabbix</li> <li> Get an IP Address for prd-fsnfs on k8s network: 192.168.109.45/24</li> <li> Disable pingdom HTTP checks</li> </ol>"},{"location":"customers/kooomo-C1362/events/2021-03-02-mysql-nfs-migration-firewall/#kooomo","title":"Kooomo","text":"<ol> <li> Disable pingdom</li> <li> Stop worker and crons</li> <li> Set maintenance page</li> </ol>"},{"location":"customers/kooomo-C1362/events/2021-03-02-mysql-nfs-migration-firewall/#section-b-urgent-tasks","title":"Section B: urgent tasks","text":""},{"location":"customers/kooomo-C1362/events/2021-03-02-mysql-nfs-migration-firewall/#criticalcase_1","title":"Criticalcase","text":"<ol> <li> <p>VM prodsqlfr00</p> </li> <li> <p>Power off prodsqlfr00</p> </li> <li>Upgrade RAM to 32G</li> <li>Unregister VM from old cluster</li> <li>Register VM into the new cluster</li> <li>Setup network interface</li> <li>Power on</li> <li>Answer the migration question: Moved</li> <li> <p>Migrate to VMFS6</p> </li> <li> <p>VM prodsqlba03</p> </li> <li> <p>Power off prodsqlba03</p> </li> <li>Unregister VM from old cluster</li> <li>Register VM into the new cluster</li> <li>Setup network interface</li> <li>Power on</li> <li>Answer the migration question: Moved</li> <li> <p>Migrate to VMFS6</p> </li> <li> <p>VM prd-fsnfs01</p> </li> <li> <p>Power off prd-fsnfs01</p> </li> <li>Unregister VM from old cluster</li> <li>Register VM into the new cluster</li> <li>Setup network interface</li> <li>Add network interface for k8s: <code>kooomo-c1.k8s.kooomo_109</code></li> <li>Power on</li> <li>Answer the migration question: Moved</li> <li> <p>Migrate to VMFS6</p> </li> <li> <p>Enable pingdom https checks</p> </li> </ol>"},{"location":"customers/kooomo-C1362/events/2021-03-02-mysql-nfs-migration-firewall/#kooomo_1","title":"Kooomo","text":"<ol> <li> Disable maintenance page</li> <li> Start workers and crons</li> <li> Enable pingdom</li> </ol>"},{"location":"customers/kooomo-C1362/events/2021-03-02-mysql-nfs-migration-firewall/#section-c-secondary-tasks","title":"Section C: secondary tasks","text":"<ol> <li>Reboot passive firewall</li> <li>Reboot active firewall</li> <li>Do the paloalto test</li> </ol>"},{"location":"customers/kooomo-C1362/events/2021-03-02-mysql-nfs-migration-firewall/#report","title":"Report","text":"<ul> <li>The <code>Section B</code> maintenance was successful. <code>~4 minutes</code> of downtime.</li> <li>The <code>Section C</code> secondary tasks take the whole windows because we rebooted firewall 3 times.</li> <li>We sent the log to Paloalto for the inspection.</li> </ul> <p>Some things we discovered:</p> <ul> <li>New ESXi nodes wasn't resovling DNS names, ntp wasn't updated</li> <li>The paloalto 192.168.100.3 is not reporting data via SNMP to zabbix.</li> </ul>"},{"location":"customers/kooomo-C1362/events/2021-06-28-Local-HAProxy-Upgrade-Procedure/","title":"2021-06-28-Local-HAProxy-Upgrade-Procedure","text":"Bash<pre><code># prd-front01\nconsul maint -enable\n\n# OR\nansible -i hosts app_prd -a \"consul maint -enable\" -f 1 --limit 'app_prd_front[0:2]'\n</code></pre> <p>Add HAProxy repoistory:</p> Bash<pre><code># your laptop\nansible-playbook app.yml --tags facts,haproxy-install --limit prd-front01.kooomo.local\n\n# OR\nansible-playbook app.yml --tags facts,haproxy-install --limit 'app_prd_front[0:2]'\n</code></pre> <p>Upgrade HAProxy package on the VM</p> Bash<pre><code># prd-front01\napt-get install -y haproxy\n\n# OR\nansible -i hosts app_prd -a \"apt-get install -y haproxy\" -f 1 --limit 'app_prd_front[0:2]'\n</code></pre> <p>Deploy new HAProxy configuration</p> Bash<pre><code># your laptop\nansible-playbook app.yml --tags facts,haproxy --limit prd-front01.kooomo.local\n\n# OR\nansible-playbook app.yml --tags facts,haproxy --limit 'app_prd_front[0:2]'\n</code></pre> Bash<pre><code># prd-front01\nconsul maint -disable\n\n# OR\nansible -i hosts app_prd -a \"consul maint -disable\" -f 1 --limit 'app_prd_front[0:2]'\n</code></pre>"},{"location":"customers/kooomo-C1362/events/2022-01-26-hardware-maintenance/","title":"2022-01-26 Kooomo Hardware Maintenance","text":"<p>Goal for this maintenance:</p> Node Maintenance VC Note 192.168.100.110 Ram upgrade WIN 5.1 Slot: B5, B6 192.168.100.113 Ram upgrade VC1 Slot: A4, A6, B5, B6. Correctable memory error rate exceeded for DIMM_B2 10.155.65.1 Raid Battery Replacement VC Enterprise 10.155.65.2 Raid Battery Replacement VC Enterprise 10.155.65.4 Raid Battery Replacement VC Enterprise <p>Put the new SAN HDD to the datacenter warehouse</p>"},{"location":"customers/kooomo-C1362/events/2022-01-26-hardware-maintenance/#25-jan-h-2130-gmt","title":"25 Jan - h 21.30 GMT","text":"<p>Set lower redis slave priority to new primary redis:</p> Bash<pre><code>ssh prodredse01.kooomo.local\nsudo -i\ndocker exec -it redis-session bash\nredis-cli CONFIG SET slave-priority 10\n\nssh prodredle01.kooomo.local\nsudo -i\ndocker exec -it redis-legacy bash\nredis-cli CONFIG SET slave-priority 10\n\nssh prodredpr01.kooomo.local\nsudo -i\ndocker exec -it redis-product bash\nredis-cli CONFIG SET slave-priority 10\n</code></pre> <ul> <li>Check replica priority: https://prom-mon-int.kooomo.com/graph?g0.expr=redis_slave_priority%20%3C%20100%20and%20redis_slave_priority%20%3E%200&amp;g0.tab=1&amp;g0.stacked=0&amp;g0.range_input=1h</li> </ul>"},{"location":"customers/kooomo-C1362/events/2022-01-26-hardware-maintenance/#25-jan-h-2200-gmt","title":"25 Jan - h 22.00 GMT","text":""},{"location":"customers/kooomo-C1362/events/2022-01-26-hardware-maintenance/#connect-to-primary-redis-and-do-the-failover","title":"Connect to primary redis and do the failover","text":"<ul> <li>Check primary redis: https://prom-mon-int.kooomo.com/graph?g0.expr=redis_connected_slaves%20%3E%200&amp;g0.tab=1&amp;g0.stacked=0&amp;g0.range_input=1h</li> </ul> Bash<pre><code># prodredse03\nssh prodredse03.kooomo.local\nsudo -i\ndocker exec -it redis-session bash\nredis-cli DEBUG sleep 45\n\n# prodredha01\nssh root@prodredha01.kooomo.local redis-cli -p 26380 SENTINEL FAILOVER mymaster\n\n## prodredse03: put the node in maintenace\nssh prodredse03.kooomo.local consul maint -enable\n\n# prodredle03\nssh prodredle03.kooomo.local\nsudo -i\ndocker exec -it redis-legacy bash\nredis-cli DEBUG sleep 45\n\n# prodredha01\nssh root@prodredha01.kooomo.local redis-cli -p 26383 SENTINEL FAILOVER mymaster\n\n# prodredle03: Put the node in maintenace\nssh prodredle03.kooomo.local consul maint -enable\n\n# prodredpr02\nssh prodredpr02.kooomo.local\nsudo -i\ndocker exec -it redis-product bash\nredis-cli DEBUG sleep 45\n\n# prodredha01\nssh root@prodredha01.kooomo.local redis-cli -p 26382 SENTINEL FAILOVER mymaster\n\n## prodredpr02: put the node in maintenace\nssh prodredpr02.kooomo.local consul maint -enable\n</code></pre> <ul> <li>Check primary redis: https://prom-mon-int.kooomo.com/graph?g0.expr=redis_connected_slaves%20%3E%200&amp;g0.tab=1&amp;g0.stacked=0&amp;g0.range_input=1h</li> </ul>"},{"location":"customers/kooomo-C1362/events/2022-01-26-hardware-maintenance/#vm-migration","title":"VM Migration","text":"VM Current Node IP Move to new Node IP Migration Note prodredse03 192.168.100.110 192.168.100.112 Mandatory Do sentinel failover to prodredse01 first prodredle03 192.168.100.110 192.168.100.112 Mandatory Do sentinel failover to prodredle01 first prd-workr01 192.168.100.110 192.168.100.111 Mandatory This will free up the node 110 prodredpr02 192.168.100.112 Optional Do sentinel failover to prodredpr01 first. This will free up the node 112 for future maintenance prd-hapxy01 192.168.100.112 192.168.100.111 Optional This will free up the node 112 for future maintenance prd-fsnfs01 10.155.65.4 10.155.65.3 Mandatory Incrase server RAM to 16G prodsqlba03 10.155.65.2 10.155.65.3 Mandatory prodsqlfr00 10.155.65.2 10.155.65.3 Mandatory prd-hapxy02 10.155.65.2 10.155.65.3 Mandatory Critical, but HA is available prd-haint02 10.155.65.2 10.155.65.3 Mandatory Critical, but HA is available zpgipa02 10.155.65.4 10.155.65.3 Mandatory Critical, but HA is available prodredpr06 10.155.65.3 Turn off Optional <code>consul maint -enable</code>. This will free up ram to 10.155.65.3 node prodredca06 10.155.65.3 Turn off Optional <code>consul maint -enable</code>. This will free up ram to 10.155.65.3 node prodredle06 10.155.65.3 Turn off Optional <code>consul maint -enable</code>. This will free up ram to 10.155.65.3 node prodredba05 10.155.65.3 Turn off Optional Not used. This will free up ram to 10.155.65.3 node prodredfr11 10.155.65.2 Turn off Optional Not used. This will free up ram to 10.155.65.2 node"},{"location":"customers/kooomo-C1362/events/2022-01-26-hardware-maintenance/#26-jan-h-930-gmt","title":"26 Jan - h 9.30 GMT","text":"<ul> <li> <p>Node 192.168.100.110:</p> </li> <li> <p>Turn off the empty node</p> </li> <li>Install RAM.</li> <li>Turn on the node.</li> <li> <p>Infrastructure team can reinstall the node and add it to VC1</p> </li> <li> <p>Node 192.168.100.113:</p> </li> <li> <p>Add consul maintenance to the VMs when possibile.</p> </li> <li>Put node in maintenance: some VMS will be migrate to node 10.155.65.4.</li> <li>Turn off the node.</li> <li>Install RAM.</li> <li>Turn on the node.</li> <li> <p>Remove the maintenance and rebalance the cluster.</p> </li> <li> <p>Node 10.155.65.1:</p> </li> <li> <p>Add consul maintenance to the VMs when possibile.</p> </li> <li>Put node in maintenance: some VMS will be migrate to node 10.155.65.4.</li> <li>Turn off the node.</li> <li>Replace battery.</li> <li>Turn on the node.</li> <li> <p>Remove the maintenance.</p> </li> <li> <p>Node 10.155.65.2:</p> </li> <li> <p>Add consul maintenance to the VMs when possibile.</p> </li> <li>Put node in maintenance: some VMS will be migrate to node 10.155.65.1.</li> <li>Turn off the node.</li> <li>Replace battery.</li> <li>Turn on the node.</li> <li> <p>Remove the maintenance.</p> </li> <li> <p>Node 10.155.65.4:</p> </li> <li>Add consul maintenance to the VMs when possibile</li> <li>Put node in maintenance: some VMS will be migrate to node 10.155.65.2</li> <li>Turn off the node</li> <li>Replace battery</li> <li>Turn on the node.</li> <li>Remove the maintenance and rebalance the cluster.</li> </ul>"},{"location":"customers/kooomo-C1362/events/2022-01-26-hardware-maintenance/#future-planning","title":"Future planning","text":"Node Maintenance VC Note 192.168.100.110 Install VCenter 6.7 WIN 5.1 Add to VC1 192.168.100.111 Install VCenter 6.7 WIN 5.1 Add to VC2 192.168.100.112 Install VCenter 6.7 WIN 5.1 Add to VC2 10.155.65.1 Downgrade license VC Enterprise Remove dvSwtich 10.155.65.2 Downgrade license VC Enterprise Remove dvSwtich 10.155.65.3 Downgrade license VC Enterprise Remove dvSwtich 10.155.65.4 Downgrade license VC Enterprise Remove dvSwtich"},{"location":"customers/kooomo-C1362/operations/AWS-SSM-Server-Patching/","title":"Server patching by using SSM Run Command","text":"<p>For PCI DSS standards, Kooomo needs to apply security patches every 30 days. In this case, we used the SSM run command to automate server patching. In the event Bridge, it has been created four rules, which belongs to four groups that each group is inculded seven prd-frontxx VMs. So, in Each week, one of these four groups would be invoked the Lambda to patch the seven servers. We have configured the AWS SSM Run Command with AWS-RunPatchBaselineWithHooks document which set the servers on the maintenance mode, patch the servers, autoremove the packages, then disable the maintenance mode.</p> <p></p>"},{"location":"customers/kooomo-C1362/operations/AWS-SSM-Server-Patching/#event-bridge-rules","title":"Event Bridge rules","text":"<p>Update a frontend webservers pool every monday. This will require a detailed schedule and we can't do directly in SSM. So we use Event Bride + Lambda</p> <p>Account: <code>cc-digital</code></p> <ul> <li>prd-front01-07: https://eu-central-1.console.aws.amazon.com/events/home?region=eu-central-1#/eventbus/default/rules/Kooomo_prd_front01_07</li> <li>prd-front08-14: https://eu-central-1.console.aws.amazon.com/events/home?region=eu-central-1#/eventbus/default/rules/Kooomo_prd_front08_14</li> <li>prd-front15-21: https://eu-central-1.console.aws.amazon.com/events/home?region=eu-central-1#/eventbus/default/rules/Kooomo_prd_front15_21</li> <li>prd-front22-28: https://eu-central-1.console.aws.amazon.com/events/home?region=eu-central-1#/eventbus/default/rules/Kooomo_prd_front22_28</li> </ul>"},{"location":"customers/kooomo-C1362/operations/AWS-SSM-Server-Patching/#lambda","title":"Lambda","text":"<ul> <li>https://github.com/criticalcase/digital-solution-lambda-cross-account/tree/ssm-run-command-PatchBaselineWithHooks</li> </ul>"},{"location":"customers/kooomo-C1362/operations/AWS-SSM-Server-Patching/#document-script","title":"Document script","text":"<p>Update procedure for the prd-frontXX VMs</p> <p>Account: <code>cc-digital</code></p> <ul> <li>https://eu-central-1.console.aws.amazon.com/systems-manager/documents/kooomo_hook_php_set_version/description?region=eu-central-1</li> </ul>"},{"location":"customers/kooomo-C1362/operations/CreateVM/","title":"How to create new VM for Redis cluster","text":"<ol> <li>Login to the FOREMAN.    https://provision.kooomo.com/</li> <li> <p>Navigate to the Hosts &gt; Create Host </p> </li> <li> <p>In our example we name it prodredcm01.</p> </li> <li> <p>The configuration should be done based on the following:</p> </li> <li> <p>Host   o Organization: Kooomo   o Location: IE1   o Host Group: Default/Ubuntu 18.04</p> </li> </ol> <p></p> <ul> <li>Virtual Machine   o CPUs: Based on customer needs   o Core Per Socket: Based on customer needs   o Memory: Based on customer needs   o Folder: vm   o Create SCSI controller: LSI Logic Parallel   o Data Store: Local-Blade1-nXX   o Disk Mode: Persistent   o Size (GB): based on customer needs   o Eager zero: select</li> </ul> <p></p> <ul> <li>Interface</li> </ul> <p>At this step, we need to find a free IP to add into the VM interface. So, we should do the SSH to the below machine: ssh prodmanag01.kooomo.local</p> <p>now, we should run the following commands to see the used IP list. \u2022 sudo fing 192.168.100.0/24</p> <p>When we select the considered free IP address, we should check whether the IP is completely free or not. \u2022 nslookup 192.168.100.67 (we have chosen this IP for our sample) \u2022 ping 192.168.100.67 \u2022 sudo arping 192.168.100.67</p> <p>when you are sure the considered IP is free, you should back to the FOREMAN and click on the interface tab. Now click on the Edit: o IPv4 Subnet: enter the considered free IP </p> <p></p> <ol> <li>click on the submit</li> </ol>"},{"location":"customers/kooomo-C1362/operations/CreateVM/#how-to-configure-redis-cluster-by-ansible-for-prod","title":"How to configure Redis cluster by Ansible for prod","text":"<ol> <li> <p>clone the ansible repository by following link:</p> </li> <li> <p>\u2022 git clone git@git.criticalcase.com:ops/zg-ansible.git</p> </li> <li> <p>main repository: https://git.criticalcase.com/ops/zg-ansible</p> </li> <li> <p>Open the Hosts file and search this keyword: redis_docker_prod:children</p> </li> <li>Under the [redis_docker_prod:children], add the name of new Redis cluster:</li> </ol> <p>[redis_docker_prod:children] prodredcm</p> <ol> <li>Under the list of Redis cluster names, add this: [write the name of your cluster here] and under it we should add the range of VMs numbers and complete domain name.</li> </ol> <p>[prodredcm] prodredcm[01:03].kooomo.local</p> <p></p> <ol> <li>Click on the group_vars and create a file with name of new Redis cluster (ex. prodredcm.yaml) and copy the config below to the file that you created. You should change the IP of master host with one of the IP address of new Redis cluster VMs that you created before by foreman.</li> </ol> <p>REDIS_MASTER_HOST=xxx.xxx.xxx.xxx</p> Text Only<pre><code>kooomo_redis_master_host: 192.168.100.68\nkooomo_redis_container_name: redis-cms\nkooomo_redis_maxmemory: 4gb\nkooomo_redis_repl_backlog_size: 2gb\nkooomo_redis_client_output_buffer_limit: slave 2gb 1gb 0\n</code></pre> <p></p> <ol> <li>At this step, we need to configure sentinel for the new Redis cluster. So, Click on the host_vars and select the prodredha01.kooomo.local.yml. you should add the configs of below at the end of the prodredha01.kooomo.local.yml file. (please change the cms with the name of the new Redis cluster). And, add the IP of VM that you entered in the step 10 in this line: REDIS_MASTER_HOST=xxx.xxx.xxx.xxx</li> </ol> Text Only<pre><code>    redis-se-cms:\n      container_name: redis-se-cms\n      image: {{ kooomo_redis_se_image }}\n      environment:\n        - REDIS_MASTER_HOST=192.168.100.67\n        - REDIS_MASTER_PASSWORD={{ kooomo_redis_password }}\n        - REDIS_SENTINEL_QUORUM=1\n      ports:\n        - '127.0.0.1:26384:26379'\n      restart: unless-stopped\n      volumes:\n        - '/opt/redis-se-cms:/bitnami'\n      sysctls:\n        net.core.somaxconn: '511'\n\ncc_docker_compose_volumes:\n- path: \"/opt/redis-se-cms/redis-sentinel/conf\"\n  owner: 1001\n</code></pre> <p> 7. navigate to group_vars and open all.yml, then in the \" kooomo_consul_services: | \" section, please add the line below and changing the cms with your new redis cluster name :</p> Text Only<pre><code>_kooomo_consul_services_redis_cms | default([]) +\n</code></pre> <p></p> <ol> <li> <p>select group_vars/app.yml :</p> </li> <li> <p>In the ##Frontend / # Redis section, add the following commands, and changing the name with the name of the new redis cluster :</p> <p>listen: \"127.0.0.1:20007 , listen: \"127.0.0.1:20017\" ===&gt; you should change the ports.</p> </li> </ol> Text Only<pre><code>- name: fr-prodredcm-master\n    description: 20007 - Production Redis CMS Master\n    bind:\n      - listen: \"127.0.0.1:20007\"\n    mode: tcp\n    option:\n      - dontlog-normal\n      - tcplog\n    balance: roundrobin\n    default_backend: bk-prodredcm-master\n\n  - name: fr-prodredcm-slaves\n    description: 20017 - Production Redis CMS Slaves\n    bind:\n      - listen: \"127.0.0.1:20017\"\n    mode: tcp\n    option:\n      - dontlog-normal\n      - tcplog\n    balance: roundrobin\n    acl:\n      - string: acl-prodredcm-slaves-empty nbsrv(bk-prodredcm-slaves) eq 0\n    default_backend: bk-prodredcm-slaves\n    use_backend:\n      - bk-prodredcm-master if acl-prodredcm-slaves-empty\n</code></pre> <p></p> <ul> <li>you should add the commands below to the end of # Redis section, then change the cm with the name of new redis cluster:</li> </ul> Text Only<pre><code>_http_check_red_cm_master:\n  - connect port 8500\n  - send meth GET uri /v1/agent/health/service/name/{{ _kooomo_env_name | default('') }}redis-cms?format=text\n  - expect status 200\n\n_http_check_red_cm_slaves:\n  - connect port 8500\n  - send meth GET uri /v1/agent/health/service/name/{{ _kooomo_env_name | default('') }}redis-cms-slave?format=text\n  - expect status 200\n</code></pre> <p></p> <ul> <li>In the # Backend / #####\u00a0Redis section, add the following commands:</li> </ul> Text Only<pre><code>- name: bk-prodredcm-master\n    mode: tcp\n    balance: roundrobin\n    default_server_params:\n      - check\n      - \"{{ _kooomo_redis_master_interval }}\"\n      - on-marked-down shutdown-sessions\n    option:\n      - httpchk\n    http_check: \"{{ _http_check_red_cm_master }}\"\n    server: \"{{ _kooomo_redcm + _kooomo_redcm_backup | default([]) }}\"\n\n  - name: bk-prodredcm-slaves\n    mode: tcp\n    balance: roundrobin\n    default_server_params:\n      - check\n      - \"{{ _kooomo_redis_slave_interval }}\"\n      - on-marked-down shutdown-sessions\n    option:\n      - httpchk\n    http_check: \"{{ _http_check_red_cm_slaves }}\"\n    server: \"{{ _kooomo_redcm + _kooomo_redcm_batch | default([]) }}\"\n</code></pre> <p></p> <ol> <li>route to the group_vars/app_prd.yaml and adding the following commands in the #Redis section, then changing the cm with the name of the new cluster: <p>you should change the port here ===&gt; listen: 192.168.100.15:20007</p> </li> </ol> Text Only<pre><code>_kooomo_redcm:\n  - name: prodredcm01\n    listen: \"192.168.100.67:6379\"\n  - name: prodredcm02\n    listen: \"192.168.100.68:6379\"\n  - name: prodredcm03\n    listen: \"192.168.100.69:6379\"\n\n_kooomo_redcm_backup:\n  - name: prodredha01\n    listen: 192.168.100.15:20007\n    param:\n      - backup\n      - no-check\n</code></pre> <p></p> <ol> <li>open the group_vars/docker_aio.yml and add the following commands:     &gt; Changing the redis-cms with the name of new redis cluster. furthermore, changing the port number.     &gt; {.is-warning}</li> </ol> Text Only<pre><code>redis-cms:\n      container_name: redis-cms\n      hostname: {{ inventory_hostname_short }}-redis-cms\n      image: {{ kooomo_redis_image }}\n      restart: unless-stopped\n      volumes:\n        - '/opt/redis-cms:/bitnami'\n        - /etc/localtime:/etc/localtime:ro\n      ports:\n        - '20007:6379'\n      environment:\n        - REDIS_MASTER_HOST={{ kooomo_redis_master_host }}\n        - REDIS_MASTER_PORT_NUMBER=20007\n        - REDIS_REPLICATION_MODE={{ kooomo_redis_replication_mode | default('slave') }}\n        - REDIS_REPLICA_PORT=20007\n        - REDIS_REPLICA_IP={{ ansible_default_ipv4.address }}\n        - REDIS_PASSWORD={{\u00a0kooomo_redis_password }}\n        - REDIS_MASTER_PASSWORD={{\u00a0kooomo_redis_password }}\n        - REDISCLI_AUTH={{\u00a0kooomo_redis_password }}\n      command: /run.sh --maxmemory 2gb --appendonly {{ kooomo_redis_appendonly | default('yes')}} --repl-backlog-size {{ kooomo_redis_repl_backlog_size | default('256mb') }} --client-output-buffer-limit {{ kooomo_redis_client_output_buffer_limit | default('slave 256mb 64mb 60') }} --maxmemory-policy allkeys-lru {{ kooomo_redis_custom_values | default('')}}\n      sysctls:\n        net.core.somaxconn: '511'\n\n    redis-cms-exporter:\n      container_name: redis-cms-exporter\n      image: \"{{\u00a0kooomo_redis_exporter_image }}\"\n      restart: unless-stopped\n      ports:\n        - '21007:9121'\n      environment:\n        - REDIS_ADDR={{ inventory_hostname }}:20007\n        - REDIS_PASSWORD={{ kooomo_redis_password }}\n</code></pre> <p></p> <ol> <li>Now, the haproxy should be configured. Route to the group_vars/redis_docker_ha.yml and adding the commands below to the redis_docker_ha.yml file.</li> </ol> Text Only<pre><code>  - name: prodredha-cms\n    description: 20007 - Production Redis CMS HA\n    bind:\n      - listen: \"*:20007\"\n    mode: tcp\n    option:\n      - tcplog\n      - tcp-check\n    tcp_check:\n      - \"connect\"\n      - 'send AUTH\\ {{ kooomo_redis_password }}\\r\\n'\n      - \"expect string +OK\"\n      - 'send PING\\r\\n'\n      - \"expect string +PONG\"\n      - 'send info\\ replication\\r\\n'\n      - \"expect string role:master\"\n      - 'send QUIT\\r\\n'\n      - \"expect string +OK\"\n    balance: source\n    server:\n      - name: prodredcm01\n        listen: \"192.168.100.67:6379\"\n        param:\n          - check\n          - inter 1s\n          - on-marked-down shutdown-sessions\n      - name: prodredcm02\n        listen: \"192.168.100.68:6379\"\n        param:\n          - check\n          - inter 1s\n          - on-marked-down shutdown-sessions\n      - name: prodredcm03\n        listen: \"192.168.100.69:6379\"\n        param:\n          - check\n          - inter 1s\n          - on-marked-down shutdown-sessions\n</code></pre> <p>you should change the Port listen: \"*:20007\" and the listen: \"192.168.100.68:6379\" IPs with the IP of new redis cluster VMs. Moreover, change the CMS and cmxx names with the new redis cluster.</p>"},{"location":"customers/kooomo-C1362/operations/CreateVM/#how-to-configure-redis-cluster-by-ansible-for-dev","title":"How to configure Redis cluster by Ansible for dev","text":"<ol> <li>Navigate to the group_vars/dev.yml and adding the following commands: <p>you should change the port number.</p> </li> </ol> Text Only<pre><code>_kooomo_consul_services_redis_cms:\n  - service:\n      name: dev-redis-cms\n              - redis\n        - cms\n        - primary\n      port: 20007\n      check:\n        name: Redis primary\n        args: [\"bash\", \"-lc\", \"redis-cli -a {{ kooomo_redis_password }} -h {{ ansible_default_ipv4.address }} -p 20007 info replication | grep 'role:master'\"]\n        interval: 1s\n        timeout: 5s\n  - service:\n      name: dev-redis-cms-slave\n              - redis\n        - cms\n        - replica\n      port: 20007\n      check:\n        name: Redis replica\n        args: [\"bash\", \"-lc\", \"redis-cli -a {{ kooomo_redis_password }} -h {{ ansible_default_ipv4.address }} -p 20007 info replication | grep 'master_link_status:up'\"]\n        interval: 1s\n        timeout: 5s\n</code></pre> <p></p> <ol> <li>Route to the group_vars/dev_app.yml, and copying the commands below.Moreover, changing the _redcm with the name of new redis cluster. <p>changing the port with the port you have choosed in previous step</p> </li> </ol> Text Only<pre><code>_kooomo_redcm:\n  - name: dev-dockr01.kooomo.local:20007\n    listen: 192.168.100.183:20007\n  - name: dev-dockr02.kooomo.local:20007\n    listen: 192.168.100.185:20007\n  - name: dev-dockr03.kooomo.local:20007\n    listen: 192.168.100.170:20007\n</code></pre> <p></p>"},{"location":"customers/kooomo-C1362/operations/Teleport-K8s-namespace-limitation/","title":"Kubernetes access from teleport","text":"<p>Teleport can connect to external Kubernetes clusters without using the Teleport Helm chart or running a pod inside of the Kubernetes cluster. To do this, Teleport needs a kubeconfig file to authenticate against the Kubernetes API.</p>"},{"location":"customers/kooomo-C1362/operations/Teleport-K8s-namespace-limitation/#generating-kubeconfig","title":"Generating kubeconfig","text":"<p>The teleport proxy needs clustreroles and clusterrolebinding wide scope view to impersonate users, service account or groups.</p> <p>In case of connecting to a kubernetes cluster, it needs to be created a kubeconfig file and adding it to the .kube/ path to give us permission for accessing to kuberentes cluster. there are two types of kubeconfig files that we can use:</p> <ol> <li>we can use admin kubeconfig with all permissions for teleport proxy. for getting the kubeconfig credential, it should be used the command below:</li> </ol> Text Only<pre><code>kubectl config view --flatten --minify\n</code></pre> <ol> <li>According to the customer requirements and following the \"Principle of Least Privilege\" (PoLP) and \"Zero Admin\" best practices, the auto generating kubeconfig script file could be used as below. (https://github.com/gravitational/teleport/blob/master/examples/k8s-auth/get-kubeconfig.sh)</li> </ol> Bash<pre><code>#!/bin/bash\n\n# Copyright 2020 Gravitational, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# This script creates a new k8s Service Account and generates a kubeconfig with\n# its credentials. This Service Account has all the necessary permissions for\n# Teleport. The kubeconfig is written in the current directory.\n#\n# You must configure your local kubectl to point to the right k8s cluster and\n# have admin-level access.\n#\n# Note: all of the k8s resources are created in namespace \"teleport\". If you\n# delete any of these objects, Teleport will stop working.\n#\n# You can override the default namespace \"teleport\" using the\n# TELEPORT_NAMESPACE environment variable.\n# You can override the default service account name \"teleport-sa\" using the\n# TELEPORT_SA_NAME environment variable.\n\nset -eu -o pipefail\n\n# Allow passing in common name and username in environment. If not provided,\n# use default.\nTELEPORT_SA=${TELEPORT_SA_NAME:-teleport-sa}\nNAMESPACE=${TELEPORT_NAMESPACE:-teleport}\n\n# Set OS specific values.\nif [[ \"$OSTYPE\" == \"linux-gnu\" ]]; then\n    BASE64_DECODE_FLAG=\"-d\"\nelif [[ \"$OSTYPE\" == \"darwin\"* ]]; then\n    BASE64_DECODE_FLAG=\"-D\"\nelif [[ \"$OSTYPE\" == \"linux-musl\" ]]; then\n    BASE64_DECODE_FLAG=\"-d\"\nelse\n    echo \"Unknown OS ${OSTYPE}\"\n    exit 1\nfi\n\necho \"Creating the Kubernetes Service Account with minimal RBAC permissions.\"\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: ${NAMESPACE}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: ${TELEPORT_SA}\n  namespace: ${NAMESPACE}\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: teleport-role\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - users\n  - groups\n  - serviceaccounts\n  verbs:\n  - impersonate\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  verbs:\n  - get\n- apiGroups:\n  - \"authorization.k8s.io\"\n  resources:\n  - selfsubjectaccessreviews\n  - selfsubjectrulesreviews\n  verbs:\n  - create\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: teleport-crb\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: teleport-role\nsubjects:\n- kind: ServiceAccount\n  name: ${TELEPORT_SA}\n  namespace: ${NAMESPACE}\nEOF\n# Get the service account token and CA cert.\nSA_SECRET_NAME=$(kubectl get -n ${NAMESPACE} sa/${TELEPORT_SA} -o \"jsonpath={.secrets[0]..name}\")\n# Note: service account token is stored base64-encoded in the secret but must\n# be plaintext in kubeconfig.\nSA_TOKEN=$(kubectl get -n ${NAMESPACE} secrets/${SA_SECRET_NAME} -o \"jsonpath={.data['token']}\" | base64 ${BASE64_DECODE_FLAG})\nCA_CERT=$(kubectl get -n ${NAMESPACE} secrets/${SA_SECRET_NAME} -o \"jsonpath={.data['ca\\.crt']}\")\n\n# Extract cluster IP from the current context\nCURRENT_CONTEXT=$(kubectl config current-context)\nCURRENT_CLUSTER=$(kubectl config view -o jsonpath=\"{.contexts[?(@.name == \\\"${CURRENT_CONTEXT}\\\"})].context.cluster}\")\nCURRENT_CLUSTER_ADDR=$(kubectl config view -o jsonpath=\"{.clusters[?(@.name == \\\"${CURRENT_CLUSTER}\\\"})].cluster.server}\")\n\necho \"Writing kubeconfig.\"\ncat &gt; kubeconfig &lt;&lt;EOF\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: ${CA_CERT}\n    server: ${CURRENT_CLUSTER_ADDR}\n  name: ${CURRENT_CLUSTER}\ncontexts:\n- context:\n    cluster: ${CURRENT_CLUSTER}\n    user: ${CURRENT_CLUSTER}-${TELEPORT_SA}\n  name: ${CURRENT_CONTEXT}\ncurrent-context: ${CURRENT_CONTEXT}\nkind: Config\npreferences: {}\nusers:\n- name: ${CURRENT_CLUSTER}-${TELEPORT_SA}\n  user:\n    token: ${SA_TOKEN}\nEOF\n\necho \"---\nDone!\nCopy the generated kubeconfig file to your Teleport Proxy server, and set the\nkubeconfig_file parameter in your teleport.yaml config file to point to this\nkubeconfig file.\nIf you need access to multiple kubernetes clusters, you can generate additional\nkubeconfig files using this script and then merge them using merge-kubeconfigs.sh.\nNote: Kubernetes RBAC rules for Teleport were created, you won't need to create them manually.\"\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/Teleport-K8s-namespace-limitation/#adding-kubeconfig-to-teleport","title":"Adding kubeconfig to Teleport","text":"<p>It is necessary to enable kubernetes service and adding kube public and listen address to the teleport config file in the teleport proxy:</p> <p>https://goteleport.com/docs/kubernetes-access/guides/standalone-teleport/</p> <p>replace this path with the actual path with your generated kubeconfig</p> Text Only<pre><code>kubeconfig_file: \"path/to/kubeconfig\"\n</code></pre> <p>{.is-warning}</p> Bash<pre><code># ...\nproxy_service:\n  # ...\n  public_addr: proxy.example.com:3080\n\n  kube_listen_addr: 0.0.0.0:3026\n  # optional: set a different public address for kubernetes access\n  kube_public_addr: kube.example.com:3026\n\nkubernetes_service:\n  enabled: yes\n  listen_addr: 0.0.0.0:3027\n  # replace this path with the actual path for your generated kubeconfig\n  kubeconfig_file: \"path/to/kubeconfig\"\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/Teleport-K8s-namespace-limitation/#how-to-limit-users-to-only-access-to-a-namespace-by-creating-groups","title":"How to limit users to only access to a namespace by creating Groups","text":"<p>If we need to limit developers team to jsut access to a namespace and getting the pods or running the exec command, we should create the Roles and Rolebinding in the target namesapcae (e.x developers) and in the rolebining we need to define subjects with kind groups and name of developers like below:</p> <p>https://github.com/criticalcase/flux-kooomo-c2/blob/main/kubify/tenants/base/core-backend/core-backend-stag-developers.yaml</p> Text Only<pre><code>---\napiVersion: v1\nitems:\n- apiVersion: rbac.authorization.k8s.io/v1\n  kind: Role\n  metadata:\n    name: developers-cr\n    namespace: core-backend-stag\n  rules:\n  - apiGroups:\n    - \"\"\n    resources:\n    - pods\n    - pods/logs\n    verbs:\n    - get\n    - list\n    - watch\n  - apiGroups:\n    - \"\"\n    resources:\n    - pods/exec\n    verbs:\n    - create\n\nkind: List\nmetadata:\n  resourceVersion: \"\"\n  selfLink: \"\"\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: core-backend-developers-rb\n  namespace: core-backend-stag\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: developers-cr\nsubjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: Group\n    name: developers\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/Teleport-K8s-namespace-limitation/#the-role-template-for-confining-users-to-developers-group","title":"The role template for confining users to developers group","text":"<ol> <li> <p>login to teleport console and navigate to Team&gt;Roles</p> </li> <li> <p>click on the CREATE NEW ROLES and copy, paste the template below:</p> </li> </ol> <p>the name of \"kubernetes_groups:\" should be same as the group we defined in Rolebinding. kubernetes_groups:</p> Text Only<pre><code>- **developers**\n</code></pre> <p>{.is-info}</p> Text Only<pre><code>kind: role\nmetadata:\n  id: 1644252286792401206\n  name: k8s-limited-ns\nspec:\n  allow:\n    kubernetes_groups:\n    - developers\n    kubernetes_labels:\n      '*': '*'\n    logins:\n    - root\n    node_labels:\n      '*': '*'\n    rules:\n    - resources:\n      - role\n      verbs:\n      - list\n      - create\n      - read\n      - update\n      - delete\n    - resources:\n      - auth_connector\n      verbs:\n      - list\n      - create\n      - read\n      - update\n      - delete\n    - resources:\n      - session\n      verbs:\n      - list\n      - read\n    - resources:\n      - trusted_cluster\n      verbs:\n      - list\n      - create\n      - read\n      - update\n      - delete\n    - resources:\n      - event\n      verbs:\n      - list\n      - read\n    - resources:\n      - user\n      verbs:\n      - list\n      - create\n      - read\n      - update\n      - delete\n    - resources:\n      - pods\n      verbs:\n      - list\n      - create\n      - read\n      - update\n      - delete\n    - resources:\n      - token\n      verbs:\n      - list\n      - create\n      - read\n      - update\n      - delete\n  deny: {}\n  options:\n    cert_format: standard\n    enhanced_recording:\n    - command\n    - network\n    forward_agent: false\n    max_session_ttl: 30h0m0s\n    port_forwarding: true\nversion: v4\n</code></pre> <ol> <li>it needs to assign this role to the intended users who we want to limit them.</li> </ol>"},{"location":"customers/kooomo-C1362/operations/Teleport-K8s-namespace-limitation/#how-to-connect-via-teleport-to-kubernetes-cluster","title":"how to connect via teleport to kubernetes cluster","text":"<ol> <li>login to teleport console and route to the Kubernetes in vertical tab    </li> <li>click on connect over the kubernetes cluster name and follow the instructions.    </li> <li>for connecting to kubernetes cluster from your local machine, you need to install the tsh teleport package on your local machine.</li> </ol>"},{"location":"customers/kooomo-C1362/operations/administrative-users/","title":"Admin user kooomo","text":""},{"location":"customers/kooomo-C1362/operations/administrative-users/#freeipa","title":"Freeipa","text":"<ul> <li>Create user on Freeipa: https://ipa.kooomo.com/ipa/ui/#/e/user</li> <li>Update Email address with @criticalcase.com email address</li> <li>Add group: admins, trust admins</li> </ul>"},{"location":"customers/kooomo-C1362/operations/administrative-users/#firewall","title":"Firewall","text":"<ul> <li>Create user on https://firewall-secure.kooomo.com &gt; Device &gt; Administrators   Authentication Profile: FreeIPA LDAP Koooomo   Administrator Type: Dynamic &gt; Superuser</li> </ul>"},{"location":"customers/kooomo-C1362/operations/administrative-users/#vpn","title":"VPN","text":"<ul> <li>Download and install the client form here: https://gp-vpn.kooomo.com</li> </ul>"},{"location":"customers/kooomo-C1362/operations/esxi/","title":"ESXi","text":""},{"location":"customers/kooomo-C1362/operations/esxi/#enable-snmp-community","title":"Enable SNMP community","text":"Bash<pre><code>esxcli system snmp set --communities public\nesxcli system snmp set --enable true\nesxcli network firewall ruleset set --ruleset-id snmp --enabled true\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/freeipa-certificates/","title":"Freeipa certificates","text":""},{"location":"customers/kooomo-C1362/operations/freeipa-certificates/#generate-freeipa-ssl","title":"Generate Freeipa ssl","text":"<p>On both VM:</p> Bash<pre><code>export CERT_HOSTNAME=logstash-elastic1-c2-int.dmz.kooomo\n</code></pre> <p>On Freeipa master:</p> Bash<pre><code>export TARGET_VM=prodmanag02.kooomo.local\n\nipa host-add ${CERT_HOSTNAME} --force\nipa service-add prom/${TARGET_VM}\nipa service-add prom/${CERT_HOSTNAME} --force\nipa service-add-host --host=${TARGET_VM} prom/${CERT_HOSTNAME}\n</code></pre> <p>On the target VM:</p> Bash<pre><code># ssh ${TARGET_VM}\nipa-getcert request -r -f /etc/ssl/certs/${CERT_HOSTNAME}.crt -k /etc/ssl/private/${CERT_HOSTNAME}.key -N CN=${CERT_HOSTNAME} -D ${CERT_HOSTNAME} -K prom/${CERT_HOSTNAME}\nipa-getcert list\n\n\ncat /etc/ssl/certs/${CERT_HOSTNAME}.crt &gt; ${CERT_HOSTNAME}.pem\ncat /etc/ssl/private/${CERT_HOSTNAME}.key &gt;&gt; ${CERT_HOSTNAME}.pem\ncat ${CERT_HOSTNAME}.pem\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/freeipa-certificates/#check-the-expiration-date","title":"Check the expiration date","text":"Bash<pre><code># one by one\nipa-getcert list | grep expires\n\n# all vms\nansible -i hosts all -a \"bash -lc \\\" ipa-getcert list | grep expires \\\"\" -f 8\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/freeipa-certificates/#renew-certificate-manually","title":"Renew certificate manually","text":"<ol> <li>login to the VM \"prodmanag02.kooomo.local\"</li> <li>check the certificate status if there is in \"Monitoring\" state or \"stuck\"</li> </ol> Text Only<pre><code>getcert list\n</code></pre> <ol> <li>if it is in \"stuck status\" you should stop the tracking by the following command :</li> </ol> Text Only<pre><code>ipa-getcert stop-tracking -i &lt;request id&gt;\n</code></pre> <ol> <li>generate new certificate for \"logstash-elastic1-c2-int.dmz.kooomo\"</li> </ol> Text Only<pre><code>ipa-getcert request -r -f /etc/ssl/certs/logstash-elastic1-c2-int.dmz.kooomo.crt -k /etc/ssl/private/logstash-elastic1-c2-int.dmz.kooomo.key -N CN=logstash-elastic1-c2-int.dmz.kooomo -D logstash-elastic1-c2-int.dmz.kooomo -K prom/logstash-elastic1-c2-int.dmz.kooomo\n</code></pre> <ol> <li>you need manually resubmit new request for certificate renewal:</li> </ol> Text Only<pre><code>ipa-getcert resubmit -i &lt;request id&gt;\n</code></pre> <ol> <li>making a PEM file for creating secret for logstash</li> </ol> Text Only<pre><code>cat /etc/ssl/certs/logstash-elastic1-c2-int.dmz.kooomo.crt &gt; logstash-elastic1-c2-int.dmz.kooomo.pem\ncat /etc/ssl/private/logstash-elastic1-c2-int.dmz.kooomo.key &gt;&gt; logstash-elastic1-c2-int.dmz.kooomo.pem\n</code></pre> <ol> <li>Encrypt the PEM file by base64</li> </ol> Text Only<pre><code>cat logstash-elastic1-c2-int.dmz.kooomo.pem | base64 -w0\n</code></pre> <ol> <li>create a secret or edit the current secret</li> </ol> Text Only<pre><code>kubectl create secret generic logstash-certificate-pem --from-file=elastic-certificate.pem\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/freeipa/","title":"Freeipa","text":""},{"location":"customers/kooomo-C1362/operations/freeipa/#fix-ssh-user-login-issue","title":"Fix ssh user login issue","text":"<p>Go to <code>zgpipa01.kooomo.local</code> and get the <code>IPA_ADMIN_PASSWORD</code></p> Bash<pre><code>cat /root/login\n</code></pre> <p>Go to the VM with the user issue:</p> Bash<pre><code>ipa-client-install --uninstall\nipa-client-install --realm=KOOOMO.LOCAL -U --mkhomedir -p admin -w $IPA_ADMIN_PASSWORD\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/freeipa/#fix-double-basic-auth-freeipa-ui","title":"Fix double Basic Auth FreeIPA UI","text":"<ul> <li>http://jdshewey.blogspot.com/2017/08/fixing-annoying-popup-in-freeipa.html</li> </ul> <p>Edit /etc/httpd/conf.d/ipa-rewrite.conf and add to the end of this file:</p> Bash<pre><code>#The following disables the annoying kerberos popup for Chrome\nRewriteCond %{HTTP_COOKIE} !ipa_session\nRewriteCond %{HTTP_REFERER} ^(.+)/ipa/ui/$\nRewriteRule ^/ipa/session/json$ - [R=401,L]\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/mysql/","title":"MySQL","text":""},{"location":"customers/kooomo-C1362/operations/mysql/#create-a-replica-from-another-replica","title":"Create a replica from another replica","text":"<ul> <li>prodsqlfr07 is the source slave</li> <li>prodsqlfr08 is the destination slave</li> </ul> Bash<pre><code>ssh prodsqlfr08.kooomo.local\nconsul maint -enable\n\n# Stop the new\nsudo systemctl stop mysql\n\n# Remove the /var/lib/mysql files\nrm -rf /var/lib/mysql/*\n\n# Do the first rsync\nscreen\nrsync -avz prodsqlfr07.kooomo.local:/var/lib/mysql /var/lib\n</code></pre> <p>After the first sync stop the mysql service on the source slave</p> Bash<pre><code>ssh prodsqlfr07.kooomo.local\nconsul maint -enable\nsleep 30\nmysqladmin processlist\nsystemctl stop mysql\n</code></pre> Bash<pre><code>ssh prodsqlfr08.kooomo.local\n# Do the final rsync\nscreen\nrsync -avz prodsqlfr07.kooomo.local:/var/lib/mysql /var/lib\n\n# Remove the uuid file\nrm /var/lib/mysql/auto.cnf\n\n# Start the slave\nsystemctl start mysql\n\n# Check if the slave status is OK\nmysql -e \"SHOW SLAVE STATUS \\G\" | grep Running\nmysql -e \"SHOW SLAVE STATUS \\G\" | grep Seconds_Behind_Master\n\nconsul maint -disable\n</code></pre> <p>Start the source slave again</p> Bash<pre><code>ssh prodsqlfr07.kooomo.local\nsystemctl start mysql\n\n# Check if the slave status is OK\nmysql -e \"SHOW SLAVE STATUS \\G\" | grep Running\nmysql -e \"SHOW SLAVE STATUS \\G\" | grep Seconds_Behind_Master\n\nconsul maint -disable\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/mysql/#align-testing-database-from-production-database","title":"Align testing database from production database","text":"<p>There are 2 database update_staging_eshop and update_staging_admin. theese processes are autoated by cron on theese VM:</p> Bash<pre><code>#prodsqlfr01.kooomo.local\ncrontab -l\n\n10 2 * * * /root/percona/remote/update_staging_eshop &gt;&gt; /var/log/zg-update-staging.log 2&gt;&amp;1\n</code></pre> Bash<pre><code>#prodsqlba01.kooomo.local\ncrontab -l\n\n0 5 * * * /root/percona/remote/update_staging_admin &gt;&gt; /var/log/zg-update-staging.log 2&gt;&amp;1\n</code></pre> <p>and the testing databases are restored on testmysql01.kooomo.local into 2 dockers (dockers are created from scratch every day):</p> Bash<pre><code>root@testmysql01:~# docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES\nc108bf7b4d3f        percona:5.7         \"/docker-entrypoin...\"   5 hours ago         Up About an hour    0.0.0.0:3308-&gt;3306/tcp   db_eshop\nc82dda37611d        percona:5.7         \"/docker-entrypoin...\"   19 hours ago        Up About an hour    0.0.0.0:3307-&gt;3306/tcp   db_admin\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/mysql/#long-procedure-with-new-instant-backup-2-hours","title":"Long procedure with new instant backup (2 hours ~)","text":"<p>If for some reason you must forcing the align, simply run the cron scripts form the correct vm, for example:</p> Bash<pre><code># from prodsqlba01.koomo.local\nscreen /root/percona/remote/update_staging_admin\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/mysql/#short-procedure-with-last-nightly-backup-1-hours","title":"Short procedure with last nightly backup (1 hours ~)","text":"<p>If for some reason you must forcing the align, simply run the cron scripts form the testmysql01.kooomo.local vm, for example:</p> Bash<pre><code>screen /root/percona/tools/restore_admin\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/paloalto-wildcard-certificate-renewal/","title":"Paloalto wildcard certificate renewal","text":"<p>Use Firefox browser to avoid an UI issue during the upload.</p>"},{"location":"customers/kooomo-C1362/operations/paloalto-wildcard-certificate-renewal/#update-the-certificate","title":"Update the certificate","text":"<ul> <li>Install the new certificate on https://firewall-secure.kooomo.com/#device::vsys1::device/certificate-management/certificates</li> </ul> Text Only<pre><code>Device &gt; Certificate Management &gt; Certificates: Import\nName: newcertificate.kooomo.com\nIf Passphrase is not present in the certificate type: 123456\n</code></pre> Text Only<pre><code>Device &gt; Certificate Management &gt; SSL/TLS Service Profile: SSL VPN Profile\nCertificate: newcertificate.kooomo.com\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/paloalto-wildcard-certificate-renewal/#global-protect","title":"Global protect","text":"Text Only<pre><code>Network &gt; Global Protect &gt; Portals: KooomoGP-Portal\nAgent &gt; ClientConfig1 &gt; Client Certificate: Local, newcertificate.kooomo.com\n</code></pre> Text Only<pre><code>Network &gt; Global Protect &gt; Gateways: KOOMO-VPN-GW\nAgent &gt; Client Settings: KOOMO-VPN-CC\nCertificate to Encrypt/Decrypt Cookie: newcertificate.kooomo.com\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/rabbitmq/","title":"RabbitMQ","text":"Bash<pre><code># Create a queue\nrabbitmqadmin -u $RABBITMQ_DEFAULT_USER -p $RABBITMQ_DEFAULT_PASS declare exchange name=HCexchange type=fanout\n\n# Add a message\nrabbitmqadmin -u $RABBITMQ_DEFAULT_USER -p $RABBITMQ_DEFAULT_PASS publish exchange=HCexchange routing_key=healthcheck payload=\"Health Check message\"\n\n# Get a message\nrabbitmqadmin -u $RABBITMQ_DEFAULT_USER -p $RABBITMQ_DEFAULT_PASS get queue=healthcheck ackmode=ack_requeue_false\n\n# Useful commands\nrabbitmqadmin -u $RABBITMQ_DEFAULT_USER -p $RABBITMQ_DEFAULT_PASS list exchanges\nrabbitmqadmin -u $RABBITMQ_DEFAULT_USER -p $RABBITMQ_DEFAULT_PASS list queues\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/rabbitmq/#join-cluster","title":"Join cluster","text":"Bash<pre><code># on rabbit2\nrabbitmqctl stop_app\n\nrabbitmqctl reset\nrabbitmqctl join_cluster rabbit@dev-dockr03\n\nrabbitmqctl start_app\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/redis/","title":"Redis","text":""},{"location":"customers/kooomo-C1362/operations/redis/#manage-masters-replica-settings","title":"Manage master's replica settings","text":"Text Only<pre><code>redis-cli -a $REDIS_PASSWORD config get client-output-buffer-limit\n\n# Disable lagging slave disconnection\nredis-cli -a $REDIS_PASSWORD config set client-output-buffer-limit \"slave 4gb 0 0\"\n\n# Incrase repl-backlog-size\nredis-cli -a $REDIS_PASSWORD config get repl-backlog-size\nredis-cli -a $REDIS_PASSWORD config set repl-backlog-size 4gb\n</code></pre>"},{"location":"customers/kooomo-C1362/operations/vcenter/","title":"vCenter","text":""},{"location":"customers/kooomo-C1362/operations/vcenter/#move-vm-fromn-vcenter-51-to-vcenter-67","title":"Move VM fromn vcenter 5.1 to vcenter 6.7","text":""},{"location":"customers/kooomo-C1362/operations/vcenter/#vcenter-51","title":"vCenter 5.1","text":"<p>Move the VM on one of these 3 nodes: 110,111,112,113 and Datacore storage</p> <ul> <li>Video Part 1</li> </ul> <p>Than deregister the VM.</p>"},{"location":"customers/kooomo-C1362/operations/vcenter/#vcenter-61","title":"vCenter 6.1","text":"<p>Register the VM on node 10.155.65.2</p> <ul> <li>Video Part 2</li> </ul>"},{"location":"customers/m-iot-C2268/","title":"M-Iot SB SRL @ C2268","text":"<p>Ex polimatica C0918</p>"},{"location":"customers/m-iot-C2268/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR210610: m-iotechnology: openplast/openplatform/helius</li> </ul> <p>Internal data:</p> <ul> <li>Start date: 19/07/2021</li> <li>Sales: Vittorino Aprile</li> <li>Phone: +39 392 912 2831</li> <li>Email: v.aprile@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Francesco Dipasquale</li> <li>Email: Francesco Dipasquale ciccio.dipasquale@gmail.com</li> <li>Phone: +39 33582780</li> </ul>"},{"location":"customers/m-iot-C2268/#data-center","title":"Data Center","text":"<ul> <li> Critical Case RZ1</li> </ul>"},{"location":"customers/m-iot-C2268/#descrizione-ambito-funzionale-e-tecnologico","title":"Descrizione ambito funzionale e tecnologico","text":"<p>Ambiente stage, produzione (non attivo), archivio e sviluppo della piattaforma openplatfrom. Ambiente monitoring pannelli solari helius Vdc: https://vc2-is1.rz1.vdc.ccws.it. Backup ???</p> <p>Monitoraggio gestito da cliente con Nagios Tutta la documentazione architetturale ed operativa si trova sotto sharepoint: https://criticalcaseazure.sharepoint.com/sites/DigitalSolution/Shared%20Documents/Forms/AllItems.aspx?viewid=55d94eb1%2Dcfc2%2D4001%2Db84c%2Dc1307e99ce36&amp;id=%2Fsites%2FDigitalSolution%2FShared%20Documents%2FClienti%2FPolimatica</p>"},{"location":"customers/m-iot-C2268/#elenco-server-e-servizi","title":"Elenco Server e servizi","text":"<p>Tutti i server sotto vdc polimatica nelle cartelle collaudo, produzione, sviluppo, archivio di openplatform. Tutti i server sotto vdc polimatica nella cartella helius</p>"},{"location":"customers/m-iot-C2268/#accesso","title":"Accesso","text":"<p>Vdc: https://vc2-is1.rz1.vdc.ccws.it. oppure tramite opevpn con certificati, configurazione in sherlock</p> <p>Credenziali: https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/list/ds/customers/Polimatica-C0918/</p>"},{"location":"customers/markeven-C2164/","title":"Markeven srl @ C2164","text":"<p>Commerciale: Ottaviano Moretti</p>"},{"location":"customers/markeven-C2164/#shopcrastanit","title":"shop.crastan.it","text":"<p>Abbiamo fatto un'attivit\u00e0 per shop.crastan.it durante il festival di sanremo. L'attivit\u00e0 \u00e8 temrinata dopo il festival. Contattare il supporto di criticalcase in caso di problemi con la VM.</p>"},{"location":"customers/markeven-C2164/#bucket-c2164-markeven-backup","title":"Bucket c2164-markeven-backup","text":"<p>Abbiamo creato un bucket S3 dove il cliente far\u00e0 in autonomia i backup. (credentials)</p>"},{"location":"customers/medspa-C2371/","title":"Medspa @ C2371","text":""},{"location":"customers/medspa-C2371/#ito-h24","title":"ITO H24","text":"<ul> <li>CR220510: e-commerce Magento2 miamo.com</li> <li>CR230232: Magento2 www.miamoacademy.it</li> <li>CR230232: medspa.it</li> </ul>"},{"location":"customers/medspa-C2371/#progetti-senza-ito","title":"Progetti senza ITO","text":"<p>Internal data:</p> <ul> <li>Sales: Morena Scudieri</li> <li>Phone: +39 366 777 6512</li> <li>Email: m.scudieri@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Giuseppe Miriello</li> <li>Phone: +39 340 222 0113</li> <li> <p>Email: g.miriello@medspa.it</p> </li> <li> <p>Full name: Enzo Franco Sparacino</p> </li> <li>Phone: +39 340 927 5210</li> <li>Email: e.sparacino@medspa.it</li> </ul> <p>Customer's Alarms: Email: hubit@medspa.it</p>"},{"location":"customers/medspa-C2371/InfrastructureScaling/","title":"Infrastructure Scaling","text":""},{"location":"customers/medspa-C2371/InfrastructureScaling/#infrastructure-scaling","title":"Infrastructure Scaling","text":"<p>Not having autoscaling, in case you downgrade/upgrade the size of the EC2 server. It is necessary to modify the Nginx/PHP parameters agreed with the customer (Remember to restart the services).</p> <p>Before downgrading/upgrading the RDS, it is important to change it from Single-AZ to Multi-AZ, reducing downtime while the downgrade/upgrade is being done. After the procedure, you should return to the single AZ.</p>"},{"location":"customers/medspa-C2371/InfrastructureScaling/#standard-normal-traffic-situation","title":"Standard normal traffic situation:","text":""},{"location":"customers/medspa-C2371/InfrastructureScaling/#ec2-c59xlarge","title":"EC2 c5.9xlarge:","text":"<p>Nginx configuration:</p> <ul> <li>/etc/nginx/nginx.conf \u2192 worker_processes \"6\";</li> </ul> <p>PHP configuration:</p> <ul> <li>/etc/php/7.3/fpm/pool.d/www.conf \u2192 pm.max_children = 400</li> <li>/etc/php/7.3/fpm/pool.d/www.conf -&gt; pm.start_servers = 50</li> <li>/etc/php/7.3/fpm/pool.d/www.conf \u2192 max_spare_servers = 100</li> </ul>"},{"location":"customers/medspa-C2371/InfrastructureScaling/#rds-dbm6g2xlarge","title":"RDS db.m6g.2xlarge","text":"<p>Parametri: log_bin_trust_function_creators = 1 innodb_thread_concurrency = 8 join_buffer_size = 8388608 long_query_time = 1 slow_query_log = 1 thread_cache_size = 32 tmp_table_size = 268435456</p>"},{"location":"customers/medspa-C2371/InfrastructureScaling/#standard-situation-heavy-traffic","title":"Standard situation heavy traffic:","text":""},{"location":"customers/medspa-C2371/InfrastructureScaling/#ec2-c512xlarge","title":"EC2 c5.12xlarge:","text":"<p>Nginx configuration:</p> <ul> <li>/etc/nginx/nginx.conf \u2192 worker_processes \"8\";</li> </ul> <p>PHP configuration:</p> <ul> <li>/etc/php/7.3/fpm/pool.d/www.conf \u2192 pm.max_children = 500</li> <li>/etc/php/7.3/fpm/pool.d/www.conf -&gt; pm.start_servers = 75</li> <li>/etc/php/7.3/fpm/pool.d/www.conf \u2192 max_spare_servers = 150</li> </ul>"},{"location":"customers/medspa-C2371/InfrastructureScaling/#rds-dbm6g4xlarge","title":"RDS db.m6g.4xlarge","text":"<p>Parametri: log_bin_trust_function_creators = 1 innodb_thread_concurrency = 16 join_buffer_size = 8388608 long_query_time = 1 slow_query_log = 1 thread_cache_size = 64 tmp_table_size = 268435456</p>"},{"location":"customers/medspa-C2371/InfrastructureScaling/#urgent-high-traffic-configuration","title":"Urgent high traffic configuration:","text":""},{"location":"customers/medspa-C2371/InfrastructureScaling/#ec2-c518xlarge","title":"EC2 c5.18xlarge:","text":"<p>Nginx configuration:</p> <ul> <li>/etc/nginx/nginx.conf \u2192 worker_processes \"16\";</li> </ul> <p>PHP configuration:</p> <ul> <li>/etc/php/7.3/fpm/pool.d/www.conf \u2192 pm.max_children = 500</li> <li>/etc/php/7.3/fpm/pool.d/www.conf -&gt; pm.start_servers = 75</li> <li>/etc/php/7.3/fpm/pool.d/www.conf \u2192 max_spare_servers = 150</li> </ul>"},{"location":"customers/medspa-C2371/InfrastructureScaling/#rds-m6g8xlarge","title":"RDS m6g.8xlarge","text":"<p>Parametri: log_bin_trust_function_creators = 1 innodb_thread_concurrency = 32 join_buffer_size = 8388608 long_query_time = 1 slow_query_log = 1 thread_cache_size = 64 tmp_table_size = 268435456</p>"},{"location":"customers/medspa-C2371/medspaitprd/","title":"http://medspa.it/","text":""},{"location":"customers/medspa-C2371/medspaitprd/#data-center","title":"Data center","text":"<ul> <li> AWS: medspa</li> </ul> Text Only<pre><code>Account Alias: medspa\nAccount ID: 703194009217\n</code></pre>"},{"location":"customers/medspa-C2371/medspaitprd/#teleport-how-to-access-to-virtual-machines","title":"Teleport (How to access to Virtual Machines)","text":"<p>The VMs are accessible via one of the following links (federated teleport cluster):</p> <ul> <li>https://cc-ds1.criticalcasecloud.com/</li> <li>https://cc-ds2.criticalcasecloud.com/</li> </ul> <p>When, you logged in, slecet the cluster in below from the \"CLUSTERS\" section:</p> <ul> <li>tp.medspa-miamo.criticalcasecloud.com</li> </ul>"},{"location":"customers/medspa-C2371/medspaitprd/#architecture","title":"Architecture","text":""},{"location":"customers/medspa-C2371/medspaitprd/#notes","title":"Notes","text":"<ul> <li>DNS is managed by the Route 53</li> <li>Backups are automated by AWS Backup</li> <li>Credentials are under Sherlock</li> <li>The project is deployed via terraform</li> </ul>"},{"location":"customers/medspa-C2371/medspaitprd/#network","title":"Network","text":"<p>There is only one VPC. There are 3 private subnets and 2 public subnets for environment (total 5 subnets with 57 IPs addresses each). The public subnets all use the same Internet Gatewayand the private subnets use Nat Gateway. here are no reachability limits on the NACLs of the subnets, but it's all managed by Security Groups.</p>"},{"location":"customers/medspa-C2371/medspaitprd/#prd","title":"Prd","text":"<p>EC2 instance has been created in the private subnet (Private_PRD_B_MEDSPA.IT), it uses the production load balancer that is created in two AZ (eu-central-1b, eu-central-1c), it forwards the traffic to medspait ec2 instance and only accept traffic that come form cloudfront in HTTPS. The Cloudfront to protect the domain using Waf which protects against malicious requests and blocks untrusted IPs. There is a Custom Header to ensure that the Loadbalancer accepts traffic only from the Cloudfront and not directly (the traffic between them is encrypted). RDS Database used from Medspait frontend server and is encrypted using a KMS-managed encryption key KMS.</p> <p>The NGINX web server and PHP 8.0-FPM were installed and configured on the server using Ansible. The default NGINX configuration, located in /etc/nginx/sites-available/default, it runs on port 80 to handle incoming HTTP requests and serves files from /var/www/medspait, with index.php as the default entry point, it processes PHP requests via PHP-FPM (FastCGI) on 127.0.0.1:9000 and restricts access to hidden files (e.g., .htaccess) for security, also it protects the /system/ directory to prevent unauthorized access to sensitive application files.</p>"},{"location":"customers/medspa-C2371/miamo/","title":"miamo.com","text":""},{"location":"customers/medspa-C2371/miamo/#data-center","title":"Data center","text":"<ul> <li> AWS: medspa</li> </ul> Text Only<pre><code>Account Alias: medspa\nAccount ID: 703194009217\n</code></pre>"},{"location":"customers/medspa-C2371/miamo/#teleport-how-to-access-to-virtual-machines","title":"Teleport (How to access to Virtual Machines)","text":"<p>The VMs are accessible via one of the following links (federated teleport cluster):</p> <ul> <li>https://cc-ds1.criticalcasecloud.com/</li> <li>https://cc-ds2.criticalcasecloud.com/</li> </ul> <p>When, you logged in, slecet the cluster in below from the \"CLUSTERS\" section:</p> <ul> <li>tp.medspa-miamo.criticalcasecloud.com</li> </ul>"},{"location":"customers/medspa-C2371/miamo/#operations","title":"Operations","text":"<ul> <li>Infrastructure Scaling: Nginx and PHP configuration change in case of normal or heavy traffic</li> </ul>"},{"location":"customers/medspa-C2371/miamo/#architecture","title":"Architecture","text":"<p>Documenti consegnati:</p> <ul> <li>medspa_miamo_architecture_eng.pdf</li> <li>medspa_miamo_architecture_eng.docx</li> </ul>"},{"location":"customers/medspa-C2371/miamo/#notes","title":"Notes","text":"<ul> <li>DNS is managed by the customer</li> <li>Backups are automated by AWS Backup</li> <li>Credentials are under Sherlock</li> <li>The project is deployed via Terraform Cloud Prd and Stg. The code is on the GitHub</li> <li>Being in ITO, the account is controlled by both the Lambdas in cc-digital</li> <li>Resources are monitoring via Cloudwatch Alarms and Zabbix</li> <li>Check Alerts for alerts from Cloudwatch Credentials</li> <li>The account under ITO is monitored by Cloudtrail which sends the logs to S3, Logstash On-Premise retrieves this data and displays it to Kibana</li> </ul>"},{"location":"customers/medspa-C2371/miamo/#network","title":"Network","text":"<p>There is only one VPC. There are 5 subnets private and 4 subnets pubbliche for environment (total 9 subnets with 254 IPs addresses each). The public subnets all use the same Internet Gatewaynd the private one is the same Nat Gateway. here are no reachability limits on the NACLs of the subnets, but it's all managed by Security Groups</p>"},{"location":"customers/medspa-C2371/miamo/#development","title":"Development","text":"<p>The development environment is all within an all-in-one machine. Everything that it is possible to encrypt on at Rest would be done with KMS or AWS default keys, keep this convention. There is a Cloudfront which caches static content. There is a Custom Header to ensure that the Loadbalancer accepts traffic only from the Cloudfront and not directly (the traffic between them is encrypted). The Cloufront is associated with a WAFwhich are associated with rate-limit rules, allowed ip and malicious ip (updated every hour by our lambdas in cc-digital). the Waf is associated with a Kinesis which is responsible for sending the logs to Bucket S3. On-Premise Logstash fetches these logs and sends them to ours Kibana(Please remember to select the correct host), the latter is mandatory for the latest version of Magento. The internal DNS is managed by Route53. Varnish is installed on docker on the machine, port 8080. Managed by ansible /opt/docker-compose.yml:</p> Text Only<pre><code># Ansible managed\n\nversion: '2.1'\nservices:\n  varnish:\n    container_name: varnish\n    restart: unless-stopped\n    image: varnish:6.5.1\n    environment:\n      VARNISH_SIZE: \"512M\"\n    extra_hosts:\n      web: \"172.17.0.1\"\n    volumes:\n      - /opt/varnish/etc/default.vcl:/etc/varnish/default.vcl:ro\n    tmpfs:\n      - /var/lib/varnish:exec\n    ports:\n      - 8080:80\n</code></pre> <p>Nginx runs on port 80. There are redirects in the file /etc/nginx/nginx.conf requested by the customer and managed with Ansible.</p> <p>The build is done by the Codepipeline. The Codebuild is based on a custom image on ECR with the PHP cli installed, generated by a Codepipeline with this Dockerfile. The necessary credentials can be found on Parameter Store</p>"},{"location":"customers/medspa-C2371/miamo/#staging","title":"Staging","text":"<p>The stage environment is all within an all-in-one machine. Everything that it is possible to encrypt on at Rest would be done with KMS or AWS default keys, keep this convention. There is a Cloudfront which caches static content. There is a Custom Header to ensure that the Loadbalancer accepts traffic only from the Cloudfront and not directly (the traffic between them is encrypted). The Cloufront is associated with a WAFwhich are associated with rate-limit rules, allowed ip and malicious ip (updated every hour by our lambdas in cc-digital). the Waf is associated with a Kinesis which is responsible for sending the logs to Bucket S3. On-Premise Logstash fetches these logs and sends them to ours Kibana(Please remember to select the correct host), the latter is mandatory for the latest version of Magento. The internal DNS is managed by Route53. Varnish is installed on docker on the machine, port 8080. Managed by ansible /opt/docker-compose.yml:</p> Text Only<pre><code># Ansible managed\n\nversion: '2.1'\nservices:\n  varnish:\n    container_name: varnish\n    restart: unless-stopped\n    image: varnish:6.5.1\n    environment:\n      VARNISH_SIZE: \"512M\"\n    extra_hosts:\n      web: \"172.17.0.1\"\n    volumes:\n      - /opt/varnish/etc/default.vcl:/etc/varnish/default.vcl:ro\n    tmpfs:\n      - /var/lib/varnish:exec\n    ports:\n      - 8080:80\n</code></pre> <p>Nginx runs on port 80. There are redirects in the file /etc/nginx/nginx.conf requested by the customer and managed with Ansible</p> <p>The build is done by the Codepipeline. The Codebuild is based on a custom image on ECR with the PHP cli installed, generated by a Codepipeline with this Dockerfile. The necessary credentials can be found on Parameter Store</p>"},{"location":"customers/medspa-C2371/miamo/#prd","title":"Prd","text":"<p>The production environment mirrors the stage environment. Everything that was possible to encrypt on at Rest was done with KMS or AWS default keys, keep this convention. There is Cloudfront which acts as a cache for static content. There is a Custom Header to ensure that the Loadbalancer accepts traffic only from the Cloudfront and not directly (the traffic between them is encrypted). The Cloufront is associated with a WAF which are associated with rate-limit rules, allowed ip and malicious ip (updated every hour by our lambdas in cc-digital). the Waf is associated with a Kinesis which is responsible for sending the logs to Bucket S3. On-Premise Logstash fetches these logs and sends them to ours Kibana(Remember to select the correct host). The bilanciatore The database is on RDS. There is a Redis(there is no https, but security is guaranteed at the networking level agreed with the customer) and a ElasticSearch, the latter is mandatory for the latest version of Magento. Internal DNS is managed by Route53</p> <p>Sulla macchine \u00e8 installato Varnish in docker, porta 8080. Gestito da ansible /opt/docker-compose.yml:</p> Text Only<pre><code># Ansible managed\n\nversion: '2.1'\nservices:\n  varnish:\n    container_name: varnish\n    restart: unless-stopped\n    image: varnish:6.5.1\n    environment:\n      VARNISH_SIZE: \"512M\"\n    extra_hosts:\n      web: \"172.17.0.1\"\n    volumes:\n      - /opt/varnish/etc/default.vcl:/etc/varnish/default.vcl:ro\n    tmpfs:\n      - /var/lib/varnish:exec\n    ports:\n      - 8080:80\n</code></pre> <p>Nginx runs on port 80. There are redirects in the file /etc/nginx/nginx.conf requested by the customer and managed with Ansible</p>"},{"location":"customers/medspa-C2371/miamo/#sftp","title":"SFTP","text":"<p>The customer needs to connect on premise to the production machine to retrieve orders via sftp. The sftp port is 22 open via whitelist with the client ip's on the bastion host. The bastion host has a login to log in, then bridges and lands on the production magento frontend machine Credentials</p> <p>Bastion Host ssh config: /etc/ssh/sshd_config</p> Text Only<pre><code>Match User miamo\n#ForceCommand internal-sftp\nPasswordAuthentication yes\n#ChrootDirectory /home\nPermitTunnel yes\nAllowAgentForwarding no\n#AllowTcpForwarding no\nX11Forwarding no\nClientAliveInterval  3600\n</code></pre> <p>Prd Magento Frontend ssh config: /etc/ssh/sshd_config</p> Text Only<pre><code>Match User miamo-sftp\nForceCommand internal-sftp -d /var/www/b2c/pub/orders_ftp\nPasswordAuthentication yes\n#ChrootDirectory /var/www/b2c/pub/orders_ftp\nPermitTunnel yes\nAllowAgentForwarding no\n#AllowTcpForwarding no\nX11Forwarding no\nClientAliveInterval  3600\n</code></pre>"},{"location":"customers/medspa-C2371/miamoacademy/","title":"http://miamoacademy.it/","text":""},{"location":"customers/medspa-C2371/miamoacademy/#data-center","title":"Data center","text":"<ul> <li> AWS: medspa</li> </ul> Text Only<pre><code>Account Alias: medspa\nAccount ID: 703194009217\n</code></pre>"},{"location":"customers/medspa-C2371/miamoacademy/#teleport-how-to-access-to-virtual-machines","title":"Teleport (How to access to Virtual Machines)","text":"<p>The VMs are accessible via one of the following links (federated teleport cluster):</p> <ul> <li>https://cc-ds1.criticalcasecloud.com/</li> <li>https://cc-ds2.criticalcasecloud.com/</li> </ul> <p>When, you logged in, slecet the cluster in below from the \"CLUSTERS\" section:</p> <ul> <li>tp.medspa-miamo.criticalcasecloud.com</li> </ul>"},{"location":"customers/medspa-C2371/miamoacademy/#architecture","title":"Architecture","text":""},{"location":"customers/medspa-C2371/miamoacademy/#notes","title":"Notes","text":"<ul> <li>DNS is managed by the customer</li> <li>Backups are automated by AWS Backup</li> <li>Credentials are under Sherlock</li> <li>The project is deployed via terraform</li> </ul>"},{"location":"customers/medspa-C2371/miamoacademy/#network","title":"Network","text":"<p>There is only one VPC. There are 5 subnets private and 4 subnets pubbliche for environment (total 9 subnets with 254 IPs addresses each). The public subnets all use the same Internet Gatewaynd the private one is the same Nat Gateway. here are no reachability limits on the NACLs of the subnets, but it's all managed by Security Groups</p>"},{"location":"customers/medspa-C2371/miamoacademy/#prd","title":"Prd","text":"<p>The project was created using the production infrastructure of miamo.com It's on a separate EC2, it uses the production balancer with a rule. The server hosts a Magento installation, with all essential services such as ElasticSearch, Redis, and MySQL deployed as Docker containers, making it an all-in-one server. Everything that was possible to encrypt on at Rest was done with KMS or AWS default keys, keep this convention. There is Cloudfront which acts as a cache for static content. There is a Custom Header to ensure that the Loadbalancer accepts traffic only from the Cloudfront and not directly (the traffic between them is encrypted).The load balancer</p> <p>Nginx runs on port 80. There are redirects in the file /etc/nginx/nginx.conf requested by the customer and managed with Ansible</p>"},{"location":"customers/medspa-C2371/miamoacademy/#sftp","title":"SFTP","text":"<p>The customer needs to connect on premise to the production machine to retrieve orders via sftp. The sftp port is 22 open via whitelist with the client ip's on the bastion host. The bastion host has a login to log in, then bridges and lands on the production magento frontend machine Credentials</p> <p>Bastion Host ssh config: /etc/ssh/sshd_config</p> Text Only<pre><code>Match User miamo\n#ForceCommand internal-sftp\nPasswordAuthentication yes\n#ChrootDirectory /home\nPermitTunnel yes\nAllowAgentForwarding no\n#AllowTcpForwarding no\nX11Forwarding no\nClientAliveInterval  3600\n</code></pre> <p>Prd Magento Frontend ssh config: /etc/ssh/sshd_config</p> Text Only<pre><code>Match User miamo-sftp\nForceCommand internal-sftp -d /var/www/b2c/pub/orders_ftp\nPasswordAuthentication yes\n#ChrootDirectory /var/www/b2c/pub/orders_ftp\nPermitTunnel yes\nAllowAgentForwarding no\n#AllowTcpForwarding no\nX11Forwarding no\nClientAliveInterval  3600\n</code></pre>"},{"location":"customers/microgame-C2311/","title":"Microgame Spa @ C2311","text":""},{"location":"customers/microgame-C2311/#ito-h24","title":"ITO H24","text":"<ul> <li>CR230146: Kubernetes on-premise su VMWare. Il servizio ITO copre l'ambiente di produzione anche se non sono ancora realmente in produzione. L'ambiente di dev e staging sono in best effort.</li> </ul>"},{"location":"customers/microgame-C2311/K8S-on-premise/","title":"Microgame Kubernetes VMWare","text":"<p>CR230146: K8s on-premise</p>"},{"location":"customers/microgame-C2311/K8S-on-premise/#note","title":"Note","text":"<p>Traffico verso 176.221.49.0/24</p> <p>Le rotte verso <code>176.221.49.0/24</code> non funzionano, se dovesse essere necessario contattare una VM su quella rete fare riferimento al ticket CS-11691.</p> <p>VC: https://vc5-is3.rz1.vdc.ccws.it</p>"},{"location":"customers/microgame-C2311/K8S-on-premise/VPN/","title":"Fortigate VPN","text":"<ul> <li>Login with administrator credentials: https://10.30.254.1:10443/</li> <li>Go to User Definition</li> <li>Select the user, i.e. s.russo</li> <li>Press Edit in CLI</li> <li>Change the password policy</li> </ul> Bash<pre><code># force the password reset\nset passwd-policy \"PrimoAccesso\"\nnext\n\n# restore the default passowrd policy\nset passwd-policy \"ScadenzaPasswordVPN\"\nnext\n\n# criticalcase passowrd policy\nset passwd-policy \"Criticalcase\"\nnext\n</code></pre>"},{"location":"customers/microgame-C2311/K8S-on-premise/VPN/#show-password-policies","title":"Show password policies","text":"<p>Open the fortigate CLI Console</p> Bash<pre><code>config user password-policy\nshow\n\nconfig user password-policy\n    edit \"ScadenzaPasswordVPN\"\n        set expire-days 90\n        set warn-days 16\n        set expired-password-renewal enable\n    next\n    edit \"PrimoAccesso\"\n        set expire-days 1\n        set warn-days 1\n        set expired-password-renewal enable\n    next\n    edit \"Criticalcase\"\n        set expired-password-renewal enable\n    next\nend\n\n# Edit\nedit \"ScadenzaPasswordVPN\"\nset expire-days 90\nset expired-password-renewal enable\nnext\nshow\nend\n</code></pre>"},{"location":"customers/microgame-C2311/K8S-on-premise/github/","title":"GitHub","text":"<p>The customer created for Criticalcase a github account with access to customer repositories. Credentials can be found on sherlock. Use Vault CLI for the MFA.</p>"},{"location":"customers/microgame-C2311/K8S-on-premise/github/#git-clone","title":"Git clone","text":"<p>Follow these steps to be able to clone the repositories.</p> <ul> <li>Generate ssh key-pair</li> </ul> Text Only<pre><code>ssh-keygen -f ~/.ssh/microgame_rsa\n</code></pre> <ul> <li>Go to github with the criticalcase account and add the public key to https://github.com/settings/keys</li> <li>Add this snippet to <code>~/.ssh/config</code></li> </ul> Text Only<pre><code>Host github.com-microgame\n    HostName github.com\n    User git\n    IdentityFile ~/.ssh/microgame_rsa\n</code></pre> <ul> <li>Finally clone the repository</li> </ul> Text Only<pre><code>git clone git@github.com-microgame:mg-sw-factory/mg-devops.git\n</code></pre>"},{"location":"customers/mobilesoft-C1887/","title":"Mobilesoft @ C1887","text":""},{"location":"customers/mobilesoft-C1887/#ito-h24","title":"ITO H24","text":"<ul> <li>CR220377: FNMPay - Portale di pagamenti sul Cloud su Azure</li> </ul>"},{"location":"customers/mobilesoft-C1887/#architettura","title":"Architettura","text":"<p>Documentazione Tecnica</p>"},{"location":"customers/mobilesoft-C1887/#note","title":"Note","text":"<ul> <li>Il progetto \u00e8 su Azure con subscription FNM PAY con id ca0a7155-8696-47a8-96c9-99b276769e96</li> <li>Il DNS \u00e8 gestito dal cliente</li> <li>Il backup del database di produzione \u00e8 sotto Azure</li> <li>Non ci sono persistent volumes</li> <li>Le credenziali sono sotto Sherlock</li> <li>Il progetto \u00e8 deployato tramite Terraform.</li> <li>Il codice applicativo del cliente per la parte CI e il repository con Flux per la parte CD sono sul GitLab del cliente.</li> <li>Si pu\u00f2 interagire con i cluster kubernetes tramite Rancher dashboard</li> <li>Le risorse sono sotto monitoring tramite Azure Monitor</li> <li>Utilizziamo Rundeck per spegnere le istanze di dev e stg nel weekend e dalle 20 alle 8 in settimana.</li> <li>Mongo Cloud Access Credentials</li> <li>Gitlab cliente Credenziali</li> </ul>"},{"location":"customers/mobilesoft-C1887/#hld-diagram","title":"HLD Diagram","text":"<p>L'infrastruttura \u00e8 quasi identica in tutti gli ambienti: stage, dev, production. Le differenze sono oltre al taglio delle istanze ed eventuale HA: - dev non \u00e8 collegato all'HUB di prod - dev non ha il private link verso storage account di tenant esterni (Trenord) - stage \u00e8 collegato in peering all'HUB Vnet per l'accesso ad una api nel tenant di Trenord - stage ha un private link che permette l'accesso ad uno storage account di dev nel tenant di Trenord - prod \u00e8 collegato in peering all'HUB Vnet per l'accesso ad una api nel tenant di Trenord - prod ha un private link che permette l'accesso ad uno storage account di produzione nel tenant di Trenord - prod ha una Hub Vnet con all'interno il Visrtual Network Gateway sul quale c'\u00e8 una connessione S2S verso Trenord - prod ha un ulteriore storage account e private link per i dati di Redmine, infatti prod ha un ulteriore applicativo installato da noi che \u00e8 appunto Redmine</p>"},{"location":"customers/mobilesoft-C1887/#dev","title":"Dev","text":""},{"location":"customers/mobilesoft-C1887/#stage-hld-diagram","title":"Stage HLD Diagram","text":""},{"location":"customers/mobilesoft-C1887/#production-hld-diagram","title":"Production HLD Diagram","text":""},{"location":"customers/mobilesoft-C1887/#azure-kubernetes-service","title":"Azure Kubernetes Service","text":""},{"location":"customers/mobilesoft-C1887/#access","title":"Access","text":"<p>You must use office IP or Criticalcase IPSec VPN to reach the clusters via Azure portal or kubectl.</p> <p>Each cluster is registered to Rancher, so it is possible to go to the Rancher dashboard to interact with all of them.</p> <p>In order to download the kubeconfig to reach via local terminal the clusters, the az cli must be installed. Then, it is possible to use the following commands to download the kubeconfig:</p> Bash<pre><code>az login # add the --use-device flag if you use wsl to login in external browser\naz account set --subscription ca0a7155-8696-47a8-96c9-99b276769e96\n# dev\naz aks get-credentials --resource-group development-rg --name dev-aks --file .kubie/fnmpay-dev-aks.yaml\n# stg\naz aks get-credentials --resource-group staging-rg --name stg-aks --file .kubie/fnmpay-stg-aks.yaml\n# prod\naz aks get-credentials --resource-group production-rg --name prod-aks --file .kubie/fnmpay-prod-aks.yaml\n</code></pre>"},{"location":"customers/mobilesoft-C1887/#sealed-secret","title":"Sealed Secret","text":"<p>Il backup delle chiavi usate da sealed secret \u00e8 presente su Sherlock</p>"},{"location":"customers/mobilesoft-C1887/AKS-Problems/","title":"AKS Problems","text":"<p>List of know AKS problems</p>"},{"location":"customers/mobilesoft-C1887/AKS-Problems/#aks-cluster-has-no-nodes","title":"AKS Cluster has no nodes","text":""},{"location":"customers/mobilesoft-C1887/AKS-Problems/#symptoms","title":"Symptoms:","text":"<p>Aks cluster does not start and has failed state.</p> <p>In AKS Activity log from Azure portal you can view:</p> Text Only<pre><code>\"The resource operation completed with terminal provisioning state 'Failed'\".\n</code></pre> <p>Also in AKS Visrtual Machine scale set activity log you can see:</p> Text Only<pre><code>\"Operation could not be completed as it results in exceeding approved Total Regional Cores quota. Additional details - Deployment Model: Resource Manager, Location: northeurope, Current Limit: 50, Current Usage: 49, Additional Required: 4, (Minimum) New Limit Required: 53. Submit a request for Quota increase at https://aka.ms/ProdportalCRP/#blade/Microsoft_Azure_Capacity/UsageAndQuota.ReactView/Parameters/%7B%22subscriptionId%22:%22ca0a7155-8696-47a8-96c9-99b276769e96%22,%22command%22:%22openQuotaApprovalBlade%22,%22quotas%22:[%7B%22location%22:%22northeurope%22,%22providerId%22:%22Microsoft.Compute%22,%22resourceName%22:%22cores%22,%22quotaRequest%22:%7B%22properties%22:%7B%22limit%22:53,%22unit%22:%22Count%22,%22name%22:%7B%22value%22:%22cores%22%7D%7D%7D%7D]%7D by specifying parameters listed in the \u2018Details\u2019 section for deployment to succeed. Please read more about quota limits at https://docs.microsoft.com/en-us/azure/azure-supportability/regional-quota-requests\".\n</code></pre> <p>Azure encountered a \"rare\" error (https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/known-issues) relative to the managed identity assigned to the agent pool. This caused the AKS scale set to create a number of nodes that, however, never went online and remained in this failed state. This saturated the vcpu limit (50) that Azure makes available per region and effectively prevented even the clusters from going on line due to the inability to create the nodes. Quotas are visible here: https://portal.azure.com/#view/Microsoft_Azure_Capacity/QuotaMenuBlade/~/myQuotas</p>"},{"location":"customers/mobilesoft-C1887/AKS-Problems/#resolution","title":"Resolution","text":"<p>We had to decouple the managed identity to the agent pool, delete all failed instances from VM scale set, re-couple the managed identity to the agent pool (section identity of portal agent pool page -&gt; User assigned) and force a hand update on all aks clusters to resolve the problem by:</p> Text Only<pre><code># az resource update --ids aks_cluster_id\n# es.\naz resource update --ids /subscriptions/ca0a7155-8696-47a8-96c9-99b276769e96/resourcegroups/staging-rg/providers/Microsoft.ContainerService/managedClusters/stg-aks\n</code></pre>"},{"location":"customers/museoegizio-C2320/","title":"Museo Egizio @ C2320","text":"<p>Commerciale: Silvano Griot</p>"},{"location":"customers/museoegizio-C2320/#aws","title":"Aws","text":"Text Only<pre><code>Account: 812953965013\nAlias: museoegizio\nroot: cr220190@criticalcase.com\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/museoegizio-C2320/#bucket-c2320-museoegizio-backup","title":"Bucket c2320-museoegizio-backup","text":"<p>Abbiamo creato un bucket S3 dove il cliente far\u00e0 in autonomia i backup. (credentials)</p>"},{"location":"customers/newpenta-C2429/","title":"New Penta Srl @ C2429","text":""},{"location":"customers/newpenta-C2429/#ito-h24","title":"ITO H24","text":"<ul> <li>CR230291: Account AWS 280010477674 - datahub.npdiet.com</li> </ul> <p>Internal data:</p> <ul> <li>Sales: Morena Scudieri</li> <li>Phone: +39 366 777 6512</li> <li>Email: m.scudieri@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Marco Devalle</li> <li>Email: marco.devalle@pentadiet.it</li> <li>Phone: +39 3202795599</li> </ul>"},{"location":"customers/newpenta-C2429/Ito/","title":"CR230291 e CR240178 : Servizio ITO su AWS","text":""},{"location":"customers/newpenta-C2429/Ito/#informazione-generali","title":"Informazione Generali","text":"<p>Il servizio CR230291 ha riguardato il servizio ITO della struttura esistente del cliente, mentre il CR240178 \u00e8 stato implementato da Criticalcase e ha riguardato la migrazione dei servizi di DigitalOcean su AWS. Di conseguenza, l'infrastruttura esistente \u00e8 inizialmente gestita tramite la console, mentre l'infrastruttura creata nel progetto CR240178 \u00e8 stata creata tramite codice terraform.</p>"},{"location":"customers/newpenta-C2429/Ito/#data-center","title":"Data center","text":"<ul> <li> AWS: newpenta</li> </ul> Text Only<pre><code>Account Alias: newpenta\nAccount ID: 280010477674\n</code></pre>"},{"location":"customers/newpenta-C2429/Ito/#operazioni","title":"Operazioni","text":"<ul> <li> <p>Consumo di RDS: ogni inizio del mese (~giorno 01) viene eseguito un processo mensile che genera avvisi sul consumo di CPU in RDS newpenta-postgres-cluster-instance-1, di solito perch\u00e9 il processo di vaccum automatico di postgreSQL viene eseguito insieme al processo. Questo comportamento \u00e8 noto al cliente e stiamo lavorando con lui per aumentare il limite massimo di ACU dell'istanza.</p> </li> <li> <p>Re-run/Re-full dei Dati Fatturati : Ricaricamento dei dati fatturati nel data lake su richiesta del cliente.</p> </li> <li> <p>Accesso ai database datalake e datahub : I passaggi per accedere ai database DATALAKE e DATAHUB.</p> </li> <li> <p>Accesso ai database: I passaggi per accedere ai database DBWEB e PDRM.</p> </li> <li> <p>Aggiungere un nuovo processo a datalake: Passi per l'aggiunta di un nuovo processo al flusso di lavoro del datalake eseguito per la tabella price_list_product_storico:</p> </li> <li>Creare la tabella orchestrator.price_list_product_storico nel database Postgresql di Datalake`</li> <li>Creare la cartella price_list_product_storico nel bucket it.newpenta.orchestrator</li> <li>Creare codice python /opt/price_list_product_storico.py sul server CRON per caricare i dati delle chiamate API nel file S3</li> <li>Creare la tabella price_list_product_storico nel Glue Catalog che punta alla cartella creata in s3</li> <li>Creare il codice python price_list_product_storico.py e includerlo in extra_files.zip nel bucket S3 it.newpenta.gluescripts. Questo codice viene utilizzato dal Glue Workflow per leggere i dati da S3 e caricarli nella tabella del database.</li> <li>Modificare Glue Crawler per caricare la tabella di catalogo price_list_product_storico</li> <li>Includere un trigger nel Glue Workflow che chiama il codice price_list_product_storico.py del job Glue</li> <li>Pianificare il codice python nel crontab dell'istanza CRON in modo che venga eseguito ogni giorno alle 02:00.`</li> </ul>"},{"location":"customers/newpenta-C2429/Ito/#teleport","title":"Teleport","text":"<p>Access to the VMs is possible via federated teleport cluster:</p> <p>Select the cluster:</p> <ul> <li><code>tp.newpenta-ito.criticalcasecloud.com</code></li> </ul>"},{"location":"customers/newpenta-C2429/Ito/#architettura","title":"Architettura","text":""},{"location":"customers/newpenta-C2429/Ito/#note","title":"Note","text":"<ul> <li> <p>Il cliente ha risorse sia nella region di Irlanda che di Parigi</p> </li> <li> <p>I backup sono automatizzati da AWS Backup</p> </li> <li>Le credenziali sono sotto Sherlock</li> <li>Le nostre risorse sono state create tramite Terraform Cloud Prd. Il codice \u00e8 su GitHub</li> <li>Essendo in ITO, l'account \u00e8 controllato sia dalle Lambda in cc-digital</li> <li>Le risorse sono sotto monitoring tramite Cloudwatch Alarms e Zabbix</li> <li>Controllare Alerta per allarmi derivanti da Cloudwatch Credenziali</li> <li>Documentazione tecnica delle tabelle</li> </ul>"},{"location":"customers/newpenta-C2429/Ito/#architettura-irlanda","title":"Architettura Irlanda","text":""},{"location":"customers/newpenta-C2429/Ito/#network","title":"Network","text":"<p>Esiste una sola: VPC. Ci sono 2 subnets pubbliche. Le subnet pubbliche utilizzano tutte lo stesso Internet Gateway. Esistono 2 subnets private che utizzano lo stesso Nat Gateway. Tutte le subnet utilizzano per S3 un VPC Endpoint dedicato</p> <p>Non ci sono limiti di raggiungibilit\u00e0 sulle NACL delle subnets, ma \u00e8 gestito tutto dai Security Groups</p>"},{"location":"customers/newpenta-C2429/Ito/#prd-e-stg","title":"Prd e Stg","text":"<p>Il DNS \u00e8 gestito dal Cliente.</p> <p>Il server NEWPENTA01 ospita il sito pentadiet.it, ha nginx come middlewire</p> <p>bastion-host non \u00e8 il bastion che ospita Teleport ma \u00e8 un server utilizzato da loro per permettere al server CRON (attraverso un tunnel SSH) di connettersi al RDS</p> <p>Il server CRON ha degli script python sotto /opt che girano a determinate ore del giorno.</p> Bash<pre><code># Edit this file to introduce tasks to be run\n#\n# At 05:45 on day-of-month 1\n45 5  1 * * python3 /opt/full_campi_calcolati_pazienti.py\n# At 05:15 on day-of-month 1\n15 5  1 * * python3 /opt/full_campi_calcolati_medici.py\n# On demand\n#30 22 * * * python3 /opt/fatturato_range_date.py\n# At 03:00 every day\n00 3  * * * python3 /opt/fatturato_daily.py\n# At 2:30 every day\n30 2  * * * python3 /opt/pazienti_daily.py\n# At 2:00 every day\n00 2  * * * python3 /opt/medici_daily.py\n# At 3:30 every day\n30 3  * * * python3 /opt/ordini_daily.py\n# At 4:45 every day\n45 4  * * * python3 /opt/storico_livello.py\n# At 1:00 every month day 1\n00 1  1 * * python3 /opt/calendario.py\n# At 5:45 every day (except 1st day of month)\n45 5  2-31 * * python3 /opt/daily_campi_calcolati_pazienti.py\n# At 5:15 every day (except 1st day of month)\n15 5  2-31 * * python3 /opt/daily_campi_calcolati_medici.py\n# At 7:00 every day\n00 7  * * * python3 /opt/check_job_runs.py\n</code></pre> <p>Alcuni di questi scripts interagiscono con RDS, recuperano delle informazioni all'interno delle tabelle, compiono operazioni e le inseriscono in altre tabelle.</p> <p>Altri scripts si connettono ad un endpoint con username e password e recuperano informazioni dal loro gestionale. Vengono poi salvate su S3 tramite questi credenziali IAM. Glue utilizza dei jobs per modificare i dati e PowerBI li recupera creando delle dashboards (non abbiamo accesso a questa parte)</p> <p>GatewayPowerBI-1 \u00e8 utilizzato dalla societ\u00e0 BiosManagement, ha installato un driver di PowerBI che si collega a RDS per recuperare le tabelle e le manda verso Microsoft.</p> <p>Le applicazioni DBWEB e PDRM utilizzano ciascuna un Load Balancer che invia le richieste alle istanze EC2 su una subnet privata.</p> <p>Il database della soluzione DBWEB \u00e8 MariaDB, mentre quello della soluzione PDRM \u00e8 MySQL, entrambi in esecuzione su RDS in una rete privata.</p> <p>Il server DBWEB_stg ospita il sito web di stage shop.dev.pentadiet.it, che funziona su NodeJS con PHP(laravel).</p> <p>Il server DBWEB_prod ospita il sito di produzione shop.pentadiet.it, anch'esso basato su NodeJS e PHP(laravel).</p> <p>Il server PDRM_frontend ospita i siti: pdrm3.dev.pentadiet.it, medico.pentadiet.it e medico.dev.pentadiet.it. I siti vengono eseguiti su NodeJS(Nuxt).</p> <p>Il server PDRM_backend ospita le API. Le api sono in PHP(laravel)</p> <p>Per accedere ai database DBWEB e PDRM, utilizzate i passaggi descritti in questo documento. Poich\u00e9 la versione di MariaDB richiesta non prevede la possibilit\u00e0 di creare un utente IAM del database, l'accesso al database DBWEB deve avvenire tramite un tunnel, mentre a MySQL di PDRM si pu\u00f2 accedere direttamente dal teleporto.</p>"},{"location":"customers/newpenta-C2429/Ito/#flusso-di-dati-data-lake","title":"Flusso di dati - Data Lake","text":"<p>I dati caricati nel data lake e visualizzati in Quicksight seguono il seguente flusso:</p> <p></p>"},{"location":"customers/newpenta-C2429/Ito/#architettura-parigi","title":"Architettura Parigi","text":""},{"location":"customers/newpenta-C2429/Ito/#network_1","title":"Network","text":"<p>Esistono due VPC, una non usata: VPC. Questa VPC \u00e8 in produzione ed \u00e8 stata creata tramite uno stack di Cloudformation Ci sono 3 subnets pubbliche. Le subnet pubbliche utilizzano tutte lo stesso Internet Gateway. Non ci sono limiti di raggiungibilit\u00e0 sulle NACL delle subnets, ma \u00e8 gestito tutto dai Security Groups</p>"},{"location":"customers/newpenta-C2429/Ito/#prd","title":"Prd","text":"<p>L'ambiente di produzione \u00e8 composto da un bilanciatore pubblico collegato ad un cluster ECS nel quale girano diversi servizi con i rispettivi tasks/containers. Alcuni di questi container sembrano collegati ad Kong altri custom. Si fa utilizzo di ECR per le immagini docker. Il container utilizzano comunque un cluster Kafka e un RDS</p>"},{"location":"customers/newpenta-C2429/Ito/#keycloak","title":"Keycloak","text":"<p>Per implementare Keycloak, \u00e8 stato utilizzato un cluster AWS ECS esistente denominato newpenta-cluster-1. Questo cluster gira su EC2 (ECS Instance - EC2ContainerService-newpenta-cluster-1), quindi \u00e8 stato necessario modificare la dimensione EC2 da m5.large a m5.xlarge, abilitando l'implementazione del contenitore Keycloak senza influire sulle prestazioni dell'altro servizi che erano gi\u00e0 in esecuzione l\u00ec.</p> <p>\u00c8 stato utilizzato anche il database Aurora Serverless PostgreSQL (newpenta-postgres-cluster) esistente, eliminando la necessit\u00e0 di creare una nuova istanza del database.</p> <p>Un certificato *.datahub.npdiet.com \u00e8 stato creato in AWS Certificate Manager e convalidato in Public Domain Register, il provider DNS di New Penta. AWS SSM Parameter Store \u00e8 stato utilizzato per archiviare il segreto dell'utente del database (utente keycloak) e il segreto Keycloak User Admin (utente amministratore). Vengono recuperati quando l'attivit\u00e0 viene sollevata.</p> <p>Sul load balancer esistente (newpenta-public-load-balancer), sono stati creati un nuovo gruppo target e una nuova regola HTTPS:443. Il gruppo target per ascoltare il Keycloak task e la nuova regola per reindirizzare tutte le chiamate con HTTP Host Header \"iam.datahub.npdiet.com\" al target group Keycloak.</p> <p>A livello di database, sono stati creati un nuovo database e un nuovo schema chiamato keycloak sull'Aurora Serverless Cluster esistente (newpenta-postgres-cluster), promuovendo la separazione logica dei dati Keycloak.</p>"},{"location":"customers/newpenta-C2429/ReRunFatturati/","title":"Re-Run Dati Fatturati","text":""},{"location":"customers/newpenta-C2429/ReRunFatturati/#contesto","title":"Contesto","text":"<p>New Penta ha un'architettura di data lake che, in generale, carica i dati acquisiti attraverso le chiamate API nel database PostgreSQL utilizzato da PowerBI, come descritto in dettaglio in questa documentazione.</p> <p>I dati principali caricati sono i dati di fatturazione che vengono caricati quotidianamente. Il processo che carica i dati di fatturazione viene eseguito sul server CRON, ed elabora sempre i dati del giorno precedente, quindi se viene eseguito il 02/10, sta caricando i dati del 01/10.</p> <p>A volte \u00e8 necessario ricaricare i dati di un giorno o di un periodo specifico per un'esigenza individuata dal cliente.</p> <p>La presente procedura ha lo scopo di descrivere nel dettaglio le azioni da intraprendere in caso di richiesta di questo tipo.</p> <p>Un esempio di come questa procedura viene richiesta dal cliente:</p> <p></p>"},{"location":"customers/newpenta-C2429/ReRunFatturati/#prerequisiti","title":"Prerequisiti","text":"<p>a) Disporre di uno strumento per l'esecuzione di comandi SQL, ad esempio DBeaver</p>"},{"location":"customers/newpenta-C2429/ReRunFatturati/#passi","title":"Passi","text":"<ol> <li>Accedere al database (tramite tunnel di tsh) per verificare il valore attuale dei dati fatturati nel giorno/nei giorni in cui \u00e8 stato richiesto il ricarico dei dati, in modo da poter confrontare se c'\u00e8 stata una differenza dopo il Re-Run.:</li> </ol> <p>a) Accedere a Teleport tramite la riga di comando tsh:</p> Text Only<pre><code>tsh login --proxy=tp.newpenta-ito.criticalcasecloud.com:443 --auth=local --user=criticalcase-admin tp.newpenta-ito.criticalcasecloud.com\n</code></pre> <p>b) Inserire la password dell'utente di teleport criticalcase-admin teleport che deve essere catturata da sherlock.</p> <p>c) Creare un tunnel utilizzando Teleport:</p> Text Only<pre><code>tsh ssh -L 5432:newpentadatalake-instance-1.cyuxtzympav6.eu-west-1.rds.amazonaws.com:5432 root@bastion-host\n</code></pre> <p>Questa connessione deve essere mantenuta aperta in modo che l'accesso al database rimanga disponibile mentre vengono effettuate le query al database.</p> <p>d) Accedere a un client SQL (esempio: DBeaver) passando le seguenti informazioni di connessione:</p> <p>Host: localhost Port: 5432 Database: NewPentaDataLake Username: powerbi Password: catturare in sherlock</p> <ol> <li>Eseguire la seguente query per acquisire il valore giornaliero dei dati fatturati, cambiare le date di esempio inserite ('2024-10-21','2024-10-22') con i giorni che devono essere ricaricati.:</li> </ol> Text Only<pre><code>select SUBSTRING ( document_date ,1 , 10 ),sum(amount) from orchestrator.v_fatturato vf\ngroup by  SUBSTRING ( document_date ,1 , 10 )\nhaving SUBSTRING ( document_date ,1 , 10 ) in ('2024-10-21','2024-10-22');\n</code></pre> <p>Registrare il risultato di questa query da qualche parte, in modo da poterlo confrontare dopo l'esecuzione del processo.</p> <ol> <li> <p>Accedere al server CRON tramite Teleport, con le credenziali ottenute da sherlock.</p> </li> <li> <p>Eseguire il processo python che ricarica i dati nel bucket S3 tramite chiamate API per l'intervallo di giorni richiesto. Questo processo verr\u00e0 eseguito con nohup per evitare che la disconnessione della sessione interrompa l'esecuzione del processo. Cambiare le date di esempio inserite '2024-10-21' '2024-10-22' e rerun_24_10_21_a_24_10_22.log con i giorni che devono essere ricaricati.</p> </li> </ol> Text Only<pre><code>nohup python3 /opt/fatturato_range_date.py '2024-10-21' '2024-10-22' &gt; rerun_24_10_21_a_24_10_22.log &amp;\n</code></pre> <p>Una volta terminato, verificate nel file di log generato che tutto sia andato a buon fine, mostrando un risultato come questo:</p> Text Only<pre><code>cat rerun_24_10_21_a_24_10_22.log\n</code></pre> <p></p> <p>In caso di problemi, il log /var/log/fatturato_range_date.log pu\u00f2 essere consultato per capire cosa \u00e8 successo.</p> <ol> <li> <p>Convalidare che il file sia stato generato/aggiornato nel bucket s3://it.newpenta.orchestrator/fatturato/, il giorno in cui \u00e8 stata eseguita la procedura. Ad esempio, se \u00e8 stata eseguita il 23/10/2024, il file generato si trover\u00e0 nel percorso s3://it.newpenta.orchestrator/fatturato/2024/10/23/.</p> </li> <li> <p>Eseguire il Workflow in AWS Glue per leggere e caricare i dati nel database PostgreSQL.</p> </li> </ol> <p>a) Accedere al servizio Glue dall'account New Penta, regione Irlanda.</p> <p>b) Accedere al \"Data Integration and ETL &gt; Workflows (orchestration)\":</p> <p></p> <p>c) Cliccare sul workflow \u201corchestrator-workflow\u201d</p> <p>d) Fare clic sul pulsante cerchiato in rosso \u201cRun Workflow\u201d: </p> <p>Il processo dura circa 12 minuti, ma pu\u00f2 variare.</p> <p>e) Confermare che il workflow \u00e8 stato eseguito correttamente controllando nella scheda \u201cHistory\u201d che la colonna Status = Completed:</p> <p></p> <ol> <li>Ora i dati sono stati caricati nel database e si pu\u00f2 verificare se i valori controllati sono cambiati confrontando quelli acquisiti nel passaggio 2, per questo \u00e8 sufficiente ripetere le attivit\u00e0 descritte nei passaggi 1 e 2.</li> </ol> <p>Rispondere a New Penta con i valori di questo confronto, i dati prima e dopo l'esecuzione del processo.</p>"},{"location":"customers/newpenta-C2429/aggiunta_nuovo_sales/","title":"Accesso ai database datalake e datahub","text":""},{"location":"customers/newpenta-C2429/aggiunta_nuovo_sales/#contesto","title":"Contesto","text":"<p>A volte \u00e8 necessario accedere ai database datalake e datahub per effettuare le analisi e controlli , per cui abbiamo creato questa guida che spiega passo passo come farlo. Per informazioni sull'architettura di questi due database, consultare la documentazione</p>"},{"location":"customers/newpenta-C2429/aggiunta_nuovo_sales/#passi","title":"Passi","text":"<ol> <li>Accedere ai database Datalake e Datahub (tramite tunnel di tsh):</li> </ol> <p>a) Accedere a Teleport tramite la riga di comando tsh:</p> Text Only<pre><code>tsh login --proxy=tp.newpenta-ito.criticalcasecloud.com:443 --auth=local --user=criticalcase-admin tp.newpenta-ito.criticalcasecloud.com\n</code></pre> <p>b) Inserire la password dell'utente di teleport criticalcase-admin teleport che deve essere catturata da sherlock.</p> <p>c) Per accedere al database del Datalake, creare un tunnel utilizzando Teleport :</p> Text Only<pre><code>tsh ssh -L 5432:newpentadatalake-instance-1.cyuxtzympav6.eu-west-1.rds.amazonaws.com:5432 root@bastion-host\n</code></pre> <p>d) Per accedere al database del Datahub, creare un tunnel utilizzando Teleport :</p> Text Only<pre><code>tsh ssh -L 5434:newpenta-postgres-cluster.cluster-cmzevur7wvwv.eu-west-3.rds.amazonaws.com:5432 root@bastion-host\n</code></pre> <p>e) Per accedere al database del Datalake, accedere a un client SQL (esempio: DBeaver) passando le seguenti informazioni di connessione:</p> <p>Host: localhost Port: 5432 Database: NewPentaDataLake Username: powerbi Password: catturare in sherlock</p> <p>f) Per accedere al database del Datahub, accedere a un client SQL (esempio: DBeaver) passando le seguenti informazioni di connessione:</p> <p>Host: localhost Port: 5434 Database: np-orders Username: postgres Password: catturare in sherlock</p>"},{"location":"customers/omicron-2231/","title":"Omicron Consulting s.r.l. @ C2231","text":""},{"location":"customers/omicron-2231/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR220535: Setup ed implementazione su ambiente Azure di PDC Academy BlockChainPro + ITO</li> </ul> <p>Internal data:</p> <ul> <li>Sales: Morena Scudieri</li> <li>Phone: +39 366 777 6512</li> <li>Email: m.scudieri@criticalcase.com</li> </ul>"},{"location":"customers/omicron-2231/PDC-Academy/","title":"PDC Academy","text":""},{"location":"customers/omicron-2231/PDC-Academy/#data-center","title":"Data Center","text":"<ul> <li> Azure: blockchainpro.onmicrosoft.com</li> </ul> Text Only<pre><code>Domain: blockchainpro.onmicrosoft.com\nName: C Blu Engineering SA\nSubscription: e3239ff7-a18c-40c4-96bc-9b28e4da6b42\n</code></pre>"},{"location":"customers/omicron-2231/PDC-Academy/#kick-off","title":"Kick-off","text":"<ul> <li>Grazia Paggi: PM lato omicron (collegamento blockchainpro - criticalcase)</li> <li>Celeste Trombacco: Sviluppo Business (parte commeriale)</li> <li>Nicola Roversi: PM lato BlockChain pro (referente)</li> <li> <p>Fabiano Battain: assistente di nicola PM consulenza e simili.</p> </li> <li> <p>Francesco De filippis: Frontend</p> </li> <li>Lombardi: Backend</li> <li> <p>marco Crotta: Backend</p> </li> <li> <p>Team URANO: Blu Engineering SA (societ\u00e0 svizzera). Confedes (Societ\u00e0 italiana)   Stefano Santaiti (Basse skills in generale)</p> </li> </ul>"},{"location":"customers/omicron-2231/PDC-Academy/#architettura","title":"Architettura","text":""},{"location":"customers/quanticogames-C2415/","title":"Quantico Games S.R.L. @ C2415","text":""},{"location":"customers/quanticogames-C2415/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR230253: Infrastruttura su AWS</li> </ul> <p>Internal data:</p> <ul> <li>Sales: Morena Scudieri</li> <li>Phone: +39 366 777 6512</li> <li>Email: m.scudieri@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name:</li> <li>Email:</li> <li>Phone: +39</li> </ul>"},{"location":"customers/quanticogames-C2415/AWS/","title":"Quantico Games su AWS CR230253","text":""},{"location":"customers/quanticogames-C2415/AWS/#data-center","title":"Data center","text":"<ul> <li> <p> AWS: quanticogames-staging</p> </li> <li> <p> AWS: quanticogames-prod</p> </li> </ul> Text Only<pre><code>Staging Account\nAccount Alias: quanticogames-staging\nAccount ID: 814665425262\n\nProduction Account\nAccount Alias: quanticogames-prd\nAccount ID: 492557707618\n</code></pre>"},{"location":"customers/quanticogames-C2415/AWS/#teleport-how-to-access-to-virtual-machines","title":"Teleport (How to access to Virtual Machines)","text":"<p>The VMs are accessible via one of the following links (federated teleport cluster):</p> <ul> <li>https://tp.quanticogames.criticalcasecloud.com/web/</li> </ul>"},{"location":"customers/quanticogames-C2415/AWS/#architecture","title":"Architecture","text":"<ul> <li>Documento consegnata</li> </ul>"},{"location":"customers/quanticogames-C2415/AWS/#notes","title":"Notes","text":"<ul> <li>Status Report Document Model</li> </ul>"},{"location":"customers/quanticogames-C2415/AWS/#network","title":"Network","text":"<p>Staging Account: </p> <p>Production Account: </p> <p>xxxxxxxxxx</p>"},{"location":"customers/quanticogames-C2415/AWS/#staging","title":"Staging","text":""},{"location":"customers/quanticogames-C2415/AWS/#production","title":"Production","text":""},{"location":"customers/realize-C2269/","title":"Realize S.R.L. @ C2269","text":""},{"location":"customers/realize-C2269/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>~~CR210585: account AWS realize brunobarbieri.blog~~ disdetto!</li> </ul>"},{"location":"customers/realize-C2269/#note-h24","title":"Note H24","text":"<ul> <li>Le risorse sono sotto backup con AWS Backup</li> <li>Le risorse sono tutte criptate at Rest, mantenere questa convenzione il pi\u00f9 possibile</li> <li>Le credenziali sono su Sherlock</li> <li>Controllare Alerta per allarmi derivanti da Cloudwatch Credenziali</li> </ul>"},{"location":"customers/realize-C2269/#accesso","title":"Accesso","text":"<p>https://cc-ds1.criticalcasecloud.com/web/cluster/tp.realizesrl.criticalcasecloud.com/nodes</p> <ul> <li>Credenziali: https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/list/ds/customers/realize-C2269/</li> </ul>"},{"location":"customers/realize-C2269/#aws","title":"Aws","text":"Text Only<pre><code>Account: 358539099564\nAlias: realizesrl\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/realize-C2269/#newrelic-dismesso","title":"Newrelic - DISMESSO","text":"<ul> <li>Dashboard (credentials)</li> </ul>"},{"location":"customers/realize-C2269/#architecture","title":"Architecture","text":""},{"location":"customers/realize-C2269/#bastion-host","title":"Bastion Host","text":"<p>La macchina \u00e8 stata migrata dal Datacenter di Criticalcase in AWS</p> <p>Esiste una NAT nel bastion host sulla porta 22222 verso i servers. In questo modo il cliente riesce a connettersi all'istanza privata dal suo ufficio in sftp. Le regole iptables vengono aggiunte all'avvio della macchina. File di configurazione:</p> <p>/etc/networkd-dispatcher/routable.d/50-ifup-hooks</p> Text Only<pre><code>#!/bin/sh\n\nfor d in up post-up; do\n    hookdir=/etc/network/if-${d}.d\n    [ -e $hookdir ] &amp;&amp; /bin/run-parts $hookdir\ndone\nexit 0\n</code></pre> <p>/etc/network/if-up.d/pf</p> Text Only<pre><code>#!/bin/sh\n\nif [ \"$IFACE\" = ens5 ]; then\n\n#Barbieri\niptables -I PREROUTING 1 -t nat -i ens5 -p tcp --dport 22222 -j DNAT --to 172.31.60.216:60001\niptables -I FORWARD 1 -p tcp -d 172.31.60.216 --dport 60001 -j ACCEPT\niptables -t nat -I POSTROUTING 1 -o ens5 -p tcp -d 172.31.60.216 --dport 60001 -j SNAT --to 172.31.48.165\n\n#CottoAlDente\niptables -I PREROUTING 1 -t nat -i ens5 -p tcp --dport 22223 -j DNAT --to 172.31.60.101:60001\niptables -I FORWARD 1 -p tcp -d 172.31.60.101 --dport 60001 -j ACCEPT\niptables -t nat -I POSTROUTING 1 -o ens5 -p tcp -d 172.31.60.101 --dport 60001 -j SNAT --to 172.31.48.165\n\n\n#CottoAlDente OpenLiteSpeed WebAdmin\niptables -I PREROUTING 1 -t nat -i ens5 -p tcp --dport 37080 -j DNAT --to 172.31.60.101:7080\niptables -I FORWARD 1 -p tcp -d 172.31.60.101 --dport 7080 -j ACCEPT\niptables -t nat -I POSTROUTING 1 -o ens5 -p tcp -d 172.31.60.101 --dport 7080 -j SNAT --to 172.31.48.165\n\nfi\n</code></pre> <p></p>"},{"location":"customers/realize-C2269/#bruno-barbieri","title":"Bruno barbieri","text":"<p>Sulla macchina \u00e8 presente architettura LAMP, PHP 7.4 (fpm), Apache 2.4, MYSql 8.0 su RDS</p> <p>Per sftp, esiste un servizio dal nome proftpd. La configurazione \u00e8: /etc/proftpd/conf.d/sftp.conf</p> Text Only<pre><code> &lt;IfModule mod_sftp.c&gt;\n  SFTPEngine ON\n  Port 60001\n  SFTPHostKey /etc/proftpd/ssh/ssh_host_rsa_key\n  SFTPHostKey /etc/proftpd/ssh/ssh_host_ecdsa_key\n  SFTPLog /var/log/proftpd/sftp.log\n  SFTPCompression delayed\n  SFTPAuthMethods publickey\n  SFTPAuthorizedUserKeys file:/etc/proftpd/ssh/authorized_keys/%u\n\n  DefaultRoot ~\n  RequireValidShell off\n  AuthUserFile /etc/proftpd/ftpd.passwd\n  AuthGroupFile /etc/proftpd/ftpd.group\n  AuthOrder mod_auth_file.c\n  SFTPPAMEngine off\n  UseReverseDNS off\n  MaxLoginAttempts 5\n &lt;/IfModule&gt;\n</code></pre> <p></p> <p>Risorse</p> <ul> <li>Route53</li> <li>Cloudfront</li> <li>S3</li> <li>ALB</li> <li>EC2</li> <li>CloudWatch</li> <li>RDS</li> <li>SNS</li> </ul>"},{"location":"customers/realize-C2269/#errori-502503504","title":"Errori 502/503/504","text":"<p>Il sito \u00e8 progettato per ricevere diversi allarmi nel caso in cui ci siano problemi applicativi. Esso \u00e8 sufficientemente prestante, ma non \u00e8 stato testato il suo limite. Nel caso in cui si ricevano errori 502/503/504 e quindi un alert di Pingdom o Cloudwatch (AWS), verr\u00e0 mostrata questa pagina statica salvata su S3 (quindi fuori dal server): https://brunobarbieri.blog/inattesa.html</p> <p></p> <p>Non farsi ingannare da questa pagina, il sito \u00e8 down e bisogna intervenire</p>"},{"location":"customers/realize-C2269/#cotto-al-dente","title":"Cotto al dente","text":"<p>Il progetto non \u00e8 ancora in ITO</p> <p>Sulla macchina \u00e8 presente architettura LAMP, PHP 7.4 (fpm), Apache 2.4, MYSql 8.0 su RDS</p> <p></p> <p>Risorse</p> <ul> <li>Il DNS \u00e8 gestito da cliente</li> <li>Cloudfront</li> <li>ALB</li> <li>EC2</li> <li>CloudWatch</li> <li>RDS</li> <li>SNS</li> </ul>"},{"location":"customers/sequar-C2113/","title":"SEQUAR S.r.l. @ C2113","text":""},{"location":"customers/sequar-C2113/#ito-h24","title":"ITO H24","text":"<ul> <li>CR190704: ERP su Kubernetes on-premise con Odoo e RabbitMQ. (erp.sequar.com, erp.libellula.eu)</li> </ul>"},{"location":"customers/sequar-C2113/#note-h24","title":"Note H24","text":"<p>Issue recenti:</p> <ul> <li> <p><code>ITO-564</code>: HAProxy gestiva le connessioni in modo estremamente lento, aggiornando il pacchetto il problema si \u00e8 risolto.</p> </li> <li> <p><code>ITO-1210</code>: Il database ha raggiunto il numero massimo di sessioni, killando le sessioni idle il problema si \u00e8 risolto.</p> </li> </ul>"},{"location":"customers/sequar-C2113/PoC/","title":"Libellula PoC AWS","text":"<ul> <li>https://calculator.aws/#/estimate?id=18e6a6aa10b2214ed213f85c1bebef0d581f60cf</li> </ul>"},{"location":"customers/sequar-C2113/PoC/#documentazione-al-1506","title":"Documentazione al 15/06","text":""},{"location":"customers/sequar-C2113/PoC/#installazione-aws-cli","title":"Installazione AWS CLI","text":"<p>Per permettere l'accesso al cluster Kubernetes utilizzando le credenziali AWS, \u00e8 necessario installare l'ultima versione della AWS CLI attraverso i seguenti comandi (per ubuntu):</p> Text Only<pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64-2.7.7.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n</code></pre> <p>Effettuare l'accesso alla AWS CLI con access key e secret key del proprio profilo AWS</p> Text Only<pre><code>aws configure --profile &lt;es. n.cognome@sequar.com&gt;\n    AWS Access Key ID [None]: &lt;access key&gt;\n    AWS Secret Access Key [None]: &lt;secret key&gt;\n    Default region name [None]: eu-central-1\n    Default output format [None]:\n</code></pre> <p>Verificare tramite il seguente comando di aver effettuato la connessione con successo:</p> Text Only<pre><code>aws s3 ls --profile &lt; es. n.cognome@sequar.com&gt;\n</code></pre>"},{"location":"customers/sequar-C2113/PoC/#installazione-kubectl","title":"Installazione kubectl","text":"<p>Utilizzare i seguenti comandi (su ubuntu) e verificare di aver installato la versione 1.24 o superiore:</p> Text Only<pre><code>snap install kubectl --classic\nkubectl version --client\n</code></pre>"},{"location":"customers/sequar-C2113/PoC/#installazione-kubie","title":"Installazione kubie","text":"<p>Al fine di semplificare l'interazione con pi\u00f9 cluster Kubernetes o con i diversi namespace all'interno di un cluster, \u00e8 possibile installare ed utilizzare il tool Kubie. Eseguire i seguenti comandi per installarlo (su Ubuntu):</p> Text Only<pre><code>wget https://github.com/sbstp/kubie/releases/download/v0.17.0/kubie-linux-amd64\nchmod +x kubie-linux-amd64\nsudo mv kubie-linux-amd64 /usr/local/bin/kubie\n</code></pre> <p>Una volta installato, bisogna creare nel path <code>~/.kube/</code> un file chiamato <code>kubie.yaml</code> contenente la seguente configurazione:</p> Text Only<pre><code>---\nshell: zsh\nconfigs:\n\n  include:\n    - /shared/kubernetes/*.config\n    - /shared/kubernetes/*.yaml\n</code></pre> <p>Tale configurazione presuppone che la shell in utilizzo sulla macchina sia zsh (si pu\u00f2 verificare con il comando <code>echo $SHELL</code>) e che i kube-config siano collocati nel path <code>/shared/kubernetes/</code>.</p> <p>Uno strumento utile che permette di interagire in maniera pi\u00f9 comoda con kubie \u00e8 fzf, installabile con il seguente comando:</p> Text Only<pre><code>sudo apt-get -y install fzf\n</code></pre>"},{"location":"customers/sequar-C2113/PoC/#accesso-al-cluster-kubernetes","title":"Accesso al cluster kubernetes","text":"<p>Utilizzare il seguente kube-config per accedere al cluster, inserendo alla riga 30 il nome del profilo con cui si \u00e8 effettuato l'accesso all'aws-cli negli step precedenti:</p> Text Only<pre><code>apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1EWXhNREV5TXpnMU0xb1hEVE15TURZd056RXlNemcxTTFvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTnlICnJndm01cU9HL1pQeWRiNzRIUjQ5RlFudjgwZERZWURidTVEVUQyNDhVdWczN1d3ckl6cmtZVnFqTCtqNWZKZmwKNDdEMGJOZE5nVlk5UVFiODdCckFMMEFQazc4d3laOUJUQnNabVZqM2RBUmRTR2hZZWVNNlRXd2drTUFMelVGNwovd2dpQzJQTEJ1SlF6ZW9YT20xOHFBNzB1eUJKRTVMR2ljaHlDVDd1YVJ1OVlJa3hkS2RNbXk1cDAzdXAxT0dxCmVSeVZNRm5RU3ZBY01rMUJsdXgyZXlJalVmQzFySDFmdkpocHFCSFJpcUE3MFBFalR4Wkx1U3hDbGFLNDNlK3oKbjYwQUh2cjhaem41Rjd4MjZLK0l6TVEyL0tmRlZVUk5pUFB3d3VkRXJuS2w5Ri9vK2VMUERpUVlpUGVPV3Q2RQpiVWpmZXVRRnBraHNaeWYvbjlzQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZQYzhjYjVLMGtTSDBTNHB3a1A3dk9WMGgwKzRNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBS0tPeGJid2dxWkQzWEtDazZlUwpvZXRGbktSeEZYY3FxNElxSU5oSTFkc0Jsb3Qvek5xQlVCaHY5TSt2QXU1RHBGTDJnUVZZa2RJYXNISk5xdWlPCjNFeE9oWkpKMnVnck82NHNxWCtydWtTVTMxT3BWZWQ3cjRXdHdxK3B1em5YeE1QNFJhby9XT2kzVzVhdm9CWlkKcXVsMmxTVHovVG14TEpHTmFoSWx6Nm1GYndCTmJ5eVpQaTAxSHZydG9hVUpJUEpGV2psMVhJNGFlWW9OL01BZQpzUlhPZk0xZnBzQkR0dStDWWowNzF2d296YWU4Z3BFUC9CSG9tdXllbGtQSERMdld6K2dpd2FkS3hoWWxxZVpjCng0MVBpUFZneStmZzJ0WXdrNEN4WlRCWjJVSml4R254T3lCYjE0aDNiZEFvWHlyYUZPVGxXaFNjL0d4NEVRRGYKL0NVPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n    server: https://84409811B8D155ABC4C16866F86F4F9A.gr7.eu-central-1.eks.amazonaws.com\n  name: arn:aws:eks:eu-central-1:408130651362:cluster/libellula\ncontexts:\n- context:\n    cluster: arn:aws:eks:eu-central-1:408130651362:cluster/libellula\n    user: arn:aws:eks:eu-central-1:408130651362:cluster/libellula\n  name: aws-libellula\ncurrent-context: aws-libellula\nkind: Config\npreferences: {}\nusers:\n- name: arn:aws:eks:eu-central-1:408130651362:cluster/libellula\n  user:\n    exec:\n      apiVersion: client.authentication.k8s.io/v1beta1\n      args:\n      - --region\n      - eu-central-1\n      - eks\n      - get-token\n      - --cluster-name\n      - libellula\n      command: aws\n      env:\n      - name: AWS_PROFILE\n        value: &lt;es n.cognome@sequar.com&gt;\n</code></pre> <p>Utilizzare quindi il seguente comando per effettuare la connessione al cluster, scegliendo il cluster <code>aws-libellula</code> in caso di presenza di pi\u00f9 cluster disponibili:</p> Text Only<pre><code>kubie ctx\n</code></pre> <p>Verificare che la connessione sia avvenuta con successo con il comando</p> Text Only<pre><code>kubectl get nodes\n</code></pre> <p>Se la connessione \u00e8 avvenuta con successo, \u00e8 possibile navigare tra i diversi namespace utilizzando il comando:</p> Text Only<pre><code>kubie ns\n</code></pre>"},{"location":"customers/sequar-C2113/Rearchitecting/","title":"Sequar rearchitecting","text":"<ul> <li>Ansible + Terrafrom: https://github.com/criticalcase/sequar</li> <li>Flux new: https://github.com/criticalcase/flux-libellula-c1</li> <li>Flux old: https://git.criticalcase.com/flux/libellula-c1</li> </ul>"},{"location":"customers/sequar-C2113/Rearchitecting/#gitea-server","title":"Gitea server","text":"<p>Docker-compose file is available in the host machine at:</p> Text Only<pre><code>/opt/docker-compose.yml\n</code></pre> <p>it contains the configuration parameters to deploy the <code>gitea</code> container and the <code>mysql</code> container, used by both gitea and drone.</p> <p>Certificates are located in the following paths of the host machine:</p> Text Only<pre><code>/etc/ssl/certs/git-int.sequar.com.crt\n/etc/ssl/private/git-int.sequar.com.key\n</code></pre> <p>In the host machine, it is necessary to follow this guide to enable SSH Container Passthrough https://docs.gitea.io/en-us/install-with-docker/#sshing-shim-with-authorized_keys</p>"},{"location":"customers/sequar-C2113/Rearchitecting/#backup","title":"Backup","text":"<p>In the host machine, the following paths are mounted as volumes:</p> Text Only<pre><code>/opt/gitea\n/etc/timezone\n/etc/localtime\n/home/git/.ssh/\n/etc/ssl/certs/git-int.sequar.com.crt\n/etc/ssl/private/git-int.sequar.com.key\n/opt/mysql\n</code></pre>"},{"location":"customers/sequar-C2113/Rearchitecting/#mysql","title":"MySQL","text":"<p>In <code>mysql</code> container two databases are present: the first one is <code>gitea</code> and it's used by the <code>gitea</code> container, the second one is <code>drone</code> that is used by drone.</p>"},{"location":"customers/sequar-C2113/Rearchitecting/#drone-server","title":"Drone server","text":"<p>Docker-compose file is available in the host machine at:</p> Text Only<pre><code>/opt/docker-compose.yml\n</code></pre> <p>it contains the configuration parameters to deploy the <code>drone</code> container and the <code>drone-runner-docker</code> container. The <code>drone</code> container is the drone server accessible through web interface, while the <code>drone-runner-docker</code> container is the runner in charge of executing build pipelines.</p> <p>Certificates are located in the following paths of the host machine:</p> Text Only<pre><code>/etc/ssl/certs/drone-int.sequar.com.crt\n/etc/ssl/private/drone-int.sequar.com.key\n</code></pre>"},{"location":"customers/sequar-C2113/Rearchitecting/#backup_1","title":"Backup","text":"<p>In the host machine, the following paths are mounted as volumes:</p> Text Only<pre><code>/opt/drone\n/etc/ssl/certs/drone-int.sequar.com.crt\n/etc/ssl/private/drone-int.sequar.com.key\n</code></pre>"},{"location":"customers/sequar-C2113/Rearchitecting/#ecr","title":"ECR","text":"<p>Images created by drone are uploaded into ECR repositories:</p> <ul> <li>https://eu-central-1.console.aws.amazon.com/ecr/repositories?region=eu-central-1</li> </ul> <p>Credentials to let drone push images into ECR are saved into secrets. For example, for the sequar-docker repository, secrets are available at the following address:</p> <ul> <li>https://drone-int.sequar.com/sequar/sequar-docker/settings/secrets</li> </ul> <p><code>aws_access_key</code> and <code>aws_secret_key</code> are available in Sherlock:</p> <ul> <li>https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/show/ds/customers/sequar-C2113/ECR/sequar-ecr-drone</li> </ul>"},{"location":"customers/sequar-C2113/Rearchitecting/#sequar-docker-repository","title":"sequar-docker repository","text":""},{"location":"customers/sequar-C2113/Rearchitecting/#publish-tag","title":"publish-tag","text":"<p>An example of <code>.drone.yml</code> file is available in the <code>test-build</code> branch of <code>sequar-docker</code>repository. In the <code>settings</code> field of the <code>publish-tag</code> step is shown how to specify the location of the dockerfile, names of the secrets and tag to assign to the image.</p> Text Only<pre><code>  - name: publish-tag\n    image: plugins/ecr\n    environment:\n      PLUGIN_STORAGE_DRIVER: overlay2\n      PLUGIN_CACHE_FROM: 055165614569.dkr.ecr.eu-central-1.amazonaws.com/test_gitea:1.0.0_drone\n    settings:\n      context: odoo\n      dockerfile: odoo/Dockerfile\n      access_key:\n        from_secret: aws_access_key\n      secret_key:\n        from_secret: aws_secret_key\n      repo:\n        from_secret: aws_ecr_repo\n      registry:\n        from_secret: aws_ecr_registry\n      region:\n        from_secret: aws_region\n              - '1.0.0_drone'\n        - ${DRONE_COMMIT_SHA:0:8}\n</code></pre>"},{"location":"customers/sequar-C2113/Rearchitecting/#trigger","title":"trigger","text":"<p>In the <code>trigger</code> step, when the event is a push, the first 8 characters of the commit SHA are saved into the <code>DOWNSTREAM_IMAGE_VER</code> variable and the build described into <code>.drone.yml</code> of the <code>aws-ecr</code> branch of <code>sequar-clusters-drone</code> repository is started.</p> Text Only<pre><code>  - name: trigger\n    image: plugins/downstream\n    settings:\n      server: https://drone-int.sequar.com\n      token:\n        from_secret: drone_token\n      params:\n        - DOWNSTREAM_IMAGE_VER=${DRONE_COMMIT_SHA:0:8}\n      fork: true\n      repositories:\n        - sequar/sequar-clusters-drone@aws-ecr\n      when:\n        event: push\n</code></pre> <p>If the <code>DOWNSTREAM_IMAGE_VER</code> has some content (check performed in the update script), it means that the build is started from a push event and not from a tag event, so only in that scenario the new image, retrieved from the ECR repository, is deployed in the Kubernetes clusters.</p>"},{"location":"customers/sequar-C2113/VPN/","title":"VPN","text":"<p>To reach Sequar private applications like Drone and Gitea, we need to use the customer VPN.</p> <p>Guide to install the configuration is present on sharepoint together with the config file.</p> <p>Credentials are available on Sherlock</p>"},{"location":"customers/sequar-C2113/aws-poc/","title":"AWS PoC","text":"Text Only<pre><code>Account: 408130651362\nAlias: libellula\nEmail: aws@libellula.eu\n</code></pre> <ul> <li> <p>Login to AWS console</p> </li> <li> <p>https://calculator.aws/#/estimate?id=dbc0a13c4911e30e1107aead3433e84011097e45</p> </li> </ul> <p></p>"},{"location":"customers/sequar-C2113/kubify-libellula-c1/","title":"Kubify kubernetes libellula-c1","text":""},{"location":"customers/sequar-C2113/kubify-libellula-c1/#operations","title":"Operations","text":"<ul> <li>Kubernetes: comandi base per accedere alle API di kubernetes</li> <li>Errore connessione Sequar a database Postgres: passaggi per analizzare e risolvere il problema del numero massimo di connessioni nel database postgres</li> </ul>"},{"location":"customers/sequar-C2113/kubify-libellula-c1/#data-center","title":"Data center","text":"<ul> <li> Kubernetes cluster on-premise VC2-IS4-RZ2</li> <li> AWS eu-central-1: sequar (only for container registry)</li> </ul> Text Only<pre><code>Account Alias: sequar\nAccount ID: 055165614569\nEmail: aws@sequar.com\n</code></pre>"},{"location":"customers/sequar-C2113/kubify-libellula-c1/#teleport","title":"Teleport","text":"<p>Access to the VMs is possible via federated teleport cluster:</p> <p>Select the cluster:</p> <ul> <li><code>to2-k4y</code></li> </ul> <p>Or via</p> <ul> <li>https://access.to2.k4y.it:3080</li> </ul>"},{"location":"customers/sequar-C2113/kubify-libellula-c1/#nodes","title":"Nodes","text":"<p>Managed Kubernetes services</p> Text Only<pre><code>libellula-c1-m01.to2.k4y control-plane\nlibellula-c1-m02.to2.k4y control-plane\nlibellula-c1-m03.to2.k4y control-plane\nlibellula-c1-n04.to2.k4y ingress,kubify,monitoring\nlibellula-c1-n05.to2.k4y ingress,kubify,monitoring\n</code></pre> <p>Managed nodes used for customer applications</p> Text Only<pre><code>libellula-c1-n01.to2.k4y customer-node\nlibellula-c1-n02.to2.k4y customer-node\nlibellula-c1-n03.to2.k4y customer-node\nlibellula-c1-n06.to2.k4y customer-node\n</code></pre> <p>Managed SPOF VMs</p> Text Only<pre><code>libellula-c1-d01.to2.k4y NFS\nlibellula-c1-h01.to2.k4y HAProxy\n</code></pre> <p>Unmanaged VMs (the customer has root access):</p> Bash<pre><code>libellula-c1-d02.to2.k4y Postgres # SPOF\n</code></pre>"},{"location":"customers/sequar-C2113/kubify-libellula-c1/#kubernetes-services","title":"Kubernetes services","text":"<ul> <li>Grafana (credentials)</li> <li>Alertmanager (credentials)</li> <li>Prometheus (credentials)</li> </ul>"},{"location":"customers/sequar-C2113/kubify-libellula-c1/#schema-architetturale","title":"Schema architetturale","text":""},{"location":"customers/sequar-C2113/software/","title":"Software","text":"<p>Il cliente vorrebbe realizzare un pannello di gestione del software Sequar basato su odoo:</p> <ul> <li>Creare una nuova istanza dell'applicazione composta da 2 deployment su kubernetes. Ciascuna istanza potr\u00e0 gestire da uno a pi\u00f9 clienti, ciascuno con il proprio database Postgresql.</li> <li>Aggiungere all'istanza un nuovo cliente</li> <li>Gestire gli aggiornamenti di odoo tra una versione e l'altra dando la possibilit\u00e0 ad un operatore di effettuare il rollback.</li> </ul> <p>Una soluzione ottimale potrebbe essere quella di scrivere un kubernetes operator che gestisca tutto il ciclo di vita dell'applicazione sequar:</p> <ul> <li>https://kubernetes.io/docs/concepts/extend-kubernetes/operator/</li> <li>https://sdk.operatorframework.io/</li> </ul> <p>Definiremo quindi una nuova CRD chiamata sequar, l'operator di occuper\u00e0 di gestire tutto il ciclo di vita dell'applicazione.</p>"},{"location":"customers/sequar-C2113/software/#definizioni","title":"Definizioni","text":"<p>Cluster odoo / Istanza odoo: Applicazione Odoo che pu\u00f2 avere pi\u00f9 di un customer. Generalmente \u00e8 costituita da:</p> <ul> <li>Cron: 1 Pod</li> <li>Frontend: 2 Pod</li> </ul> <p>Customer: Il cliente finale di sequar: ha un database e un hostname dedicato.</p> <p>Es. <code>cacao</code></p> <ul> <li>Hostname: <code>cacao.sequar.com</code></li> <li>Database: <code>sequar13-cacao</code></li> </ul> <p>Database server: Il server \u00e8 postgresql che contiene utenti e database..</p> <p>Es. <code>database1.sequar</code></p>"},{"location":"customers/sequar-C2113/software/#procedures","title":"Procedures","text":""},{"location":"customers/sequar-C2113/software/#new-cluster","title":"New cluster","text":"<ul> <li>Create namespace: <code>sequar13</code></li> <li>Create DB user and password: <code>sequar13</code> / <code>XXXXXXXX</code></li> <li>Create DB: <code>sequar13-cacao</code></li> <li>Create new cluster yaml configuration</li> </ul>"},{"location":"customers/sequar-C2113/software/#new-customer","title":"New customer","text":"<ul> <li>Select the namespace</li> <li>Create Ingress resource: <code>cacao.sequar.com</code></li> <li>Install script with params: DB <code>(sequar13)</code>, InstallationType <code>(basic/full)</code></li> <li>Restart pod to get new settings: 1 cron + 2 frontend</li> </ul>"},{"location":"customers/sequar-C2113/software/#update","title":"Update","text":"<ul> <li>Disable cron</li> <li>Execute the database migration to new version and wait the exit code</li> <li>Done: update the Pods</li> <li>Fail: rollback</li> </ul>"},{"location":"customers/sequar-C2113/software/#ui","title":"UI","text":""},{"location":"customers/sequar-C2113/software/#useful-commands-to-get-data","title":"Useful commands to get data","text":"<p>Get Namespace + Customer Hostname</p> Bash<pre><code>kubectl get ing -A  |awk '{print $1 \" | \" $4 }'\nsequar-c12 | achillizerbini.sequar.com\nsequar-c12 | achillizerbini.sequar.com\nsequar-c13 | aquacut.sequar.com\nsequar-c13 | aquacut.sequar.com\nsequar-c13 | metalscrapp.sequar.com\nsequar-c13 | metalscrapp.sequar.com\n</code></pre>"},{"location":"customers/sequar-C2113/kubify-libellula-c1/DatabaseMaxConnections/","title":"Errore connessione Sequar a database Postgres","text":"<p>Il cliente ci ha contattato dicendo che l'applicazione era inattiva con il seguente messaggio: \u201cRemaining connection slots are reserved for non-replication superuser connections\u201d.</p> <p>Per risolvere il problema sono state eseguite le seguenti operazioni:</p> <p>1) Connesso al database tramite Teleport:</p> <p>Cluster to2-k4y do Teleport &gt; Resource/server: libellula-c1-d02.to2.k4y</p> <p>Sul server libellula-c1-d02.to2.k4y, accedere con l'utente postgres:</p> Text Only<pre><code>su - postgres\n</code></pre> <p>Accedere al database:</p> Text Only<pre><code>psql\n</code></pre> <p>2) Controllare il numero massimo di connessioni configurate in postgresql:</p> Text Only<pre><code>SELECT current_setting('max_connections');\n</code></pre> <p>3) Controlla il numero massimo di connessioni attualmente in uso in postgresql:</p> Text Only<pre><code>SELECT count(*) AS total_sessoes_ativas FROM pg_stat_activity;\n</code></pre> <p>Se \u00e8 stato raggiunto il numero massimo di connessioni, vengono controllate le connessioni che sono rimaste inattive pi\u00f9 a lungo.</p> <p>4) Controlla le connessioni IDLE pi\u00f9 a lungo (ordinati in base al pi\u00f9 vecchio):</p> Text Only<pre><code>SELECT pid, usename, application_name, client_addr, state, query_start, now() - query_start AS duration\nFROM pg_stat_activity\nWHERE state = 'idle' AND now() - query_start &gt; interval '1 hour' order by duration desc;\n</code></pre> <p>Al cliente viene chiesto quale intervallo di tempo possiamo eliminare, ad esempio, le sessioni pi\u00f9 vecchie di 1 giorno e, dopo l'ok del cliente, eliminiamo le sessioni.</p> <p>5) Killare le sessioni:</p> <p>Attenzione: il comando seguente uccider\u00e0 le sessioni, quindi eseguitelo quando siete sicuri di poterlo killare.</p> <p>Eseguite il comando seguente inserendo l'elenco dei PID delle sessioni da uccidere,acquisito nel passaggio precedente, al posto della variabile : Text Only<pre><code>SELECT pg_terminate_backend(pid)\nFROM pg_stat_activity\nWHERE pid IN (&lt;CONNESSION_PIDS&gt;);\n</code></pre>"},{"location":"customers/sequar-C2113/kubify-libellula-c1/Kubernetes/","title":"kubernetes: libellula-c1","text":"<p>Connect via teleport or ssh to one control-plane node:</p> Text Only<pre><code>libellula-c1-m01.to2.k4y control-plane\nlibellula-c1-m02.to2.k4y control-plane\nlibellula-c1-m03.to2.k4y control-plane\n</code></pre> <p>Check the nodes status</p> Text Only<pre><code>\u279c kubectl get nodes\nNAME                       STATUS   ROLES                            AGE      VERSION\nlibellula-c1-m01.to2.k4y   Ready    master                           3y81d    v1.18.20\nlibellula-c1-m02.to2.k4y   Ready    master                           3y81d    v1.18.20\nlibellula-c1-m03.to2.k4y   Ready    master                           3y81d    v1.18.20\nlibellula-c1-n01.to2.k4y   Ready    node,odoo                        3y81d    v1.18.20\nlibellula-c1-n02.to2.k4y   Ready    node,odoo                        3y81d    v1.18.20\nlibellula-c1-n03.to2.k4y   Ready    db-master,node,odoo              3y81d    v1.18.20\nlibellula-c1-n04.to2.k4y   Ready    ingress,kubify,monitoring,node   2y312d   v1.18.5\nlibellula-c1-n05.to2.k4y   Ready    ingress,kubify,monitoring,node   2y312d   v1.18.5\nlibellula-c1-n06.to2.k4y   Ready    node,odoo                        2y219d   v1.18.5\n</code></pre> <p>If one node is NotReady, try to reboot the node.</p> <p>Do not reboot more than one node at time to avoid service disruption.</p> <p>Check non-running pods:</p> Text Only<pre><code>\u279c kubectl get pods -A | grep -v '1/1' | grep -v '2/2' | grep -v Completed\nNAMESPACE             NAME                                               READY   STATUS             RESTARTS   AGE\nvelero                velero-7b6599b476-qghcb                            0/1     CrashLoopBackOff   12947      529d\n</code></pre>"},{"location":"customers/sharryland-C2352/","title":"SharryLand","text":""},{"location":"customers/sharryland-C2352/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR220311: Analisi ed ottimizzazione della soluzione AWS SharryLand</li> </ul>"},{"location":"customers/sharryland-C2352/#aws","title":"AWS","text":"Text Only<pre><code>Account: 660444558662\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/sharryland-C2352/#architettura","title":"Architettura","text":"<p>Analisi: sharryland.xlsx</p>"},{"location":"customers/sharryland-C2352/#network","title":"Network","text":""},{"location":"customers/sharryland-C2352/#stg","title":"Stg","text":""},{"location":"customers/sharryland-C2352/#prd","title":"Prd","text":""},{"location":"customers/sharryland-C2352/#prd-api-serverless","title":"Prd API Serverless","text":""},{"location":"customers/sharryland-C2352/#prd-frontendwork-in-progress","title":"Prd Frontend(work in progress)","text":""},{"location":"customers/silent-pool-C0002/","title":"Silent Pool Spa @ C0002","text":""},{"location":"customers/silent-pool-C0002/#ito-h24","title":"ITO H24","text":"<ul> <li>CR010934: Gestione account AWS cc-poc-test 281670075220</li> </ul> <p>Attenzione! Questo non \u00e8 un vero cliente, serve per fare i test con il team di on-call.</p>"},{"location":"customers/sugar-C2070/","title":"Sugar S.r.l. @ C2070","text":""},{"location":"customers/sugar-C2070/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR200822: magento2 www.sugar.it</li> <li>CR200722: CDN e WAF su AWS</li> </ul> <p>Internal data:</p> <ul> <li>Referente Commerciale: Vittorino Aprile</li> <li>Telefono referente: +39 392 912 2831</li> <li>Email: v.aprile@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Nome e cognome: Ludovica Torelli</li> <li>Email: ludovica.torelli@sugar.it</li> <li>Telefono: +39 0575 354631/354866</li> </ul>"},{"location":"customers/sugar-C2070/#aws","title":"AWS","text":"Text Only<pre><code>Account: 285089775060\nAlias: sugarsrl\n</code></pre> <ul> <li>Login to AWS console</li> <li>Login to OnPremise WAF dashboard (credentials)</li> </ul>"},{"location":"customers/sugar-C2070/#cloudfront","title":"CloudFront","text":"<p>Il cliente usa l'header <code>CloudFront-Viewer-Country</code> (Variabile <code>HTTP_CLOUDFRONT_VIEWER_COUNTRY</code> da PHP) per geolicalizzare gli utenti del sito. Disabilitando CloudFront questo header non viene pi\u00f9 valorizzato.</p>"},{"location":"customers/sugar-C2070/#datacenter-rozzano","title":"Datacenter Rozzano","text":"<p>I ticket riguardanti gli applicativi vanno girati a Support L3. Il principale referente \u00e8 Eduardo Del Piano.</p> <ul> <li> RZ2: https://vc2-is3.rz1.vdc.ccws.it</li> </ul> <p>La struttura attuale di SUGAR si compone di tre server:</p> <ul> <li>1 Load Balancer: sugar-lb01.local.ccws.it</li> <li>2 Frontend Server: sugar-fr01.local.ccws.it</li> <li>3 Database Server: sugar-db01.local.ccws.it</li> </ul> <p>1 - Load Balancer (sugar-lb01.local.ccws.it)</p> Text Only<pre><code> Credenziali per l\u2019accesso SSH :\n\n IP: 176.221.51.16  UID:root\n</code></pre> <p>Sul Load Balancer \u00e8 presente un servizio proxy (haproxy) per verificare lo stato del servizio digitare il comando: systemctl status haproxy</p> <p>per riavviare il servizio digitare:</p> Text Only<pre><code>     systemctl restart haproxy\n</code></pre> <p>Una verifica preliminare una volta loggati \u00e8 quella di verificare lo spazio di storage digitando il comando df -h</p> <p>2 \u2013 Frontend Server (sugar-fr01.local.ccws.it)</p> Text Only<pre><code> Credenziali per l\u2019accesso SSH:\n\n IP:10.101.0.65 UID:root\n</code></pre> <p>Per accedere al Frontend Server bisogna loggarsi prima sul loadbalancer e da li lanciare una connessione ssh verso l\u2019ip indicato.</p> <p>Sul Frontend server sono persenti i servizi di Apache2 e PHP-FPM, per verificare lo stato dei servizi digitare il comando :</p> Text Only<pre><code>    systemctl status apache2\n</code></pre> <p>per il webserver, mentre per il PHP-FPM digitare</p> Text Only<pre><code>    systemctl status php7.2-fpm\n</code></pre> <p>Una verifica preliminare una volta loggati \u00e8 quella di verificare lo spazio di storage digitando il comando df -h</p> <p>3 - Database Server \u2013 (sugar-db01.local.ccws.it)</p> <p>Credenziali per l\u2019accesso SSH:</p> <p>IP: 10.101.0.117 UID:root</p> <p>Per accedere al Database Server bisogna loggarsi prima sul loadbalancer e da li lanciare una connessione ssh verso l\u2019ip indicato.</p> <p>Come controllo preliminare verificare sempre lo spazio di storage digitando il comando df -h</p> <p>Per verificare lo stato del servizio del database digitare il comando:</p> Text Only<pre><code>        systemctl status mysql\n</code></pre> <p>Ulteriore verifica a scopo manutentivo e quello di verificare quanto spazio occupa il database sul Fliesystem , per eseguire il controllo spostarsi nella cartella dove si trova il database di produzione digitando il comando:</p> Text Only<pre><code>    cd /var/lib/mysql/sugar\n</code></pre> <p>seguito poi da:</p> Text Only<pre><code>    du -hs\n</code></pre> <p>come riferimento del cliente il database ha una dimensione media di circa 30GB , quando questo cresce fino ad arrivare a circa 37GB , bisogna effettuare un operazione di manutenzione e ricompattazione database , da pianificare con il cliente , richiede la messa offline del sito per circa 2 ore.</p> <p>La procedura di manutenzione e ricompattazione prevede l\u2019export del database tramite comando mysqldump, la cancellazione dello stesso all\u2019interno del database e l\u2019importazione dell\u2019export precedentemente effettuato.</p>"},{"location":"customers/sugar-C2070/#accesso","title":"Accesso","text":"<p>Via Teleport:</p> <ul> <li>https://cc-ds1.criticalcasecloud.com</li> <li>https://cc-ds2.criticalcasecloud.com</li> </ul>"},{"location":"customers/sugar-C2070/operations/","title":"Sugar Operations","text":""},{"location":"customers/sugar-C2070/operations/#blank-page-on-websites","title":"Blank page on websites","text":"<p>Nel caso di blocchi da parte del php si pu\u00f2 verificare la presenza di processi digitando il comando:</p> Bash<pre><code>$ ps -ef | grep php7\nwww-data  4147     1  9 16:26 ?        00:00:01 /usr/bin/php7.2 /var/www/sugar/bin/magento cron:run --group=index --bootstrap=standaloneProcessStarted=1\n</code></pre> <p>una volta eseguito si deve controllare se i processi sono attivi da molto tempo (vedere data) e se del caso spegnerli con il comando:</p> Bash<pre><code># L'ID del processo \u00e8 il primo numero della riga dopo l\u2019utenza www-data.\n# in questo caso \u00e8 4147\nkill -9 4147\n</code></pre>"},{"location":"customers/sugar-C2070/operations/#reindex","title":"Reindex","text":"<p>Premessa: tutte le operazioni della procedura vanno effettuate sulla macchina frontend di sugar (FR01).</p> <p>Posizionarsi sulla cartella di pubblicazione web con il comando:</p> Bash<pre><code>cd /var/www/sugar\n</code></pre> <p>da li per visualizzare lo stato degli indici digitare il comando:</p> Bash<pre><code>php bin/magento indexer:status\n</code></pre> <p>dovrebbe comparire una tabella come quella in foto sotto.</p> <p></p> <p>Nel caso specifico la tabella con titolo Product Categories \u00e8 in uno stato anomalo rispetto alle altre, ha un numero di backlog alto e la data di aggiornamento (Schedule Updated) \u00e8 pi\u00f9 vecchia rispetto alle altre tabelle. Per reindicizzare la tabella bisogna prima resettare l\u2019indice con il comando (da lanciare sempre sotto /var/www/sugar):</p> Bash<pre><code>php bin/magento indexer:reset &lt;nome tabella&gt;\n</code></pre> <p>dove  corrisponde alla lista seguente: <p>-Nome Tabella- -Titolo Tabella- design_config_grid Design Config Grid customer_grid Customer Grid catalog_category_product Category Products catalog_product_category Product Categories catalogrule_rule Catalog Rule Product catalog_product_attribute Product EAV catalog_product_price Product Price catalogrule_product Catalog Product Rule cataloginventory_stock Stock catalogsearch_fulltext Catalog Search</p> <p>quindi per resettare la tabella con Titolo Product Categories si deve lanciare il comando:</p> Bash<pre><code>php bin/magento indexer:reset  catalog_product_category\n</code></pre> <p>se l\u2019operazione ha avuto successo rilanciando il comando:</p> Bash<pre><code>php bin/magento indexer:status\n</code></pre> <p>sotto la colonna Status della tabella compare la scritta Reindex required, diversamente si deve rilanciare il comando di reset , pu\u00f2 capitare se subito dopo il reset la tabella viene bloccata di nuovo.</p> <p>A questo punto si pu\u00f2 lanciare il comando di reindicizzazione seguente:</p> Bash<pre><code>php bin/magento indexer:reindex  &lt;nome tabella&gt;  2&gt;&amp;1  &amp;\n</code></pre> <p>questo comando permette di avviare il processo di reindicizzazione sganciandolo dal terminale e mostrando il numero del processo assegnato alla reindicizzazione, tenere nota del numero di processo e digitare il comando:</p> Bash<pre><code>top -d1 -p&lt;numero processo&gt;\n</code></pre> <p>cosi da monitorare il processo attivo , una volta terminata la re indicizzazione il processo scompare dalla lista.</p> <p>Per verificare la corretta esecuzione basta ridigitare lo status delle tabelle e verificare che la tabella reindicizzata abbia numeri di backlog molto pi\u00f9 bassi di prima.</p> <p>NOTA: potrebbe essere necessario effettuare una pulizia della cache del sito per una corretta visualizzazione dei prodotti mancanti, per pulire la cache digitare il comando (sempre da /var/www/sugar):</p> Bash<pre><code>php bin/magento cache:clean\n</code></pre> <p>seguito da :</p> Bash<pre><code>php bin/magento cache:flush\n</code></pre>"},{"location":"customers/sugar-C2070/operations/#ripristino-di-emergenza-mancanza-di-alcuni-prodotti-es-ticket-ito-95","title":"Ripristino di emergenza (mancanza di alcuni prodotti es. ticket ITO-95)","text":"<p>Questa procedura \u00e8 indicata quando dal sito mancano alcuni (o molti) prodotti, segno di un reindex non funzionante. E' stato schedulato infatti un cron ogni 15 minuti che si occupa di dumpare la tabella degli indici del negozio 1 ovvero quello italiano ed usato. Questo permette di restorare velocemente la tabella senza un reindex completo o un restore dell'intero db che richiederebbe ore, risolvendo quindi velocemente il problema.</p> <p>Premessa: tutte le operazioni della procedura vanno effettuate sulla macchina frontend di sugar (sugar-fr01.local.ccws.it).</p> <p>ATTENZIONE, prima di effettuare qualsiasi restore assicurarsi che il db sugar-db01.local.ccws.it non sia sotto carico (cpu &gt; 600% ~ 700%) altrimenti attendere o mettere off line il sito.</p> <p>Posizionarsi sulla cartella dump_tables con il comando:</p> Bash<pre><code>cd /var/www/sugar/tools/dump_tables\n</code></pre> <p>si vedranno una serie di file gz che sono i dump ogni 15 minuti:</p> Bash<pre><code>-rw-r--r-- 1 root     root      651646 Jun  4 08:45 2021-06-04-08-45-01-catalog_category_product_index_store1.sql.gz\n-rw-r--r-- 1 root     root      651646 Jun  4 09:00 2021-06-04-09-00-01-catalog_category_product_index_store1.sql.gz\n-rw-r--r-- 1 root     root      651682 Jun  4 09:15 2021-06-04-09-15-01-catalog_category_product_index_store1.sql.gz\n-rw-r--r-- 1 root     root      325780 Jun  4 09:30 2021-06-04-09-30-01-catalog_category_product_index_store1.sql.gz\n-rw-r--r-- 1 root     root      367623 Jun  4 09:45 2021-06-04-09-45-01-catalog_category_product_index_store1.sql.gz\n-rw-r--r-- 1 root     root      367846 Jun  4 10:00 2021-06-04-10-00-01-catalog_category_product_index_store1.sql.gz\n-rw-r--r-- 1 root     root      367235 Jun  4 10:15 2021-06-04-10-15-01-catalog_category_product_index_store1.sql.gz\n-rw-r--r-- 1 root     root      651635 Jun  4 10:30 2021-06-04-10-30-02-catalog_category_product_index_store1.sql.gz\n-rw-r--r-- 1 root     root      651635 Jun  4 10:45 2021-06-04-10-45-01-catalog_category_product_index_store1.sql.gz\n</code></pre> <p>come si pu\u00f2 notare dalle dimensioni del dump fino alle ore 9 \u00e8 andato bene, dopo \u00e8 cresciuto per poi di colpo dimezzarsi. Questo sicuramente indica una rottura degli indici e quindi spiega la sparizione dei prodotti dal sito (non \u00e8 ancora nota la causa). Quindi prenderemo il dump delle 9 per essere sicuri che la tabella sia corretta, quello delle 9:15 \u00e8 gi\u00e0 cambiato troppo di dimensioni.</p> <p>Copiamo quindi il gz corretto (in questo caso delle 9) sotto /var/www/sugar/tools/emergency-rescue, scompattiamo e rinominiamo</p> Bash<pre><code>cp 2021-06-04-09-00-01-catalog_category_product_index_store1.sql.gz /var/www/sugar/tools/emergency-rescue/\ncd ../emergency-rescue/\ngunzip 2021-06-04-09-00-01-catalog_category_product_index_store1.sql.gz\nmv 2021-06-04-09-00-01-catalog_category_product_index_store1.sql catalog_category_product_index_store1.sql\n</code></pre> <p>In questa cartella esiste un index.php, \u00e8 uno script che si occupa del restore corretto della tabella scritto dallo sviluppatore, basta far puntare il browser a https://www.sugar.it/tools/emergency-rescue/index.php per lanciarlo.</p> <p>A questo punto baster\u00e0 attendere un nuovo giro di reindex entro i 15 minuti e il sito dovrebbe tornare alla normalit\u00e0.</p> <p>Se lo script non funzionasse si pu\u00f2 usare:</p> Bash<pre><code>mysql sugar &lt; catalog_category_product_index_store1.sql\n</code></pre> <p>e attendere i 15 minuti del giro di reindex.</p>"},{"location":"customers/sugar-C2070/operations/#parametri-database","title":"Parametri Database","text":"<p>Per verificare le performance sono stati settati dei parametri volatili che in caso di riavvio del servizio o della macchina db vanno reimpostati:</p> <p>Posizionarsi sulla macchina db (sugar-db01.local.ccws.it) e usando la mysql cli, settare:</p> Bash<pre><code>SET tx_isolation = 'READ-COMMITTED'\nSET GLOBAL TRANSACTION ISOLATION LEVEL READ COMMITTED\n</code></pre>"},{"location":"customers/sugar-C2070/operations/#home-page-con-errore-404","title":"Home page con errore 404","text":"<p>Esiste un bug noto (https://github.com/magento/magento2/issues/21299) in Magento 2.3.1 e che affigge questo sito. In caso di particolare richieste HEAD, Magento risponde con un 404 e mette in cache la risposta. Lo sviluppatore ci sta lavorando ma nel mentre \u00e8 necessario pulire la cache in questo modo da utente root sulla macchina sugar-fr01.local.ccws.it:</p> Bash<pre><code>/usr/local/sbin/check_http400.sh\n</code></pre>"},{"location":"customers/tecne-C2243/","title":"Tecne srls @ C2243","text":""},{"location":"customers/tecne-C2243/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>~~CR210547: VM AWS DB Oracle Cliente 4Timing. Gestione di un'istanza RDS Oracle su Account AWS del Cliente.~~ Account cancellato.   ~~</li> </ul>"},{"location":"customers/tecne-C2243/#note-h24","title":"Note H24","text":"<ul> <li>Controllare Alerta per allarmi derivanti da Cloudwatch Credenziali</li> </ul>"},{"location":"customers/tecne-C2243/#tecne","title":"Tecne","text":"<p>Internal data:</p> <ul> <li>Start date: 17/06/2021</li> <li>Referente Commerciale: Marina Micelli</li> <li>Telefono referente: +39 337 155 4741</li> <li>Email: m.miceli@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Gianluca Enrietti</li> <li>Email: gianluca.enrietti@tecne.org</li> <li>Phone: +39 011 198 2308</li> </ul>"},{"location":"customers/tecne-C2243/#data-center","title":"Data Center","text":"<ul> <li> AWS</li> </ul>"},{"location":"customers/tecne-C2243/#descrizione-ambito-funzionale-e-tecnologico","title":"Descrizione ambito funzionale e tecnologico","text":"<p>E' presente una sola istanza RDS a Milano. Il nome \u00e8 tecne.</p>"},{"location":"customers/tecne-C2243/#accessi","title":"Accessi","text":"<p>Accessi su vault per il DB: https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/show/ds/customers/tecne-C2243/oracle</p>"},{"location":"customers/tecne-C2243/#aws","title":"Aws","text":"Text Only<pre><code>Account: 706175646605\nAlias: tecne\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/timeware-C2375/","title":"Timeware c2375","text":"<p>Customer's SPOC:</p> <ul> <li>Full name:</li> <li>Email:</li> <li>Job: sistemi informativi</li> </ul> <ul> <li>Full name:</li> <li> </li> <li>Job:</li> </ul>"},{"location":"customers/timeware-C2375/#email","title":"Email:","text":""},{"location":"customers/timeware-C2375/#aws","title":"AWS","text":"Text Only<pre><code>Account: 011611894214\nRoot:\nAlias:\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/timeware-C2375/#teleport-how-to-access-to-virtual-machines","title":"Teleport (How to access to Virtual Machines)","text":"<p>The VMs are accessible via one of the following links (federated teleport cluster):</p> <p>https://cc-ds1.criticalcasecloud.com/ https://cc-ds2.criticalcasecloud.com/</p> <p>When, you logged in, slecet the cluster in below from the \"CLUSTERS\" section:</p> <p>tp.timeware.criticalcasecloud.com</p>"},{"location":"customers/timeware-C2375/#architettura","title":"Architettura","text":"<p>Il DNS \u00e8 gestito dal cliente Timeware su un altro provider. Il cliente Timeware ha accesso ai servers tramite teleport. I backup sono automatizzati da AWS Backup. Le credenziali sono sotto Sherlock. Essendo in ITO, l'account \u00e8 controllato sia dalle Lambda in cc-digital. Le risorse sono sotto monitoring tramite Cloudwatch Alarms e Zabbix.</p>"},{"location":"customers/timeware-C2375/#network-stg","title":"Network stg","text":"<p>Abbiamo una VPC principalee il peering sulla VPC in cui \u00e8 presente MongoDB Atlas. Ci sono 2 subnets private e 2 subnets pubbliche. Le subnet pubbliche utilizzano tutte lo stesso Internet Gateway e le private uno per az Nat Gateway. Non ci sono limiti di raggiungibilit\u00e0 sulle NACL delle subnets, ma \u00e8 gestito tutto dai Security Groups.</p>"},{"location":"customers/timeware-C2375/#network-prd","title":"Network prd","text":"<p>La VPC principale \u00e8 sempre la stessa, pi\u00f9 abbiamo il peering sulla VPC in cui \u00e8 presente MongoDB Atlas (esattamente come in stg). Ci sono 2 subnets private e 2 subnets pubbliche. Le subnet pubbliche utilizzano tutte lo stesso Internet Gateway e le private uno per az Nat Gateway. Non ci sono limiti di raggiungibilit\u00e0 sulle NACL delle subnets, ma \u00e8 gestito tutto dai Security Groups.</p>"},{"location":"customers/timeware-C2375/#staging","title":"Staging","text":"<p>The project employs AWS CodePipeline as a continuous delivery solution, comprising two integral stages. In the source stage, code updates are initiated by the customer pushing changes to the designated GitHub branch (<code>aws_stag</code>), thereby triggering the pipeline. Subsequently, in the deploy stage, AWS CodeDeploy orchestrates the deployment process using the \"appspec.yaml\" file. This file dictates the copying of all relevent files from the root directory to \"/var/www/html/twApi/\" on the target instances. Notably, ownership and group settings are configured to \"www-data\" to ensure proper access controls. Additionally, the deployment includes the execution of a Bash script from the \"scripts\" directory, invoked via the hooks section, post the installation of the new application version. This script plays a pivotal role in storing the correct environment variables within the application.</p> <p>In the event of deployment failures, CodeDeploy is configured with rollback settings to revert to the last known good version, ensuring the stability of the application. The Auto Scaling Group (ASG) utilizes an Amazon Machine Image (AMI) equipped with CloudWatch for monitoring purposes. Nginx serves as the designated web server, while secure access is facilitated through Teleport. Server instances are strategically created in private subnets spanning diverse availability zones to enhance resilience.</p> <p>The Application Load Balancer (ALB) is adeptly configured to distribute network load among instances, periodically verifying their responsiveness through health checks. Furthermore, the ALB intelligently distributes network load across availability zones, thereby strengthening the application's availability and fault tolerance. For reinforcing security, AWS WAF is seamlessly integrated with the ALB, wielding predefined rules to scrutinize and filter incoming web requests prior to reaching the web applications. This setup encapsulates automated deployment processes, robust monitoring capabilities, secure access controls, efficient load balancing, and heightened security measures, collectively fortifying the overall reliability and resilience of the application.</p> <p></p>"},{"location":"customers/timeware-C2375/#production","title":"Production","text":"<p>The project leverages AWS CodePipeline for seamless continuous delivery, organized into two primary stages. In the source stage, updates are initiated by the customer pushing changes to the designated GitHub branch (<code>aws_prod</code>), which in turn triggers the pipeline. Following this, in the deploy stage, AWS CodeDeploy manages the deployment process using the \"appspec.yaml\" file. This file dictates the transfer of relevant files from the root directory to \"/var/www/html/twApi/\" on the target instances. It's worth noting that ownership and group settings are configured to \"www-data\" to ensure proper access controls.</p> <p>Additionally, the deployment involves executing a Bash script located in the \"scripts\" directory, invoked via the hooks section post the installation of the new application version. This script retrieves a customer-specific environment configuration file containing essential environment variables for the application's operation. Subsequently, it copies this file to a general .env file, enabling the application to access these environment variables seamlessly.</p> <p>To ensure application stability, CodeDeploy is equipped with rollback settings to revert to the last known good version in case of deployment failures. Different Auto Scaling Groups (ASGs) utilize the same Amazon Machine Image (AMI), equipped with CloudWatch agent, Nginx as the web server, and Teleport for secure access. However, a distinct key pair is created for each customer. Instances spawned by each ASG are distributed across diverse availability zones, enhancing resilience.</p> <p>Moreover, depending on customer requirements, scheduled scaling is employed to automate the scaling of the number of instances during lunch and dinner times, anticipating an increase in load during these periods.</p> <p>The usage of three CloudFront distributions facilitates customized configurations and segregates caching behavior for each customer. All CloudFront distributions are directed to a single Application Load Balancer (ALB), responsible for routing requests to the appropriate ASG based on the custom header included in the requests. Each CloudFront distribution is associated with a Web ACL, ensuring requests undergo filtering based on defined rules, adding an additional layer of security. Additionally, a specific CloudFront distribution is established to optimize the delivery of images. This particular distribution is configured to use Amazon S3 as its origin.</p> <p></p>"},{"location":"customers/timeware-C2375/#aws-ai","title":"AWS AI","text":""},{"location":"customers/travelmatic-C0677/","title":"Travelmatic srl @ C0677","text":""},{"location":"customers/travelmatic-C0677/#ito-h24","title":"ITO H24","text":"<ul> <li>CR230430: Account AWS 686952707636 (travelmatic.net, simplecrs.it)</li> </ul>"},{"location":"customers/travelmatic-C0677/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR210766: Progetto di Consulenza AWS</li> </ul> <p>Sales: Marina Micelli</p>"},{"location":"customers/travelmatic-C0677/#aws","title":"AWS","text":"Text Only<pre><code>Account: 686952707636\nRoot: master@contur.it\nAlias: contur\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/travelmatic-C0677/#additional-aws-accounts-unmanaged","title":"Additional AWS Accounts (unmanaged)","text":"Text Only<pre><code>Account: 149883778021\nRoot: m.cristofaro@travelmatic.com\n</code></pre> Text Only<pre><code>Account: 212799012878\nRoot: m.fiorentino@travelmatic.com\n</code></pre>"},{"location":"customers/travelmatic-C0677/#teleport-how-to-access-to-virtual-machines","title":"Teleport (How to access to Virtual Machines)","text":"<p>The VMs are accessible via one of the following links (federated teleport cluster):</p> <p>https://cc-ds1.criticalcasecloud.com/ https://cc-ds2.criticalcasecloud.com/</p> <p>When, you logged in, slecet the cluster in below from the \"CLUSTERS\" section:</p> <p>tp.travelmatic.criticalcasecloud.com</p>"},{"location":"customers/travelmatic-C0677/#architettura","title":"Architettura","text":""},{"location":"customers/websolute-C2325/","title":"WEBSOLUTE Spa @ C2325","text":""},{"location":"customers/websolute-C2325/#ito-h24","title":"ITO H24","text":"<ul> <li>CR230172: Implementazione sito Teamsystem su AWS teamsystem.com</li> </ul>"},{"location":"customers/websolute-C2325/#ito-lite","title":"ITO LITE","text":"<ul> <li>CR220382: Implementazione Magento su AWS selleroyal.com</li> </ul>"},{"location":"customers/websolute-C2325/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR240184: Test di Carico sito Teamsystem su AWS</li> </ul>"},{"location":"customers/websolute-C2325/#note-h24","title":"Note H24","text":"<p>Se il cliente chiama bisogna comunicare che non \u00e8 attivo il servizio ITO, devono aprire un ticket in orario lavorativo per il CR220382.</p> <p>Customer's SPOC:</p> <ul> <li>Full name: Simone Palazzetti</li> <li>Email: spalazzetti@websolute.it</li> <li>Job: sistemi informativi</li> </ul> <ul> <li>Full name: Mirko Soffia</li> <li>Email: msoffia@websolute.it</li> <li>+39 348 4907472</li> <li>Job: PM del progetto e il commerciale che l'ha venduto all'inizio. Profonda conoscenza del cliente e contesto cliente</li> </ul>"},{"location":"customers/websolute-C2325/SELLEROYALE/","title":"SELLEROYALE","text":"<p>Website selleroyal.com - CR220382</p>"},{"location":"customers/websolute-C2325/SELLEROYALE/#data-center","title":"Data center","text":"<ul> <li> AWS 564037702494</li> </ul> Text Only<pre><code>Account: 564037702494\nAlias: selleroyal-web\n</code></pre>"},{"location":"customers/websolute-C2325/SELLEROYALE/#teleport","title":"Teleport","text":"<p>Access to the VMs is possible via federated teleport cluster:</p> <p>Select the cluster:</p> <ul> <li><code>tp.websolute-selleroyal.criticalcasecloud.com</code></li> </ul>"},{"location":"customers/websolute-C2325/SELLEROYALE/#architettura","title":"Architettura","text":"<p>Documenti consegnati:</p> <ul> <li>websolute_lld_architecture_eng.pdf</li> <li>websolute_lld_architecture_eng.docx</li> </ul>"},{"location":"customers/websolute-C2325/SELLEROYALE/#note","title":"Note","text":"<ul> <li>Il DNS \u00e8 gestito dal cliente</li> <li>I backup sono automatizzati da AWS Backup</li> <li>Le credenziali sono sotto Sherlock</li> <li>Il progetto \u00e8 deployato tramite Terraform Cloud Prd e Stg. Il codice \u00e8 su GitHub</li> <li>Essendo in ITO, l'account \u00e8 controllato sia dalle Lambda in cc-digital</li> <li>Le risorse sono sotto monitoring tramite Cloudwatch Alarms e Zabbix</li> <li>Controllare Alerta per allarmi derivanti da Cloudwatch Credenziali</li> <li>L'account sotto ITO \u00e8 monitorato da Cloudtrail che invia i logs ad S3, Logstash On-Premise recupera questi dati e li mostra a Kibana</li> </ul>"},{"location":"customers/websolute-C2325/SELLEROYALE/#network","title":"Network","text":"<p>Esiste un unica VPC. Ci sono 5 subnets private e 4 subnets pubbliche per environment (9 subnets totali con 254 IPs address ciascuno). Le subnet pubbliche utilizzano tutte lo stesso Internet Gateway e le private lo stesso Nat Gateway. Non ci sono limiti di raggiungibilit\u00e0 sulle NACL delle subnets, ma \u00e8 gestito tutto dai Security Groups</p>"},{"location":"customers/websolute-C2325/SELLEROYALE/#stg","title":"Stg","text":"<p>L'ambiente di stage \u00e8 speculare a quello di produzione, ma singola az. Tutto quello che era possibile criptare on at Rest \u00e8 stato fatto con KMS o chiavi di default di AWS, mantere questa convenzione. Esiste un Cloudfront che fa da cache per i contenuti statici. Esiste un Custom Header per far si che il bilanciatore accetti il traffico solamente dal Cloudfront e non direttamente (il traffico tra di essi \u00e8 cifrato). Al Cloufront \u00e8 associato un WAF al quale sono associate regole di rate-limit, ip permessi e ip malevoli (aggiornati ogni ora dalle nostre lambda in cc-digital). Al Waf \u00e8 associato un Kinesis che ha il compito di inviare i logs al Bucket S3. On-Premise Logstash recupera questi logs e li invia al nostro Kibana(Ricordarsi di selezionare l'host corretto). Il database \u00e8 su RDS. Esiste un Redis(non c'\u00e8 https, ma la sicurezza \u00e8 garantita a livello di networking concordato con il cliente) e un ElasticSearch, quest'ultimo \u00e8 obbligatorio per l'ultima versione di Magento. Il DNS interno \u00e8 gestito da Route53. I servers on demand magento (al suo interno \u00e8 presente anche l'admin) sono creati da un autoscaling impostato ad 1 server come capacit\u00e0. L'EFS \u00e8 montato dal file /etc/fstab:</p> Text Only<pre><code>LABEL=cloudimg-rootfs   /        ext4   defaults,discard        0 1\nLABEL=UEFI      /boot/efi       vfat    umask=0077      0 1\nfs-06c459ea8403852cf.efs.eu-central-1.amazonaws.com:/ /mnt/b2c-nfs efs _netdev,noresvport,tls 0 0\n</code></pre> <p>Sulla macchine \u00e8 installato Varnish in docker, porta 8080. Gestito da ansible /opt/docker-compose.yml:</p> Text Only<pre><code># Ansible managed\n\nversion: '2.1'\nservices:\n  varnish:\n    container_name: varnish\n    restart: unless-stopped\n    image: varnish:6.5.1\n    environment:\n      VARNISH_SIZE: \"512M\"\n    extra_hosts:\n      web: \"172.17.0.1\"\n    volumes:\n      - /opt/varnish/etc/default.vcl:/etc/varnish/default.vcl:ro\n    tmpfs:\n      - /var/lib/varnish:exec\n    ports:\n      - 8080:80\n</code></pre> <p>Ngnix viene eseguito sulla porta 80. Sono presenti dei redirects nel file /etc/nginx/nginx.conf richiesti dal cliente e gestiti con Ansible, vedere ticket: https://criticalcase.atlassian.net/servicedesk/customer/portal/2/ITO-452</p> <p>Per risparmiare, l'EC2 e RDS di staging vengono spenti negli orari non lavorativo, rimanendo spenti durante il fine settimana. Per RDS \u00e8 stato usato Rundeck e per l'EC2 invece \u00e8 stato usato un scheduler sul'EC2 auto scaling (Automatic scaling). I servizi vengono spento \u200b\u200balle ore 19:00 e riattivati \u200b\u200balle ore 07:00.</p>"},{"location":"customers/websolute-C2325/SELLEROYALE/#prd","title":"Prd","text":"<p>L'ambiente di produzione \u00e8 speculare a quello di stage, ma multi az. Tutto quello che era possibile criptare on at Rest \u00e8 stato fatto con KMS o chiavi di default di AWS, mantere questa convenzione. Esiste un Cloudfront che fa da cache per i contenuti statici. Esiste un Custom Header per far si che il bilanciatore accetti il traffico solamente dal Cloudfront e non direttamente (il traffico tra di essi \u00e8 cifrato). Al Cloufront \u00e8 associato un WAF al quale sono associate regole di rate-limit, ip permessi e ip malevoli (aggiornati ogni ora dalle nostre lambda in cc-digital). Al Waf \u00e8 associato un Kinesis che ha il compito di inviare i logs al Bucket S3. On-Premise Logstash recupera questi logs e li invia al nostro Kibana(Ricordarsi di selezionare l'host corretto). Il bilanciatore ha una rules per il path /admin_bwe590d/ che inoltra le richieste al server admin I servers on demand magento sono creati da un autoscaling impostato ad 2 servers come capacit\u00e0 minima e 6 come massima. Esiste una scaling policy che incrementa il numero di servers se la CPU supera l'80%. I servers che verranno creati sono tutti spot instances su due availability zones diverse Il server Admin \u00e8 fuori dall'autoscaling, ma viene aggiornato dalla pipeline come gli altri servers Il database \u00e8 su RDS. Esiste un Redis(non c'\u00e8 https, ma la sicurezza \u00e8 garantita a livello di networking concordato con il cliente) e un ElasticSearch, quest'ultimo \u00e8 obbligatorio per l'ultima versione di Magento. Il DNS interno \u00e8 gestito da Route53. L'EFS \u00e8 montato dal file /etc/fstab:</p> Text Only<pre><code>LABEL=cloudimg-rootfs   /        ext4   defaults,discard        0 1\nLABEL=UEFI      /boot/efi       vfat    umask=0077      0 1\nfs-001cf65a0cda11c62.efs.eu-central-1.amazonaws.com:/ /mnt/b2c-nfs efs _netdev,noresvport,tls 0 0\n</code></pre> <p>Sulla macchine \u00e8 installato Varnish in docker, porta 8080. Gestito da ansible /opt/docker-compose.yml:</p> Text Only<pre><code># Ansible managed\n\nversion: '2.1'\nservices:\n  varnish:\n    container_name: varnish\n    restart: unless-stopped\n    image: varnish:6.5.1\n    environment:\n      VARNISH_SIZE: \"512M\"\n    extra_hosts:\n      web: \"172.17.0.1\"\n    volumes:\n      - /opt/varnish/etc/default.vcl:/etc/varnish/default.vcl:ro\n    tmpfs:\n      - /var/lib/varnish:exec\n    ports:\n      - 8080:80\n</code></pre> <p>Ngnix viene eseguito sulla porta 80. Sono presenti dei redirects nel file /etc/nginx/nginx.conf richiesti dal cliente e gestiti con Ansible, vedere ticket: https://criticalcase.atlassian.net/servicedesk/customer/portal/2/ITO-452</p>"},{"location":"customers/websolute-C2325/SELLEROYALE/#codepipeline","title":"CodePipeline","text":"<p>E' stata ideata una pipeline per il rilascio automatico del codice sulle macchine. Le pipeline sono di stg e prd Il cliente mantiene il codice in un Repos su Azure gestito da WebSolute. Quando avviene un push sul branch di staging o master (nel caso di produzione), una pipeline lato Azure ha il compito di caricare il codice verso CodeCommit. La CodePipeline viene avviata e CodeBuild ha il compito di buildare il codice. CodeBuild si basa su una immagine Docker memorizzata in ECR, Dockerfile. Il file buildspec.yaml mostra i passaggi di CodeBuild. Alcune informazioni sensibili come credenziali sono memorizzate nei Parameter Store; i nomi di questi parametri vengono passati come environments. Prima di eseguire l'upgrade di magento, viene lanciato un comando sui server attraverso un Run Command che mette in maintenance i servers (parte presente nel buildspec.yml):</p> Text Only<pre><code>- aws ssm send-command --document-name \"AWS-RunShellScript\" --document-version \"1\" --targets '[{\"Key\":\"tag:'\"$ssm_send_command_tag\"'\",\"Values\":[\"'\"${ssm_send_command_value}\"'\"]}]' --parameters '{\"commands\":[\"runuser -l selleroyal -c '\"'\"'/var/www/b2c/bin/magento maintenance:enable'\"'\"'\"],\"workingDirectory\":[\"\"],\"executionTimeout\":[\"3600\"]}' --timeout-seconds 600 --max-concurrency \"50\" --max-errors \"0\" --region $region\n</code></pre> <p>Codebuild viene eseguito in VPC per poter comunicare e applicare le modifiche alle altre risorse quali RDS,Elasticsearch, ecc. Una volta terminata la build, tutti i files generati vengono caricati su S3. Sar\u00e0 compito di CodeDeploy di caricare questi files sui servers. Il file appspec.yml descrive come viene effettuato questo deploy. Si copia in una cartella temporanea il codice /var/www/codepipeline_deploy/ con i permessi dell'utente corretto. Nello Scripts/post_deploy.sh viene creata una cartella nuova e spostati i files del nuovo deploy al suo interno e disabilitata al maintenance. Nello Scripts/validate_service.sh si occuper\u00e0 di validare il servizio con una curl interna verso Varnish</p>"},{"location":"customers/websolute-C2325/SELLEROYALE/#repository-git-su-azuredevops","title":"Repository git su AzureDevops","text":"<p>\"Connections to SSH servers using SHA-1 was disabled by default in the OpenSSH client\", si \u00e8 reso necessario modificare la configurazione del file /etc/ssh/ssh_config aggiungendo la sequente riga:</p> Text Only<pre><code>KexAlgorithms +diffie-hellman-group1-sha1,diffie-hellman-group14-sha1\n</code></pre> <p>Link documentazione: https://devblogs.microsoft.com/devops/supporting-sha-2-algorithm-in-ssh-on-azure-devops/</p>"},{"location":"customers/websolute-C2325/Teamsystem/","title":"TEAMSYSTEM","text":"<p>Website teamsystem.com - CR230172</p>"},{"location":"customers/websolute-C2325/Teamsystem/#data-center","title":"Data center","text":"<ul> <li> AWS teamsystem-test 918621053547</li> <li> AWS teamsystem-prod 844065583897</li> </ul> Text Only<pre><code>Account: 918621053547\nAlias: ts-teamsystem-site-hub-test\n</code></pre> Text Only<pre><code>Account: 844065583897\nAlias: ts-teamsystem-site-hub-prod\n</code></pre>"},{"location":"customers/websolute-C2325/Teamsystem/#teleport","title":"Teleport","text":"<p>Access to the VMs is possible via federated teleport cluster:</p> <p>Select the cluster:</p> <ul> <li><code>tp.websolute-teamsystem.criticalcasecloud.com</code></li> </ul>"},{"location":"customers/websolute-C2325/Teamsystem/#configurazioni","title":"Configurazioni","text":""},{"location":"customers/websolute-C2325/Teamsystem/#utenze","title":"Utenze","text":"<ul> <li>Windows: bisogna creare un ruolo dedicato specificando l'utenza Windows con cui pu\u00f2 accedere.</li> <li>Linux: il cliente accede tramite l'utenza websolute/www-data attraverso il ruolo websolute-hub</li> </ul>"},{"location":"customers/websolute-C2325/Teamsystem/#architettura","title":"Architettura","text":"<p>Documentazione Documentazione-Accesso locale a MySQL e SQL Server</p>"},{"location":"customers/websolute-C2325/Teamsystem/#note","title":"Note","text":"<ul> <li>Il DNS \u00e8 gestito dal cliente TeamSystem su Akamai</li> <li>Il cliente WebSolute ha accesso amministrativo ai servers, ha installato lui la configurazione</li> <li>I backup sono automatizzati da AWS Backup</li> <li>Le credenziali sono sotto Sherlock</li> <li>Essendo in ITO, l'account \u00e8 controllato sia dalle Lambda in cc-digital</li> <li>Le risorse sono sotto monitoring tramite Cloudwatch Alarms e Zabbix</li> </ul>"},{"location":"customers/websolute-C2325/Teamsystem/#network-test","title":"Network test","text":"<p>Esiste un unica VPC. Ci sono 2 subnets private e 2 subnets pubbliche. Le subnet pubbliche utilizzano tutte lo stesso Internet Gateway e le private lo stesso Nat Gateway. Non ci sono limiti di raggiungibilit\u00e0 sulle NACL delle subnets, ma \u00e8 gestito tutto dai Security Groups</p>"},{"location":"customers/websolute-C2325/Teamsystem/#network-production","title":"Network production","text":"<p>La VPC di produzione. Ci sono 2 subnets private e 2 subnets pubbliche. Le subnet pubbliche utilizzano tutte lo stesso Internet Gateway e le private uno per az Nat Gateway. Non ci sono limiti di raggiungibilit\u00e0 sulle NACL delle subnets, ma \u00e8 gestito tutto dai Security Groups</p> <p>E' presente anche la vpc del Bastion Host con le sue subnets pubbliche e Internet Gateway. Al suo interno \u00e8 stato creato il server Bastion Host, le VPC di test e produzione si parlano attraverso il VPC Peering</p>"},{"location":"customers/websolute-C2325/Teamsystem/#test","title":"Test","text":"<p>Esiste un Cloudfront che si occupa di mettere in cache i contenuti statici. Sono presenti principalmente 3 Origins, il Load Balancer, il bucket S3 con i loro assets caricati da questa utenza e il bucket s3 per la maintenance nel caso di errori 5XX viene mostrata dalla CDN automaticamente. Esiste un Custom Header per far si che il bilanciatore accetti il traffico solamente dal Cloudfront e non direttamente (il traffico tra di essi \u00e8 cifrato). Al Cloudfront \u00e8 associato un WAF il quale accetta solo gli ip provenienti da Radware. Il Bilanciatore si occupa di smistare il traffico in base al path. Per accedere al percorso \"/bom/\" bisogna prima effettuare l'accesso tramite Cognito I database sono all'interno delle EC2. Esiste un Redis utilizzato dall'applicazione Bom ovvero i servers Windows (non c'\u00e8 https, ma la sicurezza \u00e8 garantita a livello di networking concordato con il cliente). Il DNS interno \u00e8 gestito da Route53. I servers on demand, uno dedicato a Wordpress con mysql al suo interno e uno dedicato applicazione Bom su server Windows con SQL Server installato.</p>"},{"location":"customers/websolute-C2325/Teamsystem/#prd","title":"Prd","text":"<p>L'ambiente di produzione \u00e8 su multi az, tranne che per il sito wordpress. Esiste un Cloudfront che si occupa di mettere in cache i contenuti statici. Sono presenti principalmente 3 Origins, il Load Balancer, il bucket S3 con i loro assets caricati da questa utenza e il bucket s3 per la maintenance nel caso di errori 5XX viene mostrata dalla CDN automaticamente. Esiste un Custom Header per far si che il bilanciatore accetti il traffico solamente dal Cloudfront e non direttamente (il traffico tra di essi \u00e8 cifrato). Al Cloufront \u00e8 associato un WAF il quale accetta solo gli ip provenienti da Radware. Il Bilanciatore si occupa di smistare il traffico in base al path. Per il dominio teamsystem.com si occupa di effettuare un redirect a www.teamsystem.com. Per accedere al percorso \"/bom/\" bisogna prima effettuare l'accesso tramite Cognito I database sono all'interno delle EC2. Esiste un Redis utilizzato dall'applicazione Bom ovvero i servers Windows (non c'\u00e8 https, ma la sicurezza \u00e8 garantita a livello di networking concordato con il cliente). Il DNS interno \u00e8 gestito da Route53. I servers sono on demand, uno dedicato a Wordpress e due dedicati applicazione Bom. Il databases sono in PAAS su RDS</p>"},{"location":"customers/websolute-C2325/Teamsystem/#codepipeline","title":"CodePipeline","text":"<p>Il codice \u00e8 presente sui repository Azure del cliente. Quando vuole caricare i files sulle macchina, fa una push nel Azure Repos innescando una pipeline Azure che caricher\u00e0 il codice attraverso delle credenziali su CodeCommit. Viene innescata la Pipeline corrispondente.</p>"},{"location":"customers/websolute-C2325/Teamsystem/#appspecyml-della-pipeline-wordpress","title":"Appspec.yml della pipeline wordpress:","text":"Text Only<pre><code>version: 0.0\nos: linux\nfiles:\n  - source: /contrib/themes/tsmagazine-theme\n    destination: /var/www/teamsystemmagazine/contrib/themes/tsmagazine-theme\nfile_exists_behavior: OVERWRITE\npermissions:\n  - object: /var/www/teamsystemmagazine/contrib/themes/tsmagazine-theme\n    owner: www-data\n    group: www-data\n</code></pre>"},{"location":"customers/websolute-C2325/Teamsystem/#appspecyml-della-pipeline-bom","title":"Appspec.yml della pipeline bom:","text":"Text Only<pre><code>version: 0.0\nos: windows\nfiles:\n  - source: \\\n    destination: C:\\wwwroot\\TeamSystem_HUB_AWS\nhooks:\n  AfterInstall:\n    - location: scripts/after-install.ps1\n      timeout: 900\nfile_exists_behavior: OVERWRITE\n</code></pre> <p>il file scripts/after-install.ps1</p> Text Only<pre><code># Copia il file di configurazione giusto per la macchina\nif ($env:DEPLOYMENT_GROUP_NAME -eq \"bom-test\") {\n    Copy-Item -Path \"C:\\wwwroot\\TeamSystem_HUB_AWS\\Files\\Config\\Web.StagingAWS.config\" -Destination \"C:\\wwwroot\\TeamSystem_HUB_AWS\\Web.config\" -Force\n} elseif ($env:DEPLOYMENT_GROUP_NAME -eq \"bom-prod\") {\n    Copy-Item -Path \"C:\\wwwroot\\TeamSystem_HUB_AWS\\Files\\Config\\Web.Production.config\" -Destination \"C:\\wwwroot\\TeamSystem_HUB_AWS\\Web.config\" -Force\n} else {\n    # Aggiungi qui il codice che desideri eseguire se nessuna delle condizioni precedenti \u00e8 soddisfatta\n    Write-Host \"L'environment di deploy non \u00e8 n\u00e9 'bom-test' n\u00e9 'bom-prod'\"\n}\n\n# Imposta la directory di lavoro\nSet-Location -Path \"C:\\wwwroot\\TeamSystem_HUB_AWS\\client\"\n\nWrite-Host \"Esecuzione di npm install and build...\"\nnpm install ; npm run build\n\n# Comunica al sito il build degli asset nella cartella client\n# Eseguito sulla macchina di test e su SOLO UNA macchina di produzione\nif (($env:DEPLOYMENT_GROUP_NAME -eq \"bom-test\") -or ($env:computername -eq \"EC2AMAZ-SUVR3JJ\")) {\n    wget \"http://teamsystem.local/diag.aspx?RefreshMin=\"\n}\n\nWrite-Host \"\"\nWrite-Host \"Script completato.\"\n</code></pre>"},{"location":"customers/websolute-C2325/Teamsystem/#operazioni","title":"Operazioni","text":"<p>Attivit\u00e0 che possono essere richieste di routine nell'ambiente.</p>"},{"location":"customers/websolute-C2325/Teamsystem/#restore-del-backup-bak-di-sql-server","title":"Restore del backup (.bak) di SQL Server","text":"<p>In particolare durante il periodo del progetto, TeamSystem richiedeva spesso che i backup .bak da loro generati fossero ripristinati e inviati per l'esecuzione sul RDS SQL Server bom-prod di produzione.</p> <p>Questa procedura illustra il processo di esecuzione del ripristino passo per passo nella account di produzionee (AWS Account ID: 844065583897):</p> <p>1) Confermare che il ruolo teamsystem_role_s3_access_to_rds e il option group SQLSERVER_BACKUP_RESTORE siano configurati nell'istanza RDS bom-prod di produzione.</p> <p>2) Caricare il file di backup .bak nel bucket s3 restore-ts-sqlserver dell'account di produzione.</p> <p>3) Accedere al server ponte per raggiungere il database tramite il tool SQL Management Studio per eseguire i comandi. Si accede a questo server tramite Teleport, con l'utente criticalcase-admin, le cui credenziali si trovano in Sherlock-Vault. Da Teleport, si pu\u00f2 accedere facendo clic sul pulsante \"Connect\" nella risorsa \"92697329303c-static-10-1-2-59\", scegliendo l'utente ccadmin.</p> <p>Una volta entrati nel server, cercare il tool SQL Server Management Studio e accedere con le seguenti dati:</p> <ul> <li>Server type: Database Engine</li> <li>Server name: db-bom-prod.teamsystem.internal</li> <li>Authentication: SQL Server Authentication</li> <li>Login: ccadmin</li> <li>Password: prendilo da sherlock &gt; ccadmin_password</li> </ul> <p>4) Fare clic sulla directory Database, quindi fare clic con il pulsante destro del mouse su \"New query\"</p> <p>5) Prima di eseguire il ripristino del database TeamSystem_HUB, \u00e8 necessario rimuovere il database corrente, poich\u00e9 non consente di eseguire il ripristino su un database esistente. Per farlo, \u00e8 necessario eseguire il comando seguente:</p> Text Only<pre><code>EXECUTE msdb.dbo.rds_drop_database  N'TeamSystem_HUB';\n</code></pre> <p>6) Il database viene quindi ripristinato inserendo l'arn dal file di backup .bak, in questo esempio abbiamo usato \"arn:aws:s3:::restore-ts-sqlserver/Teamsystem_HUB_20231123.bak\":</p> <p>Modificare il nome del file di backup nel campo @s3_arn_to_restore_from, includendo l'arn del file caricato in S3 durante il passaggio 2).</p> Text Only<pre><code>exec msdb.dbo.rds_restore_database\n@restore_db_name='TeamSystem_HUB',\n@s3_arn_to_restore_from ='arn:aws:s3:::restore-ts-sqlserver/Teamsystem_HUB_20231123.bak';\n</code></pre> <p>7) Per seguire l'avanzamento del ripristino, utilizzare il comando seguente. Sar\u00e0 completato quando la % Complete sar\u00e0 pari al 100% e il lifecycle = SUCCESS:</p> Text Only<pre><code>exec msdb.dbo.rds_task_status;\n</code></pre> <p>8) Infine, all'utente websolute deve essere assegnato il ruolo di db_owner di questo database ripristinato. Per farlo, procedete come segue:</p> <ul> <li>Navigare nella directory Security &gt; Logins e fare clic con il pulsante destro del mouse sul login \"websolute\".</li> <li>Quindi, fare clic su Properties, poi su User Mapping e quindi sul nome del database \"TeamSystem_HUB\".</li> <li>Nell'elenco dei \"Database role membership for:TeamSystem_HUB\", selezionare il ruolo db_owner e completare l'operazione facendo clic sul pulsante OK.</li> </ul>"},{"location":"customers/websolute-C2325/Teamsystem/#backup-del-sql-server-bak","title":"Backup del SQL Server (.bak)","text":"<p>Poich\u00e9 il database Bom di produzione \u00e8 su RDS e il database di test viene eseguito su un EC2, quando viene richiesto di effettuare un backup di produzione da ripristinare sul server di test, \u00e8 necessario utilizzare il servizio di backup nativo di SQL Server invece di uno snapshot del Istanza RDS.</p> <p>Per eseguire questo backup, \u00e8 necessario seguire i seguenti passaggi nel account di produzione (AWS Account ID: 844065583897):</p> <p>1) Confermare che il ruolo teamsystem_role_s3_access_to_rds e il option group SQLSERVER_BACKUP_RESTORE siano configurati nell'istanza RDS bom-prod di produzione.</p> <p>2) Ottenere l'arn del bucket in cui verr\u00e0 salvato il backup. Se non utilizzerai il bucket restore-ts-sqlserver esistente, in questo passaggio devi creare il bucket che verr\u00e0 utilizzato.</p> <p>3) Accedere al server ponte per raggiungere il database tramite il tool SQL Management Studio per eseguire i comandi. Si accede a questo server tramite Teleport, con l'utente criticalcase-admin, le cui credenziali si trovano in Sherlock-Vault. Da Teleport, si pu\u00f2 accedere facendo clic sul pulsante \"Connect\" nella risorsa \"92697329303c-static-10-1-2-59\", scegliendo l'utente ccadmin.</p> <p>Una volta entrati nel server, cercare il tool SQL Server Management Studio e accedere con le seguenti dati:</p> <ul> <li>Server type: Database Engine</li> <li>Server name: db-bom-prod.teamsystem.internal</li> <li>Authentication: SQL Server Authentication</li> <li>Login: ccadmin</li> <li>Password: prendilo da sherlock &gt; ccadmin_password</li> </ul> <p>4) Fare clic sulla directory Database, quindi fare clic con il pulsante destro del mouse su \"New query\"</p> <p>5) Esegui il backup tramite pacchetti RDS. In questo esempio abbiamo creato il file di backup \"arn:aws:s3:::restore-ts-sqlserver/Teamsystem_HUB_20240610.bak\":</p> <p>Modificare il nome del file di backup nel campo @s3_arn_to_backup_to, includendo l'arn del bucket S3 creato/preso nel passaggio 2 + il nome del file di backup a essere creato.</p> Text Only<pre><code>exec msdb.dbo.rds_backup_database\n    @source_db_name='TeamSystem_HUB',\n    @s3_arn_to_backup_to='arn:aws:s3:::restore-ts-sqlserver/Teamsystem_HUB_20240610.bak',\n    @overwrite_s3_backup_file=0,\n    @type='FULL',\n@number_of_files=1;\n</code></pre> <p>6) Per seguire l'avanzamento del backup, utilizzare il comando seguente. Sar\u00e0 completato quando la % Complete sar\u00e0 pari al 100% e il lifecycle = SUCCESS:</p> Text Only<pre><code>exec msdb.dbo.rds_task_status;\n</code></pre>"},{"location":"customers/websolute-C2325/Teamsystem/#creare-un-nuovo-utente-cognito","title":"Creare un nuovo utente Cognito","text":"<p>La creazione di nuovi utenti in cognito viene effettuata da criticalcase seguendo i passaggi descritti di seguito:</p> <p>1) Quando viene richiesto di creare un nuovo utente, il cliente deve inviarci il nome utente e l'indirizzo e-mail dell'utente da creare.</p> <p>2) La creazione deve essere effettuata preferibilmente tramite AWS CLI, utilizzando il comando assume e accedendo all'ambiente di test o di produzione, a seconda di dove \u00e8 stata richiesta la creazione.</p> <p>Esempio:</p> Text Only<pre><code>assume teamsystem-prod\n</code></pre> <p>3) La creazione deve essere effettuata preferibilmente tramite AWS CLI, utilizzando il comando assume e accedendo all'ambiente di test o di produzione, a seconda di dove \u00e8 stata richiesta la creazione.</p> <p>Il comando di creazione dell'utente cambia tra gli ambienti test e prod perch\u00e9 il user-pool-id cambia tra gli ambienti.</p> <p>Ecco i comandi per entrambi gli ambienti:</p> <p>Modificare il {UTENTE_NAME} e {UTENTE_EMAIL} per nome utente e email utente, in successione.</p> <p>Ambiente di Test:</p> Text Only<pre><code>aws cognito-idp admin-create-user \\\n    --user-pool-id eu-central-1_VyLMsLbBf \\\n    --username {UTENTE_NAME} \\\n    --user-attributes Name=email,Value={UTENTE_EMAIL}  Name=email_verified,Value=True\n</code></pre> <p>Ambiente di Prod:</p> Text Only<pre><code>aws cognito-idp admin-create-user \\\n    --user-pool-id eu-central-1_SehBbr9NT \\\n    --username {UTENTE_NAME} \\\n    --user-attributes Name=email,Value={UTENTE_EMAIL}  Name=email_verified,Value=True\n</code></pre> <p>Ad esempio:</p> Text Only<pre><code>aws cognito-idp admin-create-user \\\n    --user-pool-id eu-central-1_VyLMsLbBf \\\n    --username l.farinazzo \\\n    --user-attributes Name=email,Value=l.farinazzo@criticalcase.com  Name=email_verified,Value=True\n</code></pre>"},{"location":"customers/websolute-C2325/Teamsystem/#inviare-una-password-di-utente-cognito","title":"Inviare una password di utente Cognito","text":"<p>De l'utente \u00e8 gi\u00e0 stato creato ma non \u00e8 stato in grado di cambiare la password nei primi giorni, la password deve essere inviata di nuovo usando il comando seguente:</p> <p>Modificare il {UTENTE_NAME} e {UTENTE_EMAIL} per nome utente e email utente, in successione.</p> <p>Ambiente di Test:</p> Text Only<pre><code>aws cognito-idp admin-create-user \\\n    --user-pool-id eu-central-1_VyLMsLbBf \\\n    --username {UTENTE_NAME} \\\n    --user-attributes Name=email,Value={UTENTE_EMAIL}  Name=email_verified,Value=True \\\n    --message-action RESEND\n</code></pre> <p>Ambiente di Prod:</p> Text Only<pre><code>aws cognito-idp admin-create-user \\\n    --user-pool-id eu-central-1_SehBbr9NT \\\n    --username {UTENTE_NAME} \\\n    --user-attributes Name=email,Value={UTENTE_EMAIL}  Name=email_verified,Value=True \\\n    --message-action RESEND\n</code></pre>"},{"location":"customers/websolute-C2325/TeamsystemTestCarico/","title":"TEAMSYSTEM TEST CARICO","text":"<p>Website teamsystem.com - CR240184</p> <p>Esecuzione di un test di carico sul TeamSystem per misurare il comportamento del sistema TeamSystem attraverso le chiamate web e identificare:</p> <ul> <li>Il numero di accessi simultanei supportati;</li> <li>Il consumo dell'infrastruttura;</li> <li>Il comportamento delle pagine chiamate,</li> <li>Identificare i limiti di capacit\u00e0 e prestazioni.</li> </ul>"},{"location":"customers/websolute-C2325/TeamsystemTestCarico/#data-center","title":"Data center","text":"<p>il test \u00e8 stato effettuato nell'ambiente di produzione, poich\u00e9 GO LIVE non era ancora stato effettuato.</p> <ul> <li> AWS teamsystem-prod 844065583897</li> </ul> Text Only<pre><code>Account: 844065583897\nAlias: ts-teamsystem-site-hub-prod\n</code></pre>"},{"location":"customers/websolute-C2325/TeamsystemTestCarico/#tools","title":"Tools","text":"<p>Il test \u00e8 stato eseguito utilizzando lo strumento grafana k6, informazioni di accesso nel Vault: K6.</p>"},{"location":"customers/websolute-C2325/TeamsystemTestCarico/#results","title":"Results","text":"<p>Report finale: 1 esecuzione -&gt; 01/03/2024</p>"},{"location":"customers/wedoo-C0050/","title":"Wedoo Srl @ C0050","text":""},{"location":"customers/wedoo-C0050/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR210799: 2022_Wedoo_P210901_WLC_StellantisUS_Consulenza</li> </ul> <p>Sales: Silvano Griot</p>"},{"location":"customers/wedoo-C0050/#haproxy","title":"HAProxy","text":"<p>Per il cliente abbiamo creato la VM wedoo-ap3-lb02.to1.ccws.it con questo file di configurazione: https://git.criticalcase.com/ops/wedoo-ap3/-/blob/master/group_vars/lbnew.yaml</p> <p>La VM non \u00e8 gestita da ITO, non \u00e8 sotto monitoraggio e le richieste di supporto vanno inoltrate a support@criticalcase.com</p> Bash<pre><code># Health checks\n\n## Old\ncurl -k \"http://192.168.20.15/vl-picker-service/operation/getImageInfo?source=alfaromeo&amp;consumer=desktop&amp;model=644N&amp;market=3931&amp;brand=83&amp;drive=2&amp;engine=42&amp;fuel=61&amp;gear=30&amp;body=289&amp;seat=690&amp;wheel=420&amp;angle=3&amp;view=EXT&amp;resolution=BIG&amp;width=1&amp;height=1&amp;raiseError=true\"\n\n## New\ncurl \"http://172.17.100.155/vl-picker-service/operation/getImageInfo?source=alfaromeo&amp;consumer=desktop&amp;model=644N&amp;market=3931&amp;brand=83&amp;drive=2&amp;engine=42&amp;fuel=61&amp;gear=30&amp;body=289&amp;seat=690&amp;wheel=420&amp;angle=3&amp;view=EXT&amp;resolution=BIG&amp;width=1&amp;height=1&amp;raiseError=true\"\n</code></pre>"},{"location":"customers/wevee-C2404/","title":"Wevee @ C2404","text":""},{"location":"customers/wevee-C2404/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR230202: progetto Quality and Lifestyle gatewayhub.wevee.com</li> </ul> <p>Internal data:</p> <ul> <li>Sales: Morena Scudieri</li> <li>Phone: +39 366 777 6512</li> <li>Email: m.scudieri@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Rahmyn Kress</li> <li>Email: r@wevee.com</li> </ul> <p>Customer's Alarms: Email: r@wevee.com, f@wevee.com and zoe.cronin@wevee.com</p>"},{"location":"customers/wevee-C2404/wevee/","title":"gatewayhub.wevee.com","text":""},{"location":"customers/wevee-C2404/wevee/#data-center","title":"Data center","text":"<ul> <li> AWS: wevee</li> </ul> Text Only<pre><code>Account Alias: wevee\nAccount ID: 392322411875\n</code></pre>"},{"location":"customers/wevee-C2404/wevee/#teleport-how-to-access-to-virtual-machines","title":"Teleport (How to access to Virtual Machines)","text":"<p>The VMs are accessible via one of the following links (federated teleport cluster):</p> <ul> <li>https://tp.wevee-navigationandlifecycle.criticalcasecloud.com/</li> </ul>"},{"location":"customers/wevee-C2404/wevee/#architecture","title":"Architecture","text":"<p>Documento consegnato:</p> <ul> <li>wevee_architecture_eng.pdf</li> <li>wevee_architecture_eng.docx</li> </ul>"},{"location":"customers/wevee-C2404/wevee/#notes","title":"Notes","text":"<ul> <li>DNS is managed by the customer</li> <li>Backups are automated by AWS Backup</li> <li>Credentials are under Sherlock</li> <li>The project is deployed via Terraform Cloud Prd and Stg. The code is on the GitHub</li> <li>Being in ITO, the account is controlled by both the Lambdas in cc-digital</li> <li>Resources are monitoring via Cloudwatch Alarms and Zabbix</li> <li>The url gatewayhub.wevee.com is monitored by the Pingdom</li> <li>Regions: Within AWS, everything is inside London region (eu-west-2)</li> </ul>"},{"location":"customers/wevee-C2404/wevee/#network","title":"Network","text":"<p>There is only one VPC. There are 4 subnets private and 4 subnets public for environment (total 8 subnets with 254 IPs addresses each). The public subnets all use the same Internet Gatewayand the private one is the same Nat Gateway. Here are no reachability limits on the NACLs of the subnets, but it's all managed by Security Groups</p>"},{"location":"customers/wevee-C2404/wevee/#staging","title":"Staging","text":"<p>The application runs in PHP on an Apache as a web server. The DNS is managed by client. Application URl from stage environment: staging.gatewayhub.wevee.com There is one Application Load Balancer shared between the stage and production environments separated by rules with different target groups. The stage target group navigationAndlifecycle-stg has EC2 apache-stg-1 as target. Everything that it is possible to encrypt on at Rest would be done with KMS or AWS default keys, keep this convention.</p> <p>Apache is installed on the EC2 machine, on port 80. The apache configuration file is /etc/apache2/apache2.conf.</p>"},{"location":"customers/wevee-C2404/wevee/#prd","title":"Prd","text":"<p>The application runs in PHP on an Apache as a web server. The DNS is managed by client. Application URl from stage environment: gatewayhub.wevee.com There is one Application Load Balancer shared between the stage and production environments separated by rules with different target groups. The stage target group navigationAndlifecycle-prd has EC2 apache-prd-1 as target. Everything that it is possible to encrypt on at Rest would be done with KMS or AWS default keys, keep this convention.</p> <p>Apache is installed on the EC2 machine, on port 80. The apache configuration file is /etc/apache2/apache2.conf.</p>"},{"location":"customers/while1-C2335/","title":"WHILE1","text":""},{"location":"customers/while1-C2335/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR220311: Analisi ed ottimizzazione della soluzione AWS While1</li> </ul>"},{"location":"customers/while1-C2335/#aws","title":"AWS","text":"Text Only<pre><code>Account: 976762414416\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"customers/while1-C2335/#architettura","title":"Architettura","text":"<p>Analisi: analisi_while1.xlsx Documenti consegnati:</p> <ul> <li>while1_lld_architecture_eng.pdf</li> <li>while1_lld_architecture_eng.docx</li> </ul> <p></p>"},{"location":"customers/worldmatch-C2214/","title":"Worldmatch @ C2214","text":""},{"location":"customers/worldmatch-C2214/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR200568: VDC Milano Microsoft</li> </ul>"},{"location":"customers/worldmatch-C2214/#note-h24","title":"Note H24","text":"<ul> <li>Le VM vengono monitorate da https://monitoring.rz2.criticalcase.cloud</li> <li>In caso di Alert/Incident contattare il supporto di criticalcase.</li> </ul>"},{"location":"customers/worldmatch-C2214/#worldmatch","title":"Worldmatch","text":"<p>Commerciale: Alessandro Zoncu</p>"},{"location":"services/aws/","title":"AWS","text":""},{"location":"services/aws/#access-to-all-aws-console-accounts","title":"Access to ALL AWS console accounts","text":"<p>This account is used to grant access to every customer's AWS Console.</p> <ul> <li>Login to AWS console</li> </ul> <p>Do not deploy resources in this account, only IAM Users!</p> <p></p>"},{"location":"services/aws/#main-aws-account","title":"Main AWS account","text":"Text Only<pre><code>Account: 137782559982\nAlias: cc-digital\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"services/aws/#criticalcase-legacy-aws-account","title":"Criticalcase legacy AWS account","text":"Text Only<pre><code>Account: 474512620407\nAlias: criticalcase\nEmail: ds-admins+criticalcase@criticalcase.com\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"services/aws/#reserved-instances-account","title":"Reserved Instances account","text":"Text Only<pre><code>Account: 253984678330\nAlias: cc-reserved-instances\nEmail: techdata@criticalcase.com\n</code></pre> <ul> <li> <p>Login to AWS console</p> </li> <li> <p>Buy reserved instances in eu-central-1: https://eu-central-1.console.aws.amazon.com/ec2/v2/home?region=eu-central-1#ReservedInstances:</p> </li> </ul>"},{"location":"services/aws/#poc-and-test-account","title":"PoC and test account","text":"Text Only<pre><code>Account: 281670075220\nAlias: cc-poc-test\nEmail: aws-poc-acccount@criticalcase.com\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"services/aws/#dedicated-payer-and-aws-organization","title":"Dedicated payer and AWS Organization","text":"Text Only<pre><code>Account: 916257564296\nAlias: criticalcase-payer\nUser: OrgUser\n</code></pre> <ul> <li>Login to AWS console</li> </ul>"},{"location":"services/aws/#setup-new-customers-account","title":"Setup new customer's account","text":""},{"location":"services/aws/#enable-access-for-billing","title":"Enable access for billing","text":"<ul> <li>Access https://iam-roles.criticalcasecloud.com/ and then choose [Abilitare l'accesso al Billing]</li> </ul> <p>Or follow the steps below:</p> <p>Login with the root account:</p> <ul> <li>Sign in to the AWS Management Console with your root account credentials (specifically, the email address and password that you used to create your AWS account).</li> <li>On the navigation bar, choose your account name, and then choose [My Account](https://console.aws.amazon.com/billing/home#/account.</li> <li>Next to IAM User and Role Access to Billing Information, choose Edit.</li> <li>Select the Activate IAM Access check box to activate access to the Billing and Cost Management console pages.</li> <li>Choose Update.</li> </ul> <p>More details: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/control-access-billing.html#ControllingAccessWebsite-Activate</p>"},{"location":"services/aws/#iam-alias","title":"IAM alias","text":"<p>Set a custom alias in IAM https://us-east-1.console.aws.amazon.com/iamv2/home#/home</p>"},{"location":"services/aws/#iam-roles","title":"IAM Roles","text":"<ul> <li>Access https://iam-roles.criticalcasecloud.com/ and then choose [Creare un ruolo di amministratore] or [Creare un ruolo di lettura]</li> </ul> <p>Or follow the steps below:</p> <ul> <li>Create Role: https://console.aws.amazon.com/iam/home?region=eu-central-1#/roles$new?step=type&amp;roleType=crossAccount</li> <li>Account ID: 580515027983</li> <li>Next: Permissions</li> <li>Select: AdministratorAccess</li> <li>Next: Tags</li> <li>Next: review</li> <li>Role name: criticalcase</li> <li>Role description: Administrator access to AWS Console by Criticalcase users</li> </ul>"},{"location":"services/aws/#enable-mfa-for-root-account","title":"Enable MFA for root account","text":""},{"location":"services/aws/#virtual-vault","title":"Virtual Vault","text":"<p>Follow this step only if we manage the root customer's account</p> <ul> <li>Login as root account: console.aws.amazon.com</li> <li>Go to account</li> <li>Set the <code>account name</code> to match the IAM Account Alias.</li> <li>Edit <code>Contact Information</code> &gt; <code>Phone Number</code>: <code>+390110888765</code></li> <li> <p>Go to assign mfa devices under security credentials:</p> </li> <li> <p>Device name: <code>criticalcase-sherlock</code></p> </li> <li>Select MFA device: Authenticator app</li> <li>Press Next</li> <li> <p>Press Show secret key and copy the token, i.e.: <code>42LY3T2GCOZDIMX2PVUNPCZWDEF4COP2OTAW54UIT3ZX4NGP3UBZYGLS7JYSOQVE</code></p> </li> <li> <p>Go to sherlock and open the: Vault Browser CLI</p> </li> <li> <p>Set <code>$KEYNAME</code> as aws-root-<code>account name</code>: i.e. aws-root-cc-reserved-instances</p> </li> <li>Set <code>$ACCOUNTNAME</code> as aws account id: i.e. 253984678330</li> <li>Set <code>$SECRET</code> as the MFA Token: i.e. <code>42LY3T2GCOZDIMX2PVUNPCZWDEF4COP2OTAW54UIT3ZX4NGP3UBZYGLS7JYSOQVE</code></li> </ul> Bash<pre><code># Replace the VARS\nvault write totp/keys/$KEYNAME url=\"otpauth://totp/Vault:$ACCOUNTNAME?secret=$SECRET\"\n\n# Save the MFA token\nvault write totp/keys/aws-root-cc-reserved-instances url=\"otpauth://totp/Vault:253984678330?secret=42LY3T2GCOZDIMX2PVUNPCZWDEF4COP2OTAW54UIT3ZX4NGP3UBZYGLS7JYSOQVE\"\n\n# Read the OTP:\nvault read totp/code/aws-root-cc-reserved-instances\n\n# Wait 30s and read again the OTP:\nvault read totp/code/aws-root-cc-reserved-instances\n</code></pre> <ul> <li>Copy the two OTP on the AWS console</li> <li>Press: Add MFA</li> <li>Copy the vault read command as secret in key vault:</li> <li>Key: <code>MFA</code></li> <li>Vaule: <code>vault read totp/code/aws-root-cc-reserved-instances</code></li> </ul>"},{"location":"services/aws/#yubikey","title":"Yubikey","text":"<p>Follow this step only if we manage the root customer's account</p> <ul> <li>Login as root account: console.aws.amazon.com</li> <li>Go to assign mfa devices under security credentials:</li> <li>Device name: <code>criticalcase-youbikey-ito1</code></li> <li>Select MFA device: Security Key</li> <li>Press Next</li> <li>USB Security Token</li> <li>Plug-in the yubikey</li> <li>Press the button</li> <li>Allow</li> </ul>"},{"location":"services/aws/#criticalcase-account","title":"Criticalcase account","text":"<p>Create <code>criticalcse</code> account with an access_key and secret_key. Update the secret on vault: https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/show/ds/services/aws-user-credentials</p>"},{"location":"services/aws/#lambda-cross-account","title":"Lambda Cross-Account","text":"<p>Please respect the naming convention as much as possible: Use the character - between words and not _ Start the branch/resource name with the name of the AWS resource the lambda will interact with, example: ec2-delete-protection</p>"},{"location":"services/aws/#architecture","title":"Architecture","text":"<p>Update in the customer account the iam role for allow the lambda assume role, example: iam.tf Cloudwatch Events trigger lambdas on the CriticalCase account which go to modify/monitor the AWS accounts of the customers via the cross account.</p> <p></p>"},{"location":"services/aws/#github-configuration","title":"GitHub configuration","text":"<p>Create new branch in this repository: https://github.com/criticalcase/digital-solution-lambda-cross-account</p>"},{"location":"services/aws/#terraform-configuration","title":"Terraform configuration","text":"<p>Clone repository: https://github.com/criticalcase/digital-solution Create module in lambda_cross_account.tf  Clone a existing directory under /terraform/prd/lambda and rename it with the name used in maint.tf  In the file main.tf under the lambda, change the module event based on what the lambda has to do, use the variable cont for create other events Change in the variable.tf the name of the new resource, the branchName and the schedule_expression if necessary  In the file events_cross_account.tf put the json in the clients where the lambda should run  Push the code from digital-solution\\terraform path, Terraform cloud will execute the plan and ask you if it can create the resource: https://app.terraform.io/app/criticalcase/workspaces/digital-solution-prd </p>"},{"location":"services/aws/#update-lambda-code","title":"Update lambda code","text":"<p>Add your code in the branch created in digital-solution-lambda-cross-account and push it. Make sure you have the buildspec.yml file in the root branch: buildspec.yml Codepipeline will update the lambda automatically.</p>"},{"location":"services/aws/#system-manager-patch-management","title":"System Manager patch management","text":"<p>The page has been moved: SSM</p>"},{"location":"services/aws/#aws-extend-switch-roles","title":"AWS Extend switch roles","text":"<p>We don't use Extend switch roles anymore, please see granted</p>"},{"location":"services/aws/#force-pin-verification","title":"Force PIN verification","text":"<p>Useful when you're tring to remove an account from an organization</p> <ul> <li>https://portal.aws.amazon.com/gp/aws/developer/registration/index.html?client=organizations&amp;enforcePI=True</li> </ul>"},{"location":"services/aws/SSM/","title":"SSM AWS System Manager patch management","text":"<p>Requriements: aws-cli</p>"},{"location":"services/aws/SSM/#architecture","title":"Architecture","text":""},{"location":"services/aws/SSM/#create-hybrid-activations","title":"Create Hybrid Activations","text":"<p>It is important to complete as many fields as possible in tags such as Location, Account, Name, Patch Group so that the tags will be associated automatically when a server is registered.</p> Bash<pre><code># Set AWS account cc-digital in aws-cli\nasp cc-digital\n# or open a cloud shell: https://eu-central-1.console.aws.amazon.com/cloudshell/home?region=eu-central-1#\n\n# Set variables\nexport CUSTOMERNAME=Hibo\nexport REGION=RZ1\nexport LOCATION=VC6-IS1\n\n# Use https://www.unixtimestamp.com/ to calculate the expiration-date (max 30 days)\nexport EXPIRE=$[`date +%s` + 2592000]\n\naws ssm create-activation --iam-role service-role/AmazonEC2RunCommandRoleForManagedInstances --registration-limit 1000 --region eu-central-1 --expiration-date $EXPIRE --default-instance-name $CUSTOMERNAME --description \"Activation for $CUSTOMERNAME\" --tags \"Key=Name,Value=$CUSTOMERNAME\" \"Key=Location,Value=$LOCATION\" \"Key=Region,Value=$REGION\" \"Key=Patch Group,Value=$CUSTOMERNAME\" \"Key=SSM,Value=true\"\n\n# Save the results as environment variables\nexport SSM_CODE=AFCuY1pVYoiOQQawdrt2\nexport SSM_ID=abbc3ac4-bcc5-4461-b335-82f180a792fa\n</code></pre>"},{"location":"services/aws/SSM/#install-ssm-agent","title":"Install SSM Agent","text":"<ul> <li>Official guide Linux: https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-install-managed-linux.html</li> <li>Official guide Windows: https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-install-managed-win.html</li> </ul>"},{"location":"services/aws/SSM/#centos","title":"Centos","text":"Bash<pre><code># Set variables from Hybrid Activation\nexport SSM_CODE=AFCuY1pVYoiOQQawdrt2\nexport SSM_ID=abbc3ac4-bcc5-4461-b335-82f180a792fa\n\nmkdir /tmp/ssm\ncurl https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm -o /tmp/ssm/amazon-ssm-agent.rpm\n\nsudo dnf install -y /tmp/ssm/amazon-ssm-agent.rpm\n\nsudo -E amazon-ssm-agent -register -region \"eu-central-1\" -code \"$SSM_CODE\" -id \"$SSM_ID\"\n\nsudo systemctl restart amazon-ssm-agent\nsudo systemctl enable amazon-ssm-agent\n</code></pre>"},{"location":"services/aws/SSM/#ubuntu","title":"Ubuntu","text":"Bash<pre><code># Set variables from Hybrid Activation\nexport SSM_CODE=AFCuY1pVYoiOQQawdrt2\nexport SSM_ID=abbc3ac4-bcc5-4461-b335-82f180a792fa\n\nmkdir /tmp/ssm\ncurl https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/debian_amd64/amazon-ssm-agent.deb -o /tmp/ssm/amazon-ssm-agent.deb\n\nsudo dpkg -i /tmp/ssm/amazon-ssm-agent.deb\n\nsudo -E amazon-ssm-agent -register -region \"eu-central-1\" -code \"$SSM_CODE\" -id \"$SSM_ID\"\n\nsudo systemctl restart amazon-ssm-agent\nsudo systemctl enable amazon-ssm-agent\n</code></pre>"},{"location":"services/aws/SSM/#windows-powershell-administrator","title":"Windows Powershell Administrator","text":"PowerShell<pre><code># Set variables from Hybris Activation\n$SSM_CODE = \"AFCuY1pVYoiOQQawdrt2\"\n$SSM_ID = \"abbc3ac4-bcc5-4461-b335-82f180a792fa\"\n\n# Execute the script\n$region = \"eu-central-1\"\n$dir = \"c:\\ssm\"\nNew-Item -ItemType directory -Path $dir -Force\ncd $dir\n(New-Object System.Net.WebClient).DownloadFile(\"https://amazon-ssm-$region.s3.$region.amazonaws.com/latest/windows_amd64/AmazonSSMAgentSetup.exe\", $dir + \"\\AmazonSSMAgentSetup.exe\")\nStart-Process .\\AmazonSSMAgentSetup.exe -ArgumentList @(\"/q\", \"/log\", \"install.log\", \"CODE=$SSM_CODE\", \"ID=$SSM_ID\", \"REGION=$region\") -Wait\nGet-Content ($env:ProgramData + \"\\Amazon\\SSM\\InstanceData\\registration\")\nGet-Service -Name \"AmazonSSMAgent\"\n</code></pre>"},{"location":"services/aws/SSM/#check-activation","title":"Check activation","text":"<ul> <li><code>Patch group = Hibo</code> https://eu-central-1.console.aws.amazon.com/systems-manager/managed-instances?region=eu-central-1</li> </ul>"},{"location":"services/aws/SSM/#create-customer-account","title":"Create customer account","text":"<p>Create Group</p> <ul> <li>https://eu-central-1.console.aws.amazon.com/cognito/v2/idp/user-pools/eu-central-1_5WzvirwxV/groups/create?region=eu-central-1   Group Name: Hibo</li> </ul> <p>Create user</p> <ul> <li>https://eu-central-1.console.aws.amazon.com/cognito/v2/idp/user-pools/eu-central-1_5WzvirwxV/users/create?region=eu-central-1</li> </ul> <p>Invitation message: Don't send an invitation   User name: customer@email.com   Temporary password: Set a password</p> <ul> <li>Go to the user page</li> <li> <p>User attributes: Edit   email: customer@email.com - Mark email address as verified</p> </li> <li> <p>Group memberships: Add user to a group   Group name: Hibo</p> </li> </ul>"},{"location":"services/aws/SSM/#reset-user-credential-if-necessary","title":"Reset user credential (If necessary)","text":"<ul> <li>Only from AWS CLI</li> </ul> Bash<pre><code>aws cognito-idp admin-set-user-password --user-pool-id USER-POOL-ID --username USERNAME --password PASSWORD --permanent\n</code></pre>"},{"location":"services/aws/SSM/#add-patch-groups","title":"Add patch groups","text":"<p>Add the VMs <code>Patch groups</code> to the lambda: https://eu-central-1.console.aws.amazon.com/systems-manager/patch-manager/baselines/pb-05e39916716bb4394/patch-groups/edit?region=eu-central-1</p> <ul> <li>Patch groups: Hibo</li> <li>Press: Add</li> </ul> <p>Execute the ScanOnly:</p> <p>https://eu-central-1.console.aws.amazon.com/systems-manager/state-manager/9040d54f-98a8-46d3-9015-f23a525a2a55/description</p>"},{"location":"services/aws/SSM/#send-welcome-email","title":"Send Welcome Email","text":"<p>Wait 1 day and check if reports are created:</p> <ul> <li>https://s3.console.aws.amazon.com/s3/buckets/ssm-patch-management-criticalcase?region=eu-central-1&amp;tab=objects</li> <li>Send the Welcome Email to the customer with the Temporary password</li> </ul>"},{"location":"services/aws/eks-IAM-profile/","title":"EKS IAM Profile","text":"<ul> <li> <p>creare utente con stessi permessi dell'utente <code>criticalcase</code> e memorizzare Access Key e Secret Key</p> </li> <li> <p>Configurare su aws cli le credenziali dell'account con cui si \u00e8 creato il cluster EKS:</p> </li> </ul> Text Only<pre><code>aws configure --profile &lt;customer&gt;\nAWS Access Key ID [None]: &lt;access-key&gt;\nAWS Secret Access Key [None]: &lt;secret-key&gt;\nDefault region name [None]: eu-central-1\nDefault output format [None]:\n</code></pre> <p>e del nuovo account personale creato su aws:</p> Text Only<pre><code>aws configure --profile &lt;es. n.cognome-customer&gt;\nAWS Access Key ID [None]: &lt;mia access key&gt;\nAWS Secret Access Key [None]: &lt;mia secret key&gt;\nDefault region name [None]: eu-central-1\nDefault output format [None]:\n</code></pre> <ul> <li>Usare il kubeconfig per accedere al cluster EKS del cliente <code>~/.kube/aws-customer.yaml</code> con le credenziali dell'account utilizzato per creare il cluster:</li> </ul> Text Only<pre><code>...\nusers:\n- name: &lt;userArn&gt;\n  user:\n    exec:\n      apiVersion: client.authentication.k8s.io/v1beta1\n      args:\n      - --region\n      - eu-central-1\n      - eks\n      - get-token\n      - --cluster-name\n      - libellula\n      env:\n      - name: AWS_PROFILE\n        value: &lt;customer&gt;\n      command: aws\n</code></pre> <ul> <li>Usare il comando <code>kubectl edit -n kube-system configmap/aws-auth</code> per modificare la configmap sul cluster.</li> </ul> <p>N.B. attenzione a non commettere errori nella modifica del file, si rischia di perdere l'accesso al cluster anche con l'account admin</p> <p>Nel file che si apre, aggiungere la seguente sezione nella lista dei <code>mapUsers</code>:</p> Text Only<pre><code>    - groups:\n      - system:masters\n      userarn: &lt;userarn del mio utente creato su aws&gt;\n      username: &lt;username del mio utente creato su aws&gt;\n</code></pre> <ul> <li>Una volta effettuata la modifica, modificare il file <code>~/.kube/aws-customer.yaml</code> sostituendo nella penultima riga <code>value: &lt;customer&gt;</code> con il nome del proprio profilo creato sulla aws cli <code>value: n.cognome-customer</code>. Adesso si pu\u00f2 eseguire l'accesso al cluster con il proprio profilo.</li> </ul>"},{"location":"services/aws/granted/","title":"Granted - AssumeRoleCli","text":"<p>This will replace AWS Extend switch roles.</p>"},{"location":"services/aws/granted/#granted-registry","title":"Granted registry","text":"<p>We use granted registry to share consistent AWS profiles amongst team members. To add a configuration or download our latest config files see aws-granted on github.</p>"},{"location":"services/aws/granted/#autocomplete","title":"Autocomplete","text":"<p>Documentation: https://docs.commonfate.io/granted/configuration/#autocompletion</p> Bash<pre><code># ZSHRC\ngranted completion -s zsh\n</code></pre>"},{"location":"services/aws/granted/#auto-reassume","title":"Auto reassume","text":"<p>By default Granted will assume a given role for <code>1hr</code>. This can be altered by passing the <code>--duration</code> flag, or by specifying a <code>duration_seconds</code> prop in your config file. This means that after a given hour of activity, you will need to re-run assume to get a new set of credentials.</p> <p>If you would like to automatically reassume roles, with ZSH you can you can add the following to your <code>~/.zshrc</code>:</p> Bash<pre><code># ZSHRC\necho 'export GRANTED_ENABLE_AUTO_REASSUME=true' &gt;&gt; ~/.zshrc\n</code></pre>"},{"location":"services/aws/granted/#how-to-avoid-switch-role-to-switch-context","title":"How to avoid switch role to switch context","text":"<p>The first two step must be executed once:</p> <ol> <li>Find the path where the assume command is installed</li> </ol> Bash<pre><code>whereis assume\n</code></pre> <ol> <li> <p>Edit the assume file and comment the export commands    <code>shell     [...]       if [ ! \"${GRANTED_1}\" = \"None\" ]; then       #  export AWS_ACCESS_KEY_ID=\"${GRANTED_1}\"       fi       if [ ! \"${GRANTED_2}\" = \"None\" ]; then       #  export AWS_SECRET_ACCESS_KEY=\"${GRANTED_2}\"       fi     [...]</code>    The following steps must be executed every time the session expires:</p> </li> <li> <p>Assume the role of a maxmara account with the <code>assume</code> command</p> </li> <li> <p>You can switch context with kubie, for example</p> </li> </ol> Bash<pre><code>kubie ctx b2c-marella-magnolia-uat\n</code></pre> <ol> <li>test if the connection to the cluster is succesfull:    Bash<pre><code>kubectl get nodes\n</code></pre></li> </ol>"},{"location":"services/aws/granted/#assume","title":"Assume","text":"<p>Visto che non creiamo utenze su ciascun account AWS del cliente, \u00e8 possibile attraverso la CLI installata sul proprio computer assumere il ruolo criticalcase nell'account di destinazione</p> <ul> <li>Installare la CLI sul proprio computer (https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)</li> <li>Creare una utenza all'interno dell'account AWS criticalcase-access , aggiungerlo al Gruppo Admin e creare una access/secret keys</li> <li>modificare i files all'interno della cartella .aws della proprio utenza nel computer (esempio per Windows-&gt; C:\\Users\\NOMEUTENTE.aws):</li> <li>nel file credentials inserire le precedenti access/secret keys create:     <code>[criticalcase-access]     aws_access_key_id = \\***\\*     aws_secret_access_key = \\*\\***</code></li> <li>nel file config andiamo a definire il profilo del cliente, specificando l'arn del ruolo con l'account id del customer. source_profile va indicato il nome definito nel file credentials inserire le precedenti access/secret keys create:   Text Only<pre><code>[profile miamo]\nrole_arn = arn:aws:iam::703194009217:role/criticalcase\nsource_profile = criticalcase-access\nregion = eu-central-1\n</code></pre></li> </ul> <p>```</p> <p>Esempio di comando per testare il funzionamento:</p> Text Only<pre><code>assume miamo\naws s3 ls\n</code></pre>"},{"location":"services/aws/granted/#granted-configuration-for-wsl-on-windows","title":"GRANTED - Configuration for WSL on Windows","text":"<p>The following guide was created with the aim of providing a step by step guide for the installation and configuration of Granted and solving some \"known problems\".</p> <p>AWS CLI installation (required)</p> Text Only<pre><code>apt install awscli\n</code></pre> <p>Configure AWS CLI access key:</p> Text Only<pre><code>aws configure\n</code></pre> <p>Enter aws_access_key_id, aws_secret_access_key, region, and press Enter, then run the command:</p> Text Only<pre><code>nano .aws/credentials\n</code></pre> <p>Rename the default profile to criticalcase-access:</p> Text Only<pre><code>[criticalcase-access]\naws_access_key_id = &lt;AWS_ACCESS_KEY_ID&gt;\naws_secret_access_key = &lt;AWS_SECRET_ACCESS_KEY&gt;\n</code></pre> <p>Install pass:</p> Text Only<pre><code>apt install pass\n</code></pre> <p>Install GRANTED:</p> Text Only<pre><code>curl -OL releases.commonfate.io/granted/v0.20.2/granted_0.20.2_linux_x86_64.tar.gz\nsudo tar -zxvf ./granted_0.20.2_linux_x86_64.tar.gz -C /usr/local/bin/\n</code></pre> <p>Generate GPG key:</p> Text Only<pre><code>gpg --gen-key\n\ngpg --list-secret-keys\n\npass init &lt;EXAMPLEGPGKEYFROMLISTSECRETKEYS&gt;\n</code></pre> <p>First configuration of the granted browser, run:</p> Text Only<pre><code>assume\n</code></pre> <p>Select the browser (FIREFOX is preferable), the granted extension must be installed on it.</p> <p>Edit the .granted/config file as follows (adding the [Keyring] part at the end of the file):</p> Text Only<pre><code>[Keyring]\nBackend = \"pass\"\n</code></pre> <p>Run the command:</p> Text Only<pre><code>granted registry add -name criticalcase -url https://github.com/criticalcase/aws-granted.git\n</code></pre> <p>Run the 3 commands that will add export GPG_TTY and other configurations to the .bashrc file:</p> Text Only<pre><code>echo 'export GPG_TTY=$(tty)' &gt;&gt; ~/.bashrc\necho 'export GRANTED_ENABLE_AUTO_REASSUME=true' &gt;&gt; ~/.bashrc\necho 'alias assume=\"source assume\"' &gt;&gt; ~/.bashrc\n</code></pre> <p>Find the assume file:</p> Text Only<pre><code>nano $(whereis assume | awk '{print $2}')\n</code></pre> <p>/usr/local/bin/assume (e.g. path with Ubuntu)</p> <p>Comment out the following lines in the file:</p> Text Only<pre><code>  #if [ ! \"${GRANTED_1}\" = \"None\" ]; then\n  #export AWS_ACCESS_KEY_ID=\"${GRANTED_1}\"\n  #fi\n  #if [ ! \"${GRANTED_2}\" = \"None\" ]; then\n  #export AWS_SECRET_ACCESS_KEY=\"${GRANTED_2}\"\n  #fi\n</code></pre> <p>Run the command:</p> Text Only<pre><code>source .bashrc\n</code></pre> <p>Import AWS credentials:</p> Text Only<pre><code>granted credentials import criticalcase-access\n</code></pre> <p>Verify with the following command that the credentials just modified are returned:</p> Text Only<pre><code>granted credential-process --profile=criticalcase-access\n</code></pre> <p>Assume a role:</p> Text Only<pre><code>assume\n</code></pre> <p>Select a role (e.g., cc-poc-test):</p> Text Only<pre><code>aws s3 ls\n</code></pre> <p>If it returns a list of buckets, it is working correctly!</p>"},{"location":"services/aws/granted/#known-issues","title":"Known issues","text":"<p>Often, virtualized WSL on Windows experiences synchronization problems leading to errors with 'assume' as the following:</p> <p></p> <p>To solve it enable automatic synchronization with an NTP server:</p> Text Only<pre><code>sudo apt install ntp\n</code></pre> <p>Alternatively:</p> Text Only<pre><code>sudo ntpdate pool.ntp.org\n</code></pre>"},{"location":"services/aws/granted/#_1","title":"granted","text":"<p>In case settings in .bashrc are not initialized with each new access, you may see this error: </p> <p>It is necessary to set it at startup, try to set a profile:</p> Text Only<pre><code>cat /etc/profile\n</code></pre> <p>You may find a situation like this or similar:</p> Text Only<pre><code>if [ -f /etc/bash.bashrc ]; then\n. /etc/bash.bashrc\nfi\n</code></pre> <p>We want it to reference ~/.bashrc for initialization at startup:</p> Text Only<pre><code>if [ -f ~/.bashrc ]; then\n. ~/.bashrc\nfi\n</code></pre>"},{"location":"services/aws/restore-rds/","title":"Restore RDS Backup","text":"<ul> <li>Identify the name of the snapshot to restore</li> <li>Create a new RDS terraform resource like in this example specifying the snapshot to use</li> <li>Once the new RDS instance is available, add to the EC2 used to reach the RDS the ARN of the db user to allow teleport connection and open the egress port to reach the security group attached to the new instance, like in this PR</li> <li>As last step, add to the <code>/etc/teleport.yaml</code> config file in the VM used to reach the db the configuration related to the new RDS, for example:</li> </ul> YAML<pre><code>...\n  databases:\n    ...\n  - name: magento-prd-restored-31052024\n    description: magento-prd-restored-31052024 RDS\n    protocol: mysql\n    uri: &lt;uri-new-rds&gt;\n    aws:\n      region: eu-central-1\n    static_labels:\n      env: prd\n...\n</code></pre> <p>then restart the teleport service:</p> Bash<pre><code>systemctl restart teleport\n</code></pre>"},{"location":"services/azure/","title":"Azure","text":"<ul> <li> <p>Go to: <code>Azure Active Directory</code> &gt; <code>Users</code>: https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/Overview</p> </li> <li> <p>Click on: <code>New guest user</code></p> </li> <li>Invite user</li> <li><code>Name</code>: es. Andrea Sosso</li> <li><code>Email address</code>: es. a.sosso@criticalcase.com</li> <li><code>Roles</code>: select <code>Global administrator</code></li> <li><code>Groups</code>: criticalcase</li> <li>Click on: <code>Invite</code></li> </ul> <p>As first, step a criticalcase group with owner permissions on the Subsrciption must be created on the customer account</p>"},{"location":"services/azure/#email-list","title":"Email list","text":"Text Only<pre><code>a.sosso@criticalcase.com\ns.russo@criticalcase.com\nj.garia@criticalcase.com\na.barbaglia@criticalcase.com\nt.petronio@criticalcase.com\nl.farinazzo@criticalcase.com\nl.tetti@criticalcase.com\nf.pasand@criticalcase.com\n\n# ITO only\non.call@criticalcase.com\n</code></pre>"},{"location":"services/azure/#add-users-to-owner-group","title":"Add users to Owner Group","text":"<ul> <li>Go to <code>Access control (IAM)</code> and then click on <code>Add role assignement</code></li> <li>Click on the Role Name (ex: <code>Owner</code>) on the Role Tab</li> <li>Switch to the <code>Members</code> Tab</li> <li>Click on <code>+ Select Members</code></li> <li>Select all members you want to add to this role and push on <code>Select</code> button on the right column</li> <li>Click on <code>Review + Assign</code></li> </ul>"},{"location":"services/azure/#create-azure-plan-on-td-synnex","title":"Create Azure plan on TD Synnex","text":"<ol> <li>Create the Customer from Customers -&gt; Add</li> <li>Go to the Customers section, open the new customer -&gt; Cloud Billing -&gt; loud Providers -&gt; Add    Cloud Provider: Microsoft CSP    Price Book: Microsoft CSP Master Pricebook    Invoice Currency: EUR</li> <li>Go to the Marketplace Section, select the customer in the \"View Catalog As\" drop menu, then select <code>Microsoft Azure Plan</code></li> <li>From \"PLANS\" tick the Azure Plan box, then <code>Add to cart</code></li> <li>Click on <code>Configuration Pending</code>:    Select \"Create New Microsoft Account\" and fill the form with the Customer Data. Then Continue. Tick the auto assignment box. Save the credentials that are shown, then Continue.</li> <li>Complete the order with the \"Place Your Order\" Button</li> <li>Go back to the Customer page, check the status in Overview -&gt; Orders. The Order is completed when it appears in Overview -&gt; Inventory.</li> <li>When the order is completed, click on the new Azure Plan, add a new subscription to the tenant (please update this section with a more precise guide) and select the owner for the subscription</li> </ol>"},{"location":"services/azure/#migrate-azure-plan-on-td-synnex","title":"Migrate Azure plan on TD Synnex","text":"<ol> <li> <p>[MIGRATE ONLY] To migrate a subscription from other distributor you must allow the TD Synnex parnter using the following link: https://admin.microsoft.com/Adminportal/Home?invType=ResellerRelationship&amp;partnerId=35fbd38b-7fdb-48d1-92ba-75adb4d76b2d&amp;msppId=0&amp;DAP=true#/BillingAccounts/partner-invitation</p> </li> <li> <p>Customer &gt; Cloud Billing &gt; Cloud provider &gt; Add:    Cloud Provider: Legacy Azure    Price Book: Legacy Azure PAYG Master Pricebook    Invoice Currency: EUR</p> </li> <li> <p>Double click on the price book and Enable provider in customer portal</p> </li> <li> <p>Create a fake User: https://ion.tdsynnex.com/v2/customer360/details/76318/customer-users    Replace 57302 with techdata customer ID    Contact Name: Criticalcase Support    Email: NotRealUsername@criticalcase.com    Password: generate a random password</p> </li> <li> <p>Add azure plan: https://ion.techdata.com/v2/customer360/details/57302/buy-products/catalog-cart#cat-search=%7B%22category%22:%22All%20Products%22%7D    Replace 57302 with techdata customer ID</p> </li> <li> <p>Go TO CART and press configure. Fill the required data and copy the tenant information.</p> </li> <li> <p>Place Order button in cart page</p> </li> <li> <p>[NEW ACCOUNT ONLY] Login to the created microsoft account and set the new password</p> </li> <li> <p>Ask TD Synnex (alessandro.borgonovo@tdsynnex.com) to enable the Azure Cost Management</p> </li> <li> <p>[MIGRATE ONLY] Execute this powershell script from azure Cloud Shell</p> </li> </ol> Text Only<pre><code># Required if not in cloud shell\nInstall-Module Az\nConnect-AzAccount Az\n\n$customertenantID = '' #Tenant ID del cliente finale\n$customerSubId = '' #Inserire l'Id della Sottosrizione di Azure del cliente\n\n$PartnerAdminAgents = 'e99d7d6c-0676-4764-9ab2-ddf4a8577fa6'\nSet-AzContext -Tenant $customertenantID\n\nNew-AzRoleAssignment -ObjectId $PartnerAdminAgents -RoleDefinitionName Owner -Scope \"/subscriptions/$customerSubId\" -ObjectType ForeignGroup\n</code></pre>"},{"location":"services/azure/#recover-azuremicrosoft365-admin-password","title":"Recover Azure/Microsoft365 admin password","text":"<ul> <li>Call the number: https://learn.microsoft.com/en-us/microsoft-365/admin/get-help-support?view=o365-worldwide</li> </ul>"},{"location":"services/azure/azure-ri/","title":"Azure Reserved Instances","text":"<p>Through the ION portal, it is possible to buy and manage Azure Reserved Instances.</p>"},{"location":"services/azure/azure-ri/#new-reserved-instance","title":"New Reserved Instance","text":"<p>As first, be sure to have a valid GDAP configured in the SecOps section https://ion.tdsynnex.com/v2/rlp/details/list of the ION portal for the customer</p> <ul> <li>Go to the Marketplace section https://ion.tdsynnex.com/v2/orb360-marketplace/categories</li> <li>Select the <code>Azure reserved istance</code> tab https://ion.tdsynnex.com/v2/orb360-marketplace/product-details?shortName=AzureRI-it</li> <li>Choose the customer from the <code>View Catalog As</code> drop-down menu</li> <li>In the <code>PLANS</code>section select the desired <code>Region</code> and the desired category (i.e. Azure Reservations Virtual Machines) and click on <code>Update Results</code></li> <li>Wait for the Product List to be populated</li> <li>Choose the desired instance size and the desired duration (it is specified in the <code>Title</code>, i.e. Reserved VM Instance, Standard_B1ls, EU West, 1 Year). In the pop-up menu, choose the Billing Frequency and click on <code>Cart &amp; Checkout</code></li> <li>It is possible to select more than one RI, and to select the desired quantity clicking on <code>View More Details</code> under each RI in the summary. Then click on <code>Add to Cart</code>.</li> <li>In the cart it is still possible to change the <code>Quantity</code> and the <code>Frequency</code> for each RI product.</li> <li>For each product, click on <code>Configuration Pending</code> and check the box to agree the auto ssignment of the administrative user and the the <code>Continue</code> button.</li> <li>Press the <code>Place Your Order</code> button to buy the products.</li> </ul>"},{"location":"services/azure/azure-ri/#customize-existing-reserved-instances","title":"Customize existing Reserved Instances","text":"<ul> <li>Go to the Customer section https://ion.tdsynnex.com/v2/orb360-customers/list and select the customer related to the Reserved Instance</li> <li>Click on the desired Reserved Instance to manage</li> <li>Click on the <code>MANAGE SUBSCRIPTION</code> button</li> <li>Here all the Reserved Instances linked to the subscription are shown. Through the <code>Edit</code> button it is possible to assigne a new User or to enable/disable the auto-renew</li> </ul>"},{"location":"services/critical0/import/","title":"Import an existing terraform workspace","text":"<p>Initialize the repository</p> Bash<pre><code>tofu init\n</code></pre>"},{"location":"services/critical0/import/#migrating-from-terraform-cloud","title":"Migrating from Terraform Cloud","text":"<ul> <li>Select the Terraform Cloud workspace</li> <li>Go to <code>States</code> and download the latest state</li> <li>Upload the <code>terraform.state</code> to S3 Bucket <code>critical0-terraform-cloud-backups</code></li> <li>Go to <code>Settings &gt; Locking</code> section and Lock the workspace</li> </ul>"},{"location":"services/critical0/import/#change-the-backend-settings","title":"Change the backend settings","text":"<p>From Terraform Cloud:</p> Terraform<pre><code>  backend \"remote\" {\n    hostname     = \"app.terraform.io\"\n    organization = \"criticalcase\"\n\n    workspaces {\n      name = \"example-project-prd\"\n    }\n  }\n</code></pre> <p>To critical0:</p> Terraform<pre><code>  backend \"s3\" {\n    bucket  = \"critical0-example-project\"\n    key     = \"state/prd/terraform.tfstate\"\n    region  = \"eu-central-1\"\n    encrypt = true\n  }\n</code></pre>"},{"location":"services/critical0/import/#migrate-the-state","title":"Migrate the state","text":"Bash<pre><code>assume cc-critical0\ntofu init -migrate-state -lock=false # type: yes\n\n# Create development branch\ngit branch develop\n\n# Move to the development branch\ngit checkout develop\n</code></pre>"},{"location":"services/critical0/import/#add-configuration-files","title":"Add configuration files","text":"<p>Copy from the following files from this template to the target repository:</p> Bash<pre><code>.gitignore\n.github/\n.pre-commit-config.yaml\n.terraform-docs.yml\ndigger.yaml\nterraform/docs/\n</code></pre>"},{"location":"services/critical0/import/#change-references-to-internal-modules","title":"Change references to internal modules","text":"<p>From <code>git::git@github.com:criticalcase</code> to <code>git::https://github.com/criticalcase</code></p> <p>I.e.</p> Terraform<pre><code># Before\nsource = \"git::git@github.com:criticalcase/terraform-aws-cloudwatch-metric-alarm.git?ref=v1.0.3\"\n\n# After\nsource = \"git::https://github.com/criticalcase/terraform-aws-cloudwatch-metric-alarm.git?ref=v1.0.3\"\n</code></pre>"},{"location":"services/critical0/import/#configure-aws-ssm-parameter-store","title":"Configure AWS SSM Parameter store","text":"<p>Change the code to use the AWS SSM Parameter store instead terraform variable.</p> <p>Add the <code>aws.critical0</code> provider or import the <code>terraform/prd/critical0.tf</code>:</p> Terraform<pre><code>provider \"aws\" {\n  alias  = \"critical0\"\n  region = \"eu-central-1\"\n}\n</code></pre> <p>For AWS Cloud parameters you need to set the variables via UI (cc-critical0 account) or CLI:</p> Bash<pre><code>aws ssm put-parameter \\\n--name \"/critical0-example-project/aws_access_key_id\" \\\n--type \"String\" \\\n--value ''\n\naws ssm put-parameter \\\n--name \"/critical0-example-project/aws_secret_access_key\" \\\n--type \"SecureString\" \\\n--value ''\n</code></pre> <p>Import the data from the AWS SSM Parameter store:</p> Terraform<pre><code>data \"aws_ssm_parameter\" \"aws_access_key_id\" {\n  provider = aws.critical0\n  name     = \"/critical0-example-project/aws_access_key_id\"\n}\n\ndata \"aws_ssm_parameter\" \"aws_secret_access_key\" {\n  provider = aws.critical0\n  name     = \"/critical0-example-project/aws_secret_access_key\"\n}\n</code></pre> <p>Update the variables in all files:</p> <ul> <li>From <code>var.aws_access_key</code> to <code>data.aws_ssm_parameter.aws_access_key_id.value</code></li> <li>From <code>var.aws_secret_key</code> to <code>data.aws_ssm_parameter.aws_secret_access_key.value</code></li> </ul> <p>Remove the old variable definitions from <code>variables.tf</code> i.e.:</p> Terraform<pre><code># variable \"aws_access_key\" {\n#   type = string\n# }\n\n# variable \"aws_secret_key\" {\n#   type = string\n# }\n</code></pre>"},{"location":"services/critical0/import/#commit-the-changes","title":"Commit the changes","text":"Bash<pre><code>tofu init\ngit add --all\ngit commit -m \"Try github actions\"\ngit push --set-upstream origin develop\n\n# Create a new pull request\ngh pr create -t \"Terraform plan\" -b \"Digger runs: terraform plan\"\n</code></pre>"},{"location":"services/critical0/import/#modules-update","title":"Modules update","text":"<ul> <li>module <code>onboarding</code>: https://github.com/criticalcase/terraform-aws-onboarding-ito/blob/main/CHANGELOG.md#v110---11082023</li> <li>module <code>waf_ito</code>: https://github.com/criticalcase/terraform-aws-waf-ito/blob/main/CHANGELOG.md#v110---11082023</li> </ul>"},{"location":"services/critical0/import/#remove-the-workspace-from-terraform-cloud","title":"Remove the workspace from Terraform Cloud","text":"<p>When you're sure that everthing is working well you can cleanup Terraform Cloud:</p> <ul> <li>Go to destruction and deletion</li> <li>Uncheck allow destroy plan</li> <li>Force delete from Terraform Cloud</li> </ul>"},{"location":"services/critical0/import/#issues","title":"Issues","text":"Bash<pre><code>\u279c tofu init\n\nInitializing the backend...\nMigrating from tofu Cloud to backend \"s3\".\nInitializing modules...\n\u2577\n\u2502 Error: Migrating state from Terraform Cloud to another backend is not yet implemented.\n</code></pre> <p>Manually download the state from Terraform Cloud and upload to the bucket</p> Bash<pre><code>tofu state pull &gt; temp.tfstate\nrm -rf .terraform\n\n# change the backend from cloud (or remote) to s3\n\ntofu init\ntofu state push temp.tfstate\nrm temp.tfstate\ntofu plan\n</code></pre>"},{"location":"services/critical0/terraform-modules/","title":"Terraform modules","text":"<ul> <li>Ricerca dei terraform modules interni: https://github.com/search?q=topic%3Aterraform-module+org%3Acriticalcase&amp;type=repositories</li> <li>Ricerca dei repository che usano il terraform module: https://github.com/search?q=criticalcase%2Fterraform-aws-db-instance+org%3Acriticalcase+path%3A*.tf&amp;type=code</li> </ul>"},{"location":"services/critical0/vmware-vdc/","title":"VDC: Packer + Terraform","text":"<p>Criticalcase's private cloud Packer + Terraform deployment. GH Repo: https://github.com/criticalcase/packer-template</p> <p>La VM GHAR deve aver accesso:</p> <ul> <li>Alle API del VCenter</li> <li>Ad una rete privata di un cliente</li> </ul> <p>Su questa VM girer\u00e0 un tool chiamato Github Actions Runner, che ad ogni modifica del codice su git eseguir\u00e0:</p> <ul> <li>Terraform: per creare/modificare VM (dischi, rete, RAM, CPU). Richiede l\u2019accesso all\u2019API del vCenter.</li> <li>Packer: per creare una VM template con le ultime versioni del software. Richiede l\u2019accesso all\u2019API del vCenter + SSH nella rete del cliente (solo connessioni in uscita)</li> </ul> <p></p>"},{"location":"services/datadog/","title":"Datadog","text":"<p>Best practices</p> Bash<pre><code># Set default log level to warning\nsed -i \"s@# log_level: 'info'@log_level: 'warn'@\" /etc/datadog-agent/datadog.yaml\nsystemctl restart datadog-agent\n\n# Disable warning for docker mountpoint\ncp /etc/datadog-agent/conf.d/disk.d/conf.yaml.default /etc/datadog-agent/conf.d/disk.d/conf.yaml\nsed -i 's/# mount_point_exclude/mount_point_exclude/' -i /etc/datadog-agent/conf.d/disk.d/conf.yaml\nsed -i \"s@#   - /proc/sys/fs/binfmt_misc@    - /run/docker/.*\\n        - /var/lib/docker/.*@\" -i /etc/datadog-agent/conf.d/disk.d/conf.yaml\nsystemctl restart datadog-agent\n</code></pre>"},{"location":"services/datadog/#php-tracer","title":"PHP Tracer","text":"Bash<pre><code># php-cli\nDD_TRACE_CLI_ENABLED=true DD_SERVICE=import-files bin/magento buffetti:import:files\n\n# php-fpm pool tracing\nenv[DD_SERVICE] = 'service-name-in-datadog'\nenv[DD_ENV] = 'environment-in-datadog'\n\n# php-fpm pool profiling\n;env[DD_PROFILING_ENABLED] = true\n;env[DD_PROFILING_EXPERIMENTAL_CPU_TIME_ENABLED] = true\n\n# nginx\nfastcgi_param DD_TRACE_ENABLED true;\nfastcgi_param DD_TRACE_ANALYTICS_ENABLED true;\n</code></pre>"},{"location":"services/datadog/Kubernetes/","title":"Datadog Kubernetes","text":""},{"location":"services/datadog/Kubernetes/#datadog-operator","title":"Datadog operator","text":"<p>Install: https://app.datadoghq.eu/account/settings/agent/latest?platform=kubernetes</p> Bash<pre><code>helm repo add datadog https://helm.datadoghq.com\nhelm install datadog-operator datadog/datadog-operator\nkubectl create secret generic datadog-secret --from-literal api-key=XXXX\n</code></pre> <p>Operator Agent configuration</p> YAML<pre><code>apiVersion: datadoghq.com/v2alpha1\nkind: DatadogAgent\nmetadata:\n  name: datadog\nspec:\n  global:\n    clusterName: libellula-c1\n    site: datadoghq.eu\n    kubelet:\n      tlsVerify: false\n    credentials:\n      apiSecret:\n        secretName: datadog-secret\n        keyName: api-key\n  override:\n    nodeAgent:\n      tolerations:\n        - operator: Exists\n  features:\n    apm:\n      enabled: true\n    eventCollection:\n      collectKubernetesEvents: true\n    logCollection:\n      enabled: true\n      containerCollectAll: true\n    liveProcessCollection:\n      enabled: true\n    liveContainerCollection:\n      enabled: true\n</code></pre>"},{"location":"services/datadog/Kubernetes/#pyhton-pod-intstrumentation","title":"Pyhton pod intstrumentation","text":"<p>Deployment</p> YAML<pre><code>annotations:\n  admission.datadoghq.com/python-lib.version: v2.6.3\nlabels:\n  admission.datadoghq.com/enabled: \"true\"\n  tags.datadoghq.com/env: c4\n  tags.datadoghq.com/service: sequa-cron\n  tags.datadoghq.com/version: v1\n\n  env:\n    - name: DD_PROFILING_ENABLED\n      value: \"true\"\n    - name: DD_LOGS_INJECTION\n      value: \"true\"\n    - name: DD_TRACE_SAMPLE_RATE\n      value: \"1\"\n</code></pre>"},{"location":"services/elastic/","title":"Elastic","text":""},{"location":"services/elastic/#elasticsearch-7x-installation","title":"Elasticsearch 7.x installation","text":""},{"location":"services/elastic/#generate-secrets-from-your-laptop","title":"Generate secrets from your laptop","text":"Bash<pre><code>kubie ctx kooomo-c2\nkns ns elastic1\ngit clone -b v7.15.0 git@github.com:elastic/helm-charts.git /tmp/elastic-helm-charts\ncd /tmp/elastic-helm-charts\ncd /tmp/elastic-helm-charts/elasticsearch/examples/security\nmake secrets\ncd /tmp/elastic-helm-charts/kibana/examples/security\nmake secrets\n</code></pre>"},{"location":"services/elastic/#aws-waf","title":"AWS WAF","text":""},{"location":"services/elastic/#create-the-helmrelease","title":"Create the HelmRelease","text":"<p>Sample configuration file</p> Text Only<pre><code>---\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: logstash\n\nspec:\n  releaseName: logstash\n\n  values:\n    envFrom:\n      - secretRef:\n          name: logstash-s3-credentials\n\n    logstashPipeline:\n      input.conf: |\n        input {\n          s3 {\n              \"access_key_id\" =&gt; \"${S3_ACCESS_KEY}\"\n              \"secret_access_key\" =&gt; \"${S3_SECRET_KEY}\"\n              \"bucket\" =&gt; \"${S3_BUCKET}\"\n              \"region\" =&gt; \"us-east-1\"\n\n              \"codec\" =&gt; \"json\"\n\n              \"additional_settings\" =&gt; {\n                  \"force_path_style\" =&gt; true\n                  \"follow_redirects\" =&gt; false\n              }\n          }\n        }\n\n      elasticsearch.conf: |\n        output { elasticsearch {\n          hosts =&gt; [\"https://security-master:9200\"]\n          cacert =&gt; \"/usr/share/logstash/config/certs/elastic-certificate.crt\"\n          user =&gt; '${ELASTICSEARCH_USERNAME}'\n          password =&gt; '${ELASTICSEARCH_PASSWORD}'\n          index =&gt; \"awswaf-%{+YYYY.MM.dd}\"\n          }\n        }\n</code></pre> <p>Sample secret file</p> Text Only<pre><code>---\napiVersion: v1\ndata:\n  S3_ACCESS_KEY: XXXX\n  S3_SECRET_KEY: YYYY\n  S3_BUCKET: BUCKETNAME\nkind: Secret\nmetadata:\n  name: logstash-s3-credentials\n  namespace: elastic1\ntype: Opaque\n</code></pre>"},{"location":"services/elastic/#import-the-dashboard","title":"Import the dashboard","text":"<p>Import dashboard data: awswaf.ndjson</p>"},{"location":"services/elastic/#set-from-kibana-dev-tools","title":"Set from Kibana: Dev Tools","text":"<p>Create the AWS waf policy</p> JSON<pre><code>PUT _ilm/policy/awswaf-policy\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {}\n      },\n      \"delete\": {\n        \"min_age\": \"7d\",\n        \"actions\": {\n          \"delete\": {\n            \"delete_searchable_snapshot\": true\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Set the policy to existing index</p> JSON<pre><code>PUT /awswaf-*/_settings?pretty\n{\n  \"lifecycle\": {\n    \"name\": \"awswaf-policy\"\n  }\n}\n</code></pre> <p>Set some template parameters</p> JSON<pre><code>PUT _template/awswaf-logs\n{\n  \"settings\": {\n    \"index.lifecycle.name\": \"awswaf-policy\"\n  },\n  \"index_patterns\": [\"awswaf-*\"],\n  \"mappings\": {\n    \"properties\" : {\n      \"httpRequest\" : {\n        \"properties\" : {\n          \"clientIp\" : {\n            \"type\" : \"keyword\",\n            \"ignore_above\": 256,\n            \"fields\" : {\n              \"keyword\" : {\n              \"type\" : \"ip\"\n              }\n            }\n          }\n        }\n      },\n      \"timestamp\" : {\n        \"type\" : \"date\",\n        \"format\" : \"epoch_millis\"\n      }\n    }\n  }\n}\n</code></pre> <p>Set global parameter to ignore last 3m: Stack Management &gt; Advanced Settings <code>/app/management/kibana/settings</code></p> <p>Time filter defaults</p> JSON<pre><code>{\n  \"from\": \"now-1h\",\n  \"to\": \"now-3m\"\n}\n</code></pre>"},{"location":"services/elastic/#create-role","title":"Create role","text":"<ul> <li>https://kibana-elastic2-ccd-c2.rz1.criticalcase.cloud/app/management/security/roles/edit</li> </ul> Text Only<pre><code>Name: awswaf-CUSTOMERNAME\nindices: awswaf-CUSTOMERNAME-*; read\nKibana: Spaces Default; Analytics &gt; Dashboard: read; Analytics &gt; Discover: read;\n</code></pre>"},{"location":"services/elastic/#create-user","title":"Create user","text":"<p>https://kibana-elastic2-ccd-c2.rz1.criticalcase.cloud/app/management/security/users/create</p> Text Only<pre><code>Privileges: awswaf-CUSTOMERNAME\n</code></pre>"},{"location":"services/elastic/#useful-commands","title":"Useful commands","text":"Bash<pre><code># Show elastisearch status\n$ curl http://127.0.0.1:9200/_cat/health\n\n# cluster.max_shards_per_node\n$ curl -XPUT localhost:9200/_cluster/settings -H 'Content-type: application/json' --data-binary $'{\"transient\":{\"cluster.max_shards_per_node\":5000}}'\n\n# search.max_buckets\n$ curl -XPUT http://localhost:9200/_cluster/settings -H 'Content-type: application/json' --data-binary $'{\"transient\":{\"search.max_buckets\":100000}}'\n</code></pre>"},{"location":"services/erp/","title":"ERP","text":""},{"location":"services/erp/#workflow","title":"Workflow","text":"<ol> <li>After a push in a repository, a GitHub Action creates an image and pushes it to Amazon ECR.</li> <li>Thanks to webhooks, the Flux Notification Controller Receiver gets notified for a change in the git repository.</li> <li>The Flux Image Reflector Controller fetches tags of the last image on Amazon ECR and it checks if tag satisfies the Image Policy.</li> <li>If the tag is valid, the Flux Image Automation Controller updates the image tag on <code>erp-k8s/overlays/prod/kustomization.yaml</code> and pushes changes to GitHub.</li> <li>The Kustomize Controller then produces the updated yaml files, it pulls the images from Amazon ECR and then applies them to the K8s cluster.</li> </ol>"},{"location":"services/firewalls/PALOALTO/","title":"Paloalto","text":"<p>Cercare tutte le regole disabilitate: <code>(disabled eq yes)</code></p>"},{"location":"services/firewalls/PALOALTO/#upgrade-procedure","title":"Upgrade procedure","text":"<ul> <li> <p>Controllare che i firewall abbiano lo stesso content release version &gt; Device &gt; Dynamic Updates</p> </li> <li> <p>If you want to test that HA is functioning properly before the upgrade, consider upgrading the active peer in an active/passive configuration first to ensure that failover occurs without incident.</p> </li> </ul> <p>Riavviare il master   Riavviare il nuovo master</p> <ul> <li>Cancellare l'OS 8.0.1 da entrambi i firewall</li> <li> <p>Scaricare OS 8.0.17 su entrambi i firewall</p> </li> <li> <p>Export named configuration for each firewall: Device &gt; Setup &gt; Operations and click Export named configuration snapshot: running-config.xml</p> </li> <li> <p>Disable preemption on the first peer in each pair. You only need to disable this setting on one firewall in the HA pair but ensure that the commit is successful before you proceed with the upgrade.</p> </li> </ul> <p>Select Device &gt; High Availability and edit the Election Settings.   If enabled, disable (clear) the Preemptive setting and click OK.</p> <ul> <li>Installare OS 8.0.17 sul passive peer</li> <li>Suspend the active peer (fail over): Device &gt; High Availability &gt; Operational Commands and click Suspend local device.   View the High Availability widget on the Dashboard and verify that the state changes to Passive.</li> <li>Update suspended peer</li> <li>From the CLI of the peer you just upgraded, run the following command to make the firewall functional again:</li> </ul> <p>request high-availability state functional</p> <p>Run the following CLI commands to confirm that the upgrade succeeded:</p> Text Only<pre><code>show session all\nshow high-availability interface ha2\n</code></pre> <ul> <li>Scaricare OS 8.1.0 su entrambi i firewall</li> <li>Installare OS 8.1.0 sul passive peer</li> <li>Suspend the active peer (fail over): Device &gt; High Availability &gt; Operational Commands and click Suspend local device.   View the High Availability widget on the Dashboard and verify that the state changes to Passive.</li> <li>Update suspended peer</li> <li>From the CLI of the peer you just upgraded, run the following command to make the firewall functional again:</li> </ul> <p>request high-availability state functional</p> <p>Run the following CLI commands to confirm that the upgrade succeeded:</p> Text Only<pre><code>show session all\nshow high-availability interface ha2\n</code></pre> <ul> <li>Scaricare OS 8.1.7 su entrambi i firewall</li> <li>Installare OS 8.1.7 sul passive peer e confermare il riavvio</li> <li>Suspend the active peer (fail over): Device &gt; High Availability &gt; Operational Commands and click Suspend local device.   View the High Availability widget on the Dashboard and verify that the state changes to Passive.</li> <li>Update suspended peer</li> <li>From the CLI of the peer you just upgraded, run the following command to make the firewall functional again:</li> </ul> <p>request high-availability state functional</p> <p>Run the following CLI commands to confirm that the upgrade succeeded:</p> Text Only<pre><code>show session all\nshow high-availability interface ha2\n</code></pre>"},{"location":"services/firewalls/PALOALTO/#globalprotect-disable-unsafe-ciphhers","title":"Globalprotect disable unsafe ciphhers","text":"Text Only<pre><code>PA-820(active)# set shared ssl-tls-service-profile \"SSL VPN Profile\" protocol-settings min-version tls1-2\nPA-820(active)# set shared ssl-tls-service-profile \"SSL VPN Profile\" protocol-settings enc-algo-aes-128-cbc no\nPA-820(active)# set shared ssl-tls-service-profile \"SSL VPN Profile\" protocol-settings enc-algo-aes-128-gcm no\n</code></pre>"},{"location":"services/foreman/","title":"Foreman","text":"<p>Foreman pu\u00f2 essere usato per creare VM interne. Nessun cliente deve avere accesso SSH a queste VMs.</p>"},{"location":"services/foreman/#creare-una-vm","title":"Creare una VM","text":"<p>Andare su https://foreman-int.criticalcase.com/hosts/new</p> <ul> <li> <p>Mettere il nome (deve iniziare con ccd-)</p> </li> <li> <p>Selezionare Default Organization</p> </li> <li> <p>Selezionare Default Location</p> </li> <li> <p>Selezionare <code>Hostgroup</code> &gt; <code>cc.local/rz2/digital</code></p> </li> </ul> <p>La maggior parte dei campi sono precompilati, non modificarli se non sai esattamente costa stai facendo.</p>"},{"location":"services/foreman/#macchina-virtuale","title":"Macchina virtuale","text":"<ul> <li> <p><code>CPU/Core per socket</code>: il numero di CPU deve essere uguale al core per socket. Se devo creare una VM con 16 CPU dovr\u00f2 mettere 16 in entrambi i campi</p> </li> <li> <p><code>Memoria (MB)</code>: deve essere un multiplo di 1024, altrimenti il deploy da errore.</p> </li> <li> <p><code>Storage</code> &gt; <code>Dimensione</code>: impostare la dimensione desiderata.</p> </li> </ul> <p>Attenzione: Non cambiare nessun altro campo da questa pagina. Quale Archivio dati utilizzare viene gi' settato nel Default dell'Hostgroup.</p>"},{"location":"services/foreman/#sistema-opreativo","title":"Sistema opreativo","text":"<p>Per tutte le macchine usiamo <code>Ubuntu 18.04</code>. Non usare altri sistemi operativi poich\u00e9 alcune integrazioni potrebbero non funzionare correttamente</p>"},{"location":"services/foreman/#interfacce","title":"Interfacce","text":"<p>L'indirizzo ip viene assegnato automaticamente e non deve essere modificato.</p> <p>Attenzione: La sezione relativa \u00e8 quella nel tab interfacce non quella dentro Macchina Virtuale</p>"},{"location":"services/foreman/#known-issues","title":"Known issues","text":""},{"location":"services/foreman/#request-failed-with-status-code-500","title":"Request Failed With Status Code 500","text":"<p>The cache data is corrupted, go to the compute resources https://foreman-int.criticalcase.com/compute_resources/6-RZ1-ISCC2-VCCC2 and click on <code>Refresh cache</code></p>"},{"location":"services/gcp/GCP/","title":"GCP","text":""},{"location":"services/gcp/GCP/#report-consumi","title":"Report consumi:","text":"<ul> <li>Magloo</li> <li>Comau</li> </ul>"},{"location":"services/gcp/GCP/#td-synnex","title":"TD Synnex","text":"<ul> <li>Andare alla piattaforma Ion: https://ion.tdsynnex.com/v2/customer360/</li> <li>[NEW CUSTOMER] Customers \u2192 Add \u2192 Add user: https://ion.tdsynnex.com/v2/customer360/details/new/overview</li> <li>Cloud billing \u2192 Cloud providers \u2192 Add: https://ion.tdsynnex.com/v2/customer360/details/16559/cloudProviders</li> <li>(GCP, master pricebook, no enable provider)</li> <li>Customers \u2192 customer creato \u2192 products (left) \u2192 buy products \u2192 google</li> <li>Completare la procedura di acquisto, inserendo come customer domain il dominio del billing account (ad esempio per la nostra era criticalcase.com) e come email della contact information la stessa usata come responsabile fatturazione della sottoscrizione GCP. Completare con \u201cSkip &amp; Complete\u201d e poi close \u2192 place order</li> </ul>"},{"location":"services/gcp/GCP/#modo-1","title":"Modo 1","text":"<p>Criticalcase modifica la fatturazione degli account del cliente.</p>"},{"location":"services/gcp/GCP/#cliente-dare-i-permessi-dallaccount-account-cliente","title":"[Cliente] Dare i permessi dall'account Account cliente","text":"<ul> <li>Andare sull'account GCP del cliente e selezionare IAM</li> <li>Selezionare il progetto (in alto a sinistra)</li> <li>Premere su Grant Access</li> <li>New principals: <code>gcp-billing@gcp.criticalcase.com</code></li> <li>Assign roles: - Project Billing Manager</li> <li>Basic \u2192 Viewer</li> <li>Premere su Save</li> </ul>"},{"location":"services/gcp/GCP/#criticalcase-modificare-la-fatturazione-da-gcpcriticalcasecom","title":"[Criticalcase] Modificare la fatturazione da gcp.criticalcase.com","text":"<ul> <li>Andare sull'account gcp.criticalcase.com &gt; Billing &gt; My Projects</li> <li>Selezionare (con i tre puntini) il progetto desiderato: Change Billing</li> <li>Billing Account: C Nomecliente</li> <li>Premere: Set account</li> </ul>"},{"location":"services/gcp/GCP/#modo-2","title":"Modo 2","text":"<p>Il cliente modifica l'account di fatturazione con uno di Criticalcase</p>"},{"location":"services/gcp/GCP/#criticalcase-dare-i-permessi-dallaccount-gcp-di-criticalcase","title":"[Criticalcase] Dare i permessi dall'account GCP di criticalcase","text":"<ul> <li>Andare su GCP di criticalcase \u2192 Billing</li> <li>Selezionare l'account di fatturazione appena creato da ION: es. C Criticalcase Srl</li> <li>Andare su Account management: es Criticalcase account management</li> <li>Add principal:</li> <li>New principals: utente o dominio del cliente che potr\u00e0 selezionare il profilo di fatturazione es. info@mycompany.com o mycompany.com</li> <li>Assign roles:<ul> <li>Billing Account User</li> <li>Billing Account Viewer</li> <li>Consumer Procurement Administrator (necessario solo per attivare il supporto GCP a pagamento)</li> </ul> </li> </ul>"},{"location":"services/gcp/GCP/#cliente-impostare-il-profilo-fatturazione-dallaccount-cliente","title":"[Cliente] Impostare il profilo fatturazione dall'account cliente","text":"<ul> <li>Andare su GCP del cliente \u2192 Billing \u2192 My Projects (Se non vedo i progetti selezionare l'organizzazione)</li> <li>Selezionare (con i tre puntini) il progetto desiderato: Change Billing</li> <li>Billing Account: C Nomecliente</li> <li>Premere: Set account</li> </ul>"},{"location":"services/ito/","title":"ITO","text":""},{"location":"services/ito/#restore-snapshot-policy","title":"Restore snapshot policy","text":"<p>In caso di richiesta snapshot o ripristino di VM tramite veeam/nakivo ingaggiare criticalservice o Simone.</p>"},{"location":"services/ito/#polly","title":"Polly","text":"Text Only<pre><code># Bianca, Female\nBenvenuti nel Servizio, ITO, di Critical Cheis. Rimanete in linea, grazie.\n\n# Joanna, Female\nWelcome to I.T.O. IT Operation Service by Criticalcase. Hold the line, Thank you.\n\n# Bianca, Female\nServizio di reperibilit\u00e0, ITO, di Critical Cheis. Rimanete in attesa per essere collegati con un operatore.\n</code></pre>"},{"location":"services/ito/Reporting/","title":"Onboarding procedure for reporting","text":"<p>Send to the new customer an email with the following content in PDF and Zabbix user and password. Pingdom configuration is for internal purpose, send to the customer only the info section</p>"},{"location":"services/ito/Reporting/#to-be-sent-to-the-customer","title":"To be sent to the customer","text":""},{"location":"services/ito/Reporting/#zabbix","title":"Zabbix","text":"<p>Criticalcase ITO serverce uses Zabbix for Virtual Machines monitoring</p>"},{"location":"services/ito/Reporting/#zabbix-montly-pdf-report","title":"Zabbix montly PDF report","text":"<p>By default the customer will receive via email a monthly report automatically generated by Criticalcase, this report will contain all the graphs related to the IaaS environment</p>"},{"location":"services/ito/Reporting/#zabbix-custom-pdf-report-generator","title":"Zabbix custom PDF report generator","text":"<p>Zabbix can generate PDF report dynamically based on the custom defined metrics. If required the customer can create new different time repos:</p> <ol> <li>Click on the link below:     https://monitoring.rz2.criticalcase.cloud/zabbix-pdf-report/chooser.php</li> <li>Log in into the system using the username and pwd provided by Criticalcase</li> <li>According to your needs, the report generator is customizable by changing the metrics and you can generate report based on different periods of time.</li> <li>For complete report, replace the default metrics with metrics below:</li> </ol> Text Only<pre><code>#(Ping|System load|CPU jumps|Disk utilization and queue|Disk average\nwaiting time|Disk read/write rates|CPU load|CPU usage|CPU utilization|\nProcesses|Disk space|:/Disk space usage|Swap|Ethernet|Memory utilization|\nMemory usage|Network traffic|^Traffic on |traffic on eth)#\n</code></pre> <p></p>"},{"location":"services/ito/Reporting/#how-to-see-graph-reports-directly-on-zabbix-console","title":"How to see Graph reports directly on Zabbix console:","text":"<ol> <li>Log in to the address below by your provisioned username and password:    https://monitoring.rz2.criticalcase.cloud/</li> <li>In the left pane side, click on the Monitoring &gt; Hosts</li> <li>Click on one of the Hosts, then select Graphs </li> </ol>"},{"location":"services/ito/Reporting/#pingdom","title":"Pingdom","text":"<p>Pingdom is used by ITO for the HTTP/HTTPS external application monitoring</p> <p>The Customer will receive a Monthly report regarting the Uptime in this period, like following</p> <p></p>"},{"location":"services/ito/Reporting/#jira-service-management","title":"Jira Service Management","text":"<p>Jira Service Management is used by Criticalcase to track, manage, and resolve customers ITO requests.</p>"},{"location":"services/ito/Reporting/#how-to-get-a-tickets-report","title":"How to get a tickets report","text":"<p>Just click on the following link and login with username and passowrd https://criticalcase.atlassian.net/servicedesk/customer/user/requests?page=1&amp;reporter=all</p> <p>The otput will be similar to the following</p> <p></p>"},{"location":"services/ito/Reporting/#criticalcase-enabling-procedures","title":"Criticalcase enabling procedures","text":""},{"location":"services/ito/Reporting/#zabbix_1","title":"Zabbix","text":"<p>After deploying infrastructural component, create a new user and password for the customer based on the contacts provided</p>"},{"location":"services/ito/Reporting/#pingdom_1","title":"Pingdom","text":"<p>After creating the alarms as on pingdom</p>"},{"location":"services/ito/Reporting/#setup-a-new-report-if-not-requred-use-monthly","title":"Setup a new report (if not requred use monthly)","text":"<ol> <li>log in to the address below:    https://my.pingdom.com/app</li> <li>In the left side of panel, click on the Synthetic &gt; Email Reports</li> <li>In the right pane side click on Add email report according to your needs, we can choose the Page Speed or Uptime in different time periods.    </li> </ol>"},{"location":"services/ito/Reporting/#jira-service-management_1","title":"Jira Service Management","text":"<p>After creating customer credentials as on: jira</p> <p>If we need to send a manual report to the customer:</p> <ol> <li>Log in to the Link below: <p>https://criticalcase.atlassian.net/</p> </li> <li>Click on the Projects &gt; Recent &gt; Criticalcase IT Operations(ITO)</li> </ol> <p></p> <ol> <li> <p>In case of getting report for considered customer, it should be set some filterations:</p> </li> <li> <p>Click on Filters &gt; Advanced issue search</p> </li> </ol> <p></p> <ul> <li>click on +More and select Reporter and Updated Date</li> </ul> <p></p> <ol> <li>Now, in the Reporter, the customer should be selected and in the Updated Date, the time range should be set.</li> <li>Finally, the report could be sent to customer through sendin to customer email by clicking on the Share or Export as a file and then send to the customer by clicking on the Export</li> </ol> <p></p>"},{"location":"services/ito/Zabbix_sending_report_automation/","title":"How to update the new customer email list","text":""},{"location":"services/ito/Zabbix_sending_report_automation/#automation-architecture","title":"Automation architecture","text":"<ol> <li>log in to the link below :</li> </ol> <p>https://monitoring.rz2.criticalcase.cloud/zabbix-pdf-report/index.php</p> <ol> <li>select the Host Group and tick the Config under the Generate button.</li> </ol> <p></p> <ol> <li>In the first line of generated log, we can see the GroupID.</li> </ol> <p></p> <ol> <li>Clone the digital-solutin repo from the GitHub link below:</li> </ol> <p>git clone https://github.com/criticalcase/digital-solution.git</p> <ol> <li> <p>Open the cloned repo in your editor and route to the following Path ===&gt; digital-solution/terraform/prd/lambda/zabbix_report and open the main.tf</p> </li> <li> <p>there is a module \"events_rule\" which each of them could handle five customers email. we can add more module \"events_rule\" when reach to the five.</p> </li> <li> <p>we should add this template for each customer and change GroupID and Recipients. Moreover, it is possible to change timePeriod.    </p> </li> <li> <p>then, push to the GitHub repo and log in to the terraform cloud and deploy it.</p> <p>https://app.terraform.io/session</p> </li> </ol>"},{"location":"services/ito/offboarding/","title":"Offboarding customer","text":""},{"location":"services/ito/offboarding/#k3s-bastion","title":"K3s bastion","text":"<p>If the customer has a K3s bastion managed by flux for teleport and zabbix:</p> <ul> <li>remove the customer folder from the flux-k3s-bastion repository https://github.com/criticalcase/flux-k3s-bastions/tree/main/customers</li> <li>remove the key used by flux from the repository https://github.com/criticalcase/flux-k3s-bastions/settings/keys</li> <li> <p>remove the teleport cluster form <code>cc-ds1.criticalcasecloud.com</code> and <code>cc-ds2.criticalcasecloud.com</code></p> Bash<pre><code>tctl rm rc/CUSTOMER_TELEPORT_URL\n</code></pre> </li> <li> <p>remove the zabbix proxy from zabbix.</p> </li> </ul>"},{"location":"services/ito/onboarding/","title":"Onboarding new customer","text":""},{"location":"services/ito/onboarding/#leaf","title":"Leaf","text":"<p>Create the customer main page on leaf, <code>/customers/customername_CXXX/CUSTOMERNAME</code> following this template</p>"},{"location":"services/ito/onboarding/#monitoring","title":"Monitoring","text":"<p>Add the VMs on ITO to the monitoring system following: customer monitoring</p>"},{"location":"services/ito/onboarding/#jira","title":"Jira","text":"<ul> <li>Ask to the customer the unique email address for JIRA: welcome email</li> <li>When you have the email address add it to JIRA: add customer to jira</li> <li>Ask the customer to do the first login: first login template</li> </ul>"},{"location":"services/ito/onboarding/#credentials","title":"Credentials","text":"<ul> <li>Add customers credentials on vault</li> <li>For linux VMs install teleport</li> <li>For AWS customer follow this guide</li> <li>For Azure customer follow this guide</li> </ul>"},{"location":"services/ito/onboarding/#backup","title":"Backup","text":"<ul> <li>Policy di default: daily 7, monthly 2.</li> </ul>"},{"location":"services/ito/onboarding/#reporting","title":"Reporting","text":"<ul> <li>Follow the Reporting section</li> </ul>"},{"location":"services/ito/onboarding/#ito-dashboard","title":"ITO Dashboard","text":"<ul> <li>Add all VMs to SSM</li> </ul>"},{"location":"services/jira/","title":"Jira","text":"<ul> <li>Admin access: https://criticalcase.atlassian.net</li> <li>Customer access: https://criticalcase.atlassian.net/servicedesk</li> </ul>"},{"location":"services/jira/#add-customer","title":"Add customer","text":"<p>Send to the customer the welcome e-mail</p> <ul> <li><code>Projects</code> &gt; <code>Criticalcase IT Operations</code> &gt; Channels &amp; PEOPLE &gt; Customers: https://criticalcase.atlassian.net/jira/servicedesk/projects/ITO/customers</li> <li>Click on <code>Add customers</code></li> <li> <p>Customer emails: <code>customer@domain.com</code></p> </li> <li> <p><code>Products</code> &gt; <code>SITES AND PRODUCTS</code> &gt; <code>criticalcase site</code> &gt; <code>USER MANAGEMENT</code> &gt; <code>Jira Service Management</code>: https://admin.atlassian.com/s/d028417e-c867-47de-8bf0-b9726024d2d3/jira-service-desk/portal-only-customers</p> </li> <li>Find customer and click on <code>Edit full name</code></li> <li>Full name <code>CustomerName CXXXX SXXX</code></li> </ul> <p><code>SXXX</code> is needed only for customers billed by Criticalcase SAGL (C1362)</p>"},{"location":"services/jira/#email-not-received-by-the-customer","title":"Email not received by the customer","text":"<p>In order to resend the invite you need to delete the customer account from the link above and re-invite the customer to your project.</p>"},{"location":"services/jira/#create-ticket-as-customer","title":"Create ticket as customer","text":"<p>The right procedure to create a customer's ticket is to Raise a request: https://criticalcase.atlassian.net/servicedesk/customer/portal/2/create/16</p> <p></p> <p>If you use the Create button from admin panel you have to set the right issue type: <code>Technical support</code></p> <p></p> <p></p>"},{"location":"services/jira/#ticket-openend-on-the-wrong-platform","title":"Ticket openend on the wrong platform","text":"<p>This is a draft, do not use yet</p> <p>Il ticket sembra fare riferimento ad un contratto in ITO (IT Operations). Per ricevere supporto la segnalazione deve essere aperta al seguente link: https://criticalcase.atlassian.net/servicedesk/customer/portal/2</p> <p>In caso di problema bloccante e urgente ad un servizio ITO. (IT Operations) \u00e8 sempre necessario chiamare il numero di telefono +39 0110888761.</p> <p>Se dovesse essere visualizzato il sguente messaggio \u00e8 necessario fare il login con l'indirizzo email fornito in fase di attivazione del servizio ITO:</p> Text Only<pre><code>Nessun accesso\nNon disponi dell'autorizzazione necessaria per visualizzare questo portale.\n</code></pre> <p>Il ticket su questa piattaforma verr\u00e0 chiuso</p> <p>Grazie e cordiali saluti</p>"},{"location":"services/kubify/","title":"Kubify","text":"<p>Kubify \u00e8 la distribuzione kubernetes gestita di Criticalcase.</p> <p>E' composta da:</p> <ul> <li>Gestione del cluster tramite flux/k</li> <li> <p>Gestione dei servizi in SaaS tramite i repository kubify:</p> </li> <li> <p>kubify-c1</p> </li> <li>kubify-c3</li> </ul>"},{"location":"services/kubify/#links","title":"Links","text":"<ul> <li>cluster</li> <li>helm</li> <li>k3s</li> <li>kubeadm</li> <li>network</li> <li>network</li> </ul>"},{"location":"services/kubify/#todo","title":"ToDo","text":"<ul> <li>Kubelet --serialize-image-pulls false: https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/</li> <li>Sealed all secrets</li> <li>Migrate to Flux V2</li> <li>Create user to manage the cluster (not service account)</li> <li>Deploy goldpinger</li> </ul>"},{"location":"services/kubify/cluster/","title":"VMWare","text":"<p>Abilita il <code>Mac Address Change</code> su VMWare!</p>"},{"location":"services/kubify/cluster/#setup","title":"SETUP","text":""},{"location":"services/kubify/cluster/#setup-gw","title":"Setup GW","text":"Text Only<pre><code>route delete default gw 10.101.0.1 eth0; ip route add default via 10.12.0.1 dev eth1\n</code></pre>"},{"location":"services/kubify/cluster/#init-cluster","title":"Init cluster","text":"Text Only<pre><code>kubeadm init --apiserver-advertise-address=10.12.0.100 --service-cidr=10.112.0.0/12 --pod-network-cidr=192.168.0.0/16 --apiserver-cert-extra-sans=176.221.52.181,ck8s-cl01.ccws.it\n</code></pre>"},{"location":"services/kubify/cluster/#setup-internal-ip","title":"Setup internal IP","text":"Text Only<pre><code>vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n</code></pre> <p>Add before the last 2 lines:</p> Text Only<pre><code>Environment=\"KUBELET_EXTRA_ARGS=--node-ip=10.12.0.100 --authentication-token-webhook --authorization-mode=Webhook\"\n</code></pre> <ul> <li>Node IP: current node ip address</li> <li>Prometheus: authentication-token-webhook authorization-mode=Webhook</li> </ul> <p>Change DNS Environment</p> Text Only<pre><code>Environment=\"KUBELET_DNS_ARGS=--cluster-dns=10.112.0.10 --cluster-domain=cluster.local\"\n</code></pre> <ul> <li> <p>Cluster DNS:</p> <p>--cluster-dns=10.112.0.10</p> </li> </ul> <p>Reload</p> Text Only<pre><code>systemctl daemon-reload\nsystemctl restart kubelet\n</code></pre>"},{"location":"services/kubify/cluster/#copy-configuration","title":"Copy configuration","text":"Text Only<pre><code>mkdir -p /root/.kube/; cp -i /etc/kubernetes/admin.conf /root/.kube/config\n</code></pre>"},{"location":"services/kubify/cluster/#setup-calico","title":"Setup Calico","text":"Text Only<pre><code>kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml\nkubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml\n</code></pre> <p>Force to use eth1 (10.12.0.0/24) for pod networking</p> Text Only<pre><code>kubectl -n kube-system set env daemonset calico-node IP_AUTODETECTION_METHOD=interface=eth1\n</code></pre> <p>Other examples</p> Text Only<pre><code>IP_AUTODETECTION_METHOD=first-found (Default)\nIP_AUTODETECTION_METHOD=can-reach=10.12.0.1 (To check)\n</code></pre>"},{"location":"services/kubify/cluster/#deploy-dashboard","title":"Deploy dashboard","text":"Text Only<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml\n</code></pre>"},{"location":"services/kubify/cluster/#create-admin-user","title":"Create admin user","text":"Text Only<pre><code>kubectl apply -f yaml/admin.user.yaml\nkubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')\n</code></pre> <p>Docs: https://github.com/kubernetes/dashboard/wiki/Creating-sample-user</p>"},{"location":"services/kubify/cluster/#accessing-dashboard","title":"Accessing Dashboard","text":"Text Only<pre><code>kubectl proxy\n</code></pre> <p>Open: <code>http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</code></p>"},{"location":"services/kubify/cluster/#installing-helm","title":"Installing helm","text":"Text Only<pre><code>kubectl create serviceaccount -n kube-system tiller\nkubectl create clusterrolebinding tiller-binding --clusterrole=cluster-admin --serviceaccount kube-system:tiller\n\nhelm init --upgrade --service-account tiller --node-selectors=\"type=control\"\n</code></pre>"},{"location":"services/kubify/cluster/#install-nginx","title":"Install nginx","text":"Text Only<pre><code>helm install --name nginx-ingress stable/nginx-ingress --namespace=kube-ingress --set rbac.create=true --set controller.service.type=NodePort --set controller.livenessProbe.timeoutSeconds=5 --set controller.readinessProbe.timeoutSeconds=1 --set controller.stats.enabled=true --set controller.metrics.enabled=true --set controller.service.externalTrafficPolicy=Local --set controller.kind=DaemonSet --set controller.daemonset.useHostPort=true --set controller.nodeSelector.\"type\"=control --set defaultBackend.nodeSelector.\"type\"=control\n</code></pre> <p>Upgrade nginx using helm reusing values</p> Text Only<pre><code>helm upgrade --reuse-values nginx-ingress stable/nginx-ingress\n</code></pre> <p>Delete all</p> Text Only<pre><code>helm del --purge nginx-ingress;\n</code></pre>"},{"location":"services/kubify/cluster/#install-certmanager","title":"Install CertManager","text":"Text Only<pre><code>helm install --name cert-manager --namespace kube-system stable/cert-manager --set nodeSelector.\"type\"=control\n</code></pre>"},{"location":"services/kubify/cluster/#rancher2","title":"Rancher2","text":"Text Only<pre><code>kubectl apply -f https://k8s-cp.ccws.it/v3/import/CLUSTERFILE.yml # from rancher2\n\n# Node selector\nkubectl patch deploy cattle-cluster-agent -n cattle-system --patch '{\"spec\": {\"template\": {\"spec\": {\"nodeSelector\": {\"type\": \"control\"}}}}}'\n</code></pre>"},{"location":"services/kubify/cluster/#glusterfs","title":"GlusterFS","text":"Text Only<pre><code>kubectl apply -f gluster-storage-class.yml\nkubectl apply -f gluster-pvc.yml\n</code></pre>"},{"location":"services/kubify/cluster/#test-app","title":"Test app","text":"Text Only<pre><code>kubectl create namespace test-app\nkubectl apply -f test-app.yaml\n</code></pre>"},{"location":"services/kubify/cluster/#backup-master","title":"Backup master","text":"<p>Change master name</p> Text Only<pre><code>nodeSelector:\n    kubernetes.io/hostname: ck8s-cl01-mas01\n</code></pre> <p>Setup backup cron</p> Text Only<pre><code>kubectl apply -f backup.yaml\n</code></pre> <p>Change pv reclaim policy to retain (prevent pv deletion)</p> Text Only<pre><code>kubectl get pv -n kube-system -o wide\nkubectl patch pv pvc-cbcd2e58-6986-11e8-98bb-00505686664c -n kube-system -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Retain\"}}'\n</code></pre>"},{"location":"services/kubify/cluster/#upgrade","title":"Upgrade","text":""},{"location":"services/kubify/cluster/#master","title":"Master","text":"Text Only<pre><code>apt-get update -y\napt-get install -y kubectl\n\nexport VERSION=$(curl -sSL https://dl.k8s.io/release/stable.txt) # or manually specify a released Kubernetes version\nexport ARCH=amd64 # or: arm, arm64, ppc64le, s390x\ncurl -sSL https://dl.k8s.io/release/${VERSION}/bin/linux/${ARCH}/kubeadm &gt; /usr/bin/kubeadm\nchmod a+rx /usr/bin/kubeadm\n\nkubeadm version\nkubeadm upgrade plan\n\nkubeadm upgrade apply v1.11.0\n\napt-get -y install kubeadm kubelet\n\nkubectl drain $HOST --ignore-daemonsets\n#\u00a0Upgrade $HOST\nkubectl uncordon $HOST\n</code></pre>"},{"location":"services/kubify/cluster/#node","title":"Node","text":"Text Only<pre><code>apt-get update -y\napt-get install -y kubelet kubeadm\nkubeadm upgrade node config --kubelet-version $(kubelet --version | cut -d ' ' -f 2)\nsystemctl restart kubelet\nsystemctl status kubelet\n</code></pre>"},{"location":"services/kubify/cluster/#clean-node","title":"Clean node","text":"Text Only<pre><code>kubeadm reset\nrm -rf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/pki/ca.crt\n</code></pre>"},{"location":"services/kubify/helm/","title":"Helm","text":"<p>How to deploy a helm package</p> Bash<pre><code>helm package elastalert\nhelm repo index --merge --url https://helm-chart-kooomo.s3-eu-west-1.amazonaws.com .\naws s3 cp --acl public-read index.yaml s3://helm-chart-kooomo\naws s3 cp --acl public-read elastalert-1.8.5.tgz s3://helm-chart-kooomo\n\nhelm repo add kooomo http://helm-chart-kooomo.s3-eu-west-1.amazonaws.com\nhelm install elastalert kooomo/elastalert\n</code></pre>"},{"location":"services/kubify/k3s/","title":"K3S","text":""},{"location":"services/kubify/k3s/#wireguard","title":"Wireguard","text":"Bash<pre><code>apt -y install software-properties-common\nadd-apt-repository -y ppa:wireguard/wireguard\napt -y install wireguard\n</code></pre>"},{"location":"services/kubify/k3s/#zerotier","title":"Zerotier","text":"Bash<pre><code>curl -s https://install.zerotier.com | sudo bash\n</code></pre>"},{"location":"services/kubify/k3s/#masters","title":"Masters","text":"Bash<pre><code>export K3S_TOKEN=GENERATE A TOKEN\nexport K3S_DATASTORE_ENDPOINT='mysql://k3s:PASSWORD@tcp(10.152.161.51:3306)/k3s_c1'\n\n# M1\ncurl -sfL https://get.k3s.io | sh -s - server --tls-san api.c4.to2.k4y.it --flannel-backend host-gw --node-ip 10.152.161.47 --node-external-ip 10.147.20.31 --advertise-address 10.147.20.31 --flannel-iface ztzlgpble3 --node-taint k3s-controlplane=true:NoExecute --disable servicelb --disable traefik\n\n\n# M2\n</code></pre>"},{"location":"services/kubify/k3s/#worker","title":"Worker","text":"Bash<pre><code>curl -sfL https://get.k3s.io | K3S_URL=https://api.c4.to2.k4y.it:6443 K3S_TOKEN=JOIN_TOKEN sh -s - agent --flannel-iface ztzlgpble3\n</code></pre>"},{"location":"services/kubify/kubeadm/","title":"Upgrade kubeadm cluster","text":"<p>Read this page before start, something could be changed since 1.16 version: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/</p>"},{"location":"services/kubify/kubeadm/#note","title":"Note","text":"<p>Before upgrade</p> Text Only<pre><code>kubeadm=1.15.4-00 kubelet=1.15.4-00 kubectl=1.15.4-00\n</code></pre> <p>After upgrade</p> Text Only<pre><code>kubeadm=1.16.0-00 kubelet=1.16.0-00 kubectl=1.16.0-00\n</code></pre>"},{"location":"services/kubify/kubeadm/#how-to-upgrade-a-control-plane-cluster","title":"How to upgrade a control-plane cluster","text":""},{"location":"services/kubify/kubeadm/#all-control-planes","title":"All control planes","text":"<p>Make sure control-plane is marked as a master and kubelet config is up to date</p> Text Only<pre><code>kubeadm init phase mark-control-plane\nkubeadm init phase upload-config kubelet\n</code></pre> <p>Upgrade kubeadm</p> Text Only<pre><code>apt-mark unhold kubeadm &amp;&amp; apt-get install -y kubeadm=1.16.0-00 &amp;&amp; apt-mark hold kubeadm\n</code></pre>"},{"location":"services/kubify/kubeadm/#first-control-plane","title":"First control plane","text":"<p>Show differences</p> Text Only<pre><code>kubeadm upgrade diff\n</code></pre> <p>Plan the upgrade</p> Text Only<pre><code>kubeadm upgrade plan\n</code></pre> <p>Do the upgrade</p> Text Only<pre><code>kubeadm upgrade apply v1.16.0\n</code></pre> <p>Upgrade kubelet and kubectl</p> Text Only<pre><code>apt-mark unhold kubelet kubectl &amp;&amp; apt-get install -y kubelet=1.16.0-00 kubectl=1.16.0-00 &amp;&amp; apt-mark hold kubelet kubectl\n</code></pre> <p>Restart the kubelet</p> Text Only<pre><code>systemctl restart kubelet\n</code></pre> <p>Eventually upgrade CNI Network plugin, check here: https://docs.projectcalico.org/latest/getting-started/kubernetes/</p> Text Only<pre><code>kubectl apply -f https://docs.projectcalico.org/v3.9/manifests/calico.yaml\n</code></pre>"},{"location":"services/kubify/kubeadm/#other-control-planes","title":"Other control planes","text":"<p>Make sure control-plane is marked as a master and kubelet config is up to date</p> Text Only<pre><code>kubeadm upgrade node\n</code></pre> <p>If etcd was upgraded by the first control plane, do the ETCD upgrade</p> Text Only<pre><code>kubeadm upgrade node --etcd-upgrade\n</code></pre> <p>Upgrade kubelet and kubectl</p> Text Only<pre><code>apt-mark unhold kubelet kubectl &amp;&amp; apt-get install -y kubelet=1.16.0-00 kubectl=1.16.0-00 &amp;&amp; apt-mark hold kubelet kubectl\n</code></pre> <p>Restart the kubelet</p> Text Only<pre><code>systemctl restart kubelet\n</code></pre>"},{"location":"services/kubify/kubeadm/#how-to-upgrade-a-worker-node","title":"How to upgrade a worker node","text":"<p>Drain the node</p> Text Only<pre><code>kubectl drain $NODE --ignore-daemonsets\n</code></pre> <p>Upgrade kubeadm</p> Text Only<pre><code>apt-mark unhold kubeadm &amp;&amp; apt-get install -y kubeadm=1.16.0-00 &amp;&amp; apt-mark hold kubeadm\n</code></pre> <p>Make sure kubelet config is up to date</p> Text Only<pre><code>kubeadm upgrade node\n</code></pre> <p>Upgrade kubelet</p> Text Only<pre><code>apt-mark unhold kubelet &amp;&amp; apt-get install -y kubelet=1.16.0-00 &amp;&amp; apt-mark hold kubelet\n</code></pre> <p>Restart the kubelet</p> Text Only<pre><code>systemctl restart kubelet\n</code></pre> <p>Uncordon the node</p> Text Only<pre><code>kubectl uncordon\n</code></pre>"},{"location":"services/kubify/kubeadm/#reset-and-test","title":"Reset and Test","text":"Text Only<pre><code>kubeadm reset\nkubeadm reset -f\napt-mark unhold kubeadm kubelet kubectl\napt-get install -y kubeadm=1.15.3-00 kubelet=1.15.3-00 kubectl=1.15.3-00 --allow-downgrades\n</code></pre>"},{"location":"services/kubify/kubeadm/#update-kube-apiserver-configuration","title":"Update kube-apiserver configuration","text":"<p>After cluster intialized kubeadm-config is stored inside a kubernetes config-map.</p> Text Only<pre><code>kubectl edit cm kubeadm-config\n</code></pre> <p>After edited this file do a simple upgrade node:</p> Text Only<pre><code>kubeadm upgrade node\n</code></pre>"},{"location":"services/kubify/kubeadm/#renew-certificates-before-expiration","title":"Renew certificates before expiration","text":"Bash<pre><code># Exec the commands on all the control plane nodes\nkubeadm alpha certs check-expiration\nkubeadm alpha certs renew all\nsystemctl restart kubelet\ndocker rm -f `docker ps | grep k8s_kube-apiserver | cut -d\" \" -f1`\n\n# Only on first master\nmv /etc/kubernetes/kubelet.conf /etc/kubernetes/kubelet.conf.bak\nkubeadm init phase kubeconfig all --config /etc/kubernetes/kubeadm-config.yaml\n</code></pre>"},{"location":"services/kubify/kubeadm/#renew-certificates-after-expiration","title":"Renew certificates after expiration","text":"Bash<pre><code>mkdir -p ~/k8s_backup/etcd\n\ncd /etc/kubernetes/pki/\nmv {apiserver.crt,apiserver-etcd-client.key,apiserver-kubelet-client.crt,front-proxy-ca.crt,front-proxy-client.crt,front-proxy-client.key,front-proxy-ca.key,apiserver-kubelet-client.key,apiserver.key,apiserver-etcd-client.crt} ~/k8s_backup\n\ncd /etc/kubernetes/pki/etcd\nmv {healthcheck-client.crt,healthcheck-client.key,peer.crt,peer.key,server.crt,server.key} ~/k8s_backup/etcd/\n\n# On the primary node\nkubeadm init phase certs all --config /etc/kubernetes/kubeadm-config.yaml\ncd /etc/kubernetes\nmv {admin.conf,controller-manager.conf,kubelet.conf,scheduler.conf} ~/k8s_backup\nkubeadm init phase kubeconfig all --config /etc/kubernetes/kubeadm-config.yaml\nsystemctl restart kubelet\n\n# On the others control plane nodes\n# copy /etc/kubernetes/kubeadm-config.yaml from the primary node and save as /etc/kubernetes/restore-kubeadm-config.yaml on the new node\n# edit the hostname and ip address\nkubeadm init phase certs all --config /etc/kubernetes/restore-kubeadm-config.yaml\ncd /etc/kubernetes\nmv {admin.conf,controller-manager.conf,kubelet.conf,scheduler.conf} ~/k8s_backup\nkubeadm init phase kubeconfig all --config /etc/kubernetes/restore-kubeadm-config.yaml\nsystemctl restart kubelet\n</code></pre> <p>For each master node, use also the docker commands reported in the <code>Known issues</code> section down in this page. That's needed to reload the certificates in the control plane pods.</p>"},{"location":"services/kubify/kubeadm/#renew-prometheus-etcd-certificate","title":"Renew prometheus etcd certificate","text":"Bash<pre><code># ssh to-master\nkubectl -n kube-monitoring delete secret etcd-client-cert\nkubectl -n kube-monitoring create secret generic etcd-client-cert --from-file=ca.crt=/etc/kubernetes/pki/etcd/ca.crt --from-file=healthcheck-client.crt=/etc/kubernetes/pki/etcd/peer.crt --from-file=healthcheck-client.key=/etc/kubernetes/pki/etcd/peer.key\nkubectl -n kube-monitoring rollout restart statefulset prometheus-prom-p0-prometheus\n</code></pre>"},{"location":"services/kubify/kubeadm/#quick-upgrade","title":"Quick upgrade","text":"Bash<pre><code># Upgrade Kubify\n\napt-get -y update\n\napt-mark hold kubeadm kubelet kubectl docker-ce docker-ce-cli containerd.io\napt-get -y update; apt-get -y dist-upgrade; apt-get -y autoremove\napt-get -y --allow-change-held-packages install kubeadm=1.18.*\n\n# First Master\nkubeadm upgrade plan\nkubeadm upgrade apply v1.18.X\n\n# Nodes\nkubeadm upgrade node\n\n# All\napt-get -y --allow-change-held-packages install kubelet=1.18.* kubectl=1.18.* docker-ce=5:19.03* docker-ce-cli=5:19.03* containerd.io\n\napt-mark hold kubeadm kubelet kubectl docker-ce docker-ce-cli containerd.io\n\napt-get -y --allow-change-held-packages install kubeadm=1.18.*\nkubeadm upgrade node\napt-get -y --allow-change-held-packages install kubelet=1.18.* kubectl=1.18.* docker-ce=5:19.03* docker-ce-cli=5:19.03* containerd.io\n</code></pre>"},{"location":"services/kubify/kubeadm/#known-issues","title":"Known issues","text":""},{"location":"services/kubify/kubeadm/#unable-to-authenticate-the-request-due-to-an-error-x509-certificate-has-expired-or-is-not-yet-valid","title":"Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid","text":"<p>Restart apiserver container directly from the master nodes</p> Bash<pre><code>docker rm -f `docker ps | grep k8s_kube-apiserver | cut -d\" \" -f1`\ndocker rm -f `docker ps | grep k8s_kube-controller | cut -d\" \" -f1`\ndocker rm -f `docker ps | grep k8s_kube-scheduler | cut -d\" \" -f1`\n</code></pre> <ul> <li>https://reece.tech/posts/renewing-kubernetes-certificates/</li> </ul>"},{"location":"services/kubify/network/","title":"Kubify Network","text":"<p>Disable egress traffic to mng network:</p> Text Only<pre><code>iptables -I OUTPUT -d 10.101.0.0/24 -j DROP; iptables -I OUTPUT -s 10.101.0.0/24 -m state --state ESTABLISHED,RELATED -j ACCEPT\n</code></pre>"},{"location":"services/kubify/secrets/","title":"Secrets","text":""},{"location":"services/kubify/secrets/#bulk-importer","title":"Bulk importer","text":"<p>To convert variables from a .env file to k8s secrets one can follow the procedure below. Supposing you have a .env file like the following: <code>KEY_HERE=value</code>.</p> <p>Then you can use the python script to generate secrets:</p> Python<pre><code>import argparse\nimport sys\nimport configparser\nimport itertools\nimport base64\nfrom string import Template\n\nparser = argparse.ArgumentParser(description='Convert environment files to kubernetes secrets')\nparser.add_argument('--name', metavar='name', nargs='?', type=str, default='my-secrets', help='Name of the secret store')\nparser.add_argument('--env', metavar='.env', nargs='?', type=argparse.FileType('r'), default=sys.stdin, help='Environment input file, stdin by default')\nparser.add_argument('--secrets', metavar='.yaml', nargs='?', type=argparse.FileType('w'), default=sys.stdout, help='Secrets output file, stdout by default')\n\nargs = parser.parse_args()\n\nconfig = configparser.ConfigParser()\nconfig.read_file(itertools.chain(['[global]'], args.env), source=\"env\")\nsecrets = config.items('global')\nargs.env.close()\n\ndef loadFiles(secret):\n  if (secret[1].startswith('filecontent=')):\n    with open(secret[1][12:], 'r') as secretfile:\n      data = secretfile.read()\n      return [secret[0], data]\n  return secret\n\nsecrets = map(loadFiles, secrets)\n\nencodedSecrets = ['  {0}: {1}'.format(\n    secret[0].upper(),\n    base64.b64encode(secret[1].encode('utf-8')).decode('utf-8')\n) for secret in secrets]\n\nyamlTemplate = Template(\"\"\"apiVersion: v1\nkind: Secret\nmetadata:\n  name: $name\ntype: Opaque\ndata:\n$encodedSecrets\"\"\")\nyamlOutput = yamlTemplate.substitute(name=args.name, encodedSecrets='\\n'.join(encodedSecrets))\n\nargs.secrets.write(yamlOutput)\nargs.secrets.close()\n</code></pre> <p>Install the missing deps if needed and then proceed to use the script by passing args like these:</p> Bash<pre><code>python main.py --env envfile.env --name namespace-name-for-k8s &gt; secrets.yaml\n</code></pre>"},{"location":"services/leaf/leaf-wiki/","title":"Leaf Wiki | mkdocs","text":"<ul> <li>Clone the repository: https://github.com/criticalcase/leaf-wiki.git</li> </ul>"},{"location":"services/leaf/leaf-wiki/#vscode-extension","title":"VSCode extension","text":"<p>Install vscode extensions:</p> <ul> <li>markdown-all-in-one</li> <li>markdown-link-updater</li> <li>Draw.io Integration</li> </ul>"},{"location":"services/leaf/leaf-wiki/#obsidian","title":"Obsidian","text":"<p>Download at obsidian.md/download</p> <ul> <li>Open folder as vault</li> <li>Select leaf-wiki directory</li> <li>Go to preferences &gt; Files and links &gt; Uncheck <code>Use [[Wikilinks]]</code></li> <li>Go to preferences &gt; Community plugins: Turn on community plugins.</li> <li>Press: Browse</li> <li>Search Custom Attachment location<ul> <li>Install and Enable</li> <li>Go to options and set:</li> <li>Duplicate name separator: <code>-</code></li> <li>Location for New Attachments: <code>./assets</code> (remove <code>/${filename}</code>).</li> </ul> </li> <li>Search: <code>git</code><ul> <li>Install and Enable</li> </ul> </li> </ul>"},{"location":"services/leaf/leaf-wiki/#file-upload","title":"File upload","text":"<p>Create an <code>assets/</code> directory</p>"},{"location":"services/leaf/leaf-wiki/#useful-links","title":"Useful links","text":"<ul> <li>Local env configuration</li> <li>Auto deploy to cloudflare R2</li> </ul>"},{"location":"services/legacy/wikijs/","title":"WikiJS","text":"<p>Esempio di setup della vecchia Wiki</p>"},{"location":"services/legacy/wikijs/#database","title":"Database","text":"Bash<pre><code>CREATE USER \"leaf\" WITH PASSWORD 'XXXXXXXXXXXXXXX';\nCREATE DATABASE \"leaf\" OWNER \"leaf\";\nREVOKE CONNECT ON DATABASE \"leaf\" FROM public;\n\n\\c leaf\nCREATE EXTENSION pg_trgm;\n</code></pre>"},{"location":"services/legacy/wikijs/#configuration","title":"Configuration","text":""},{"location":"services/legacy/wikijs/#general","title":"General","text":"<ul> <li>Site url: https://leaf.rz2.criticalcase.cloud</li> <li>Site Title: Leaf RZ2</li> <li>Logo url: https://www.criticalcase.com/logo-criticalcase.png</li> <li>Company: Criticalcase Digital Solution</li> <li>Meta Robots: NoIndex, NoFollow</li> <li>Comments: disabled</li> </ul>"},{"location":"services/legacy/wikijs/#locale","title":"Locale","text":"<p>Site locale: english Download Locale: italian</p>"},{"location":"services/legacy/wikijs/#groups","title":"Groups","text":"<ul> <li>Guest &gt; Page rules &gt; Deny</li> </ul>"},{"location":"services/legacy/wikijs/#authentication-freeipa","title":"Authentication FreeIPA","text":"<ul> <li>Display Name: FreeIPA</li> <li>ldap URL <code>ldap://prodfripa06-rz2.cc.local:389</code></li> <li>Admin Bind DN: <code>uid=g.user,cn=users,cn=accounts,dc=cc,dc=local</code></li> <li>Search Base: <code>cn=users,cn=accounts,dc=cc,dc=local</code></li> <li><code>(uid={{username}})</code></li> <li>Limit to specific email domains: criticalcase.com</li> <li>Assign to group: administrator</li> </ul>"},{"location":"services/legacy/wikijs/#authentication-azure-ad","title":"Authentication Azure AD","text":"<ul> <li>Follow this guide: https://docs.requarks.io/auth/azure</li> <li>Go to App registrations &gt; Select the app &gt; Azure AD Token configuration &gt; Add optional claim</li> <li>Token type: ID</li> <li>Add <code>email</code> <code>preferred_username</code> and <code>upn</code></li> <li>Try to login/logout with azure AD</li> </ul>"},{"location":"services/monitoring/","title":"Monitoring","text":""},{"location":"services/monitoring/#zabbix","title":"Zabbix","text":"<ul> <li>Install zabbix</li> </ul>"},{"location":"services/monitoring/#pingdom","title":"Pingdom","text":"<p>URL: https://www.pingdom.com/ Password: https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/show/ds/services/pingdom</p>"},{"location":"services/monitoring/customer/","title":"Monitoraggio cliente","text":"<p>Usiamo due sistemi di monitoraggio pubblici:</p> <ul> <li>Zabbix: per il monitoraggio di tutte le VM https://monitoring.rz2.criticalcase.cloud</li> <li>Pingdom: per il monitoraggio di una sola risorsa da esterno https://my.pingdom.com/app</li> </ul>"},{"location":"services/monitoring/customer/#pingdom","title":"Pingdom","text":"<p>Warning</p> <p>The <code>Optional</code> tab in a Pingdom Check is not managed by terraform provider. Configure it manually.</p> <p>Example: <code>Monitor SSL/TLS certificate</code> checkbox and <code>Consider down prior to certificate expiring</code> number of days.</p> <p>Pingdom is configured via this terraform repository: https://github.com/criticalcase/pingdom/tree/main/terraform/prd</p> <p>Please make no changes into the GUI.</p>"},{"location":"services/monitoring/customer/#add-alert-recipients","title":"Add alert recipients","text":"<p>Only one Phone Number for each customer!</p> <ul> <li><code>Settings</code> &gt; <code>Alert Recipients</code>: ~~https://my.pingdom.com/app/3/alert-recipients/users~~ (use terraform)</li> <li>Click on <code>Add contact</code></li> <li>Name: <code>CXXXX SXXX CustomerName</code>. es. C1362 S103 Kooomo SAAS Limited</li> </ul> <p><code>SXXX</code> is needed only for customers billed by Criticalcase SAGL (C1362)</p>"},{"location":"services/monitoring/customer/#add-check","title":"Add check","text":"<p>Only one sensor for each customer!</p> <ul> <li><code>Synthetics</code> &gt; <code>Uptime</code>: ~~https://my.pingdom.com/app/newchecks/checks~~ (use terraform)</li> <li>Click on <code>Add new</code></li> <li>Name of check: <code>DC CustomerName Device [CRXXXXXX]</code>. es. IE1 Kooomo FW [CR200591]</li> <li>Check interval: 1 minute</li> <li>(For HTTPS checks only) Optional:</li> <li>Monitor SSL/TLS certificate: checked</li> <li>Consider down prior to certificate expiring: 7</li> <li>When down, alert after: 2 minute (Choose between 1-5 minutes)</li> <li>Test from: <code>Europe</code></li> <li>Tags: <code>ccd</code></li> <li>Who to alert?:</li> <li><code>Criticalcase Digital Solution</code></li> <li><code>CXXXX SXXX CustomerName</code></li> <li>Unselect <code>Tecnici Criticalcase</code></li> </ul>"},{"location":"services/monitoring/customer/#zabbix","title":"Zabbix","text":""},{"location":"services/monitoring/customer/#add-host-group","title":"Add host group","text":"<ul> <li><code>Configuration</code> &gt; <code>Host group</code>: https://monitoring.rz2.criticalcase.cloud/hostgroups.php</li> <li>Click on <code>Create host group</code></li> <li>Group name: <code>CustomerName CRXXXXXXX</code></li> </ul>"},{"location":"services/monitoring/customer/#add-user","title":"Add user","text":"<ul> <li><code>Administration</code> &gt; <code>Users</code>: https://monitoring.rz2.criticalcase.cloud/zabbix.php?action=user.list</li> <li>Click on <code>Create user</code></li> </ul>"},{"location":"services/monitoring/customer/#user-tab","title":"User tab","text":"<ul> <li>Alias: <code>CustomerName.CXXXX.SXXX</code></li> <li>Name: <code>CustomerName</code></li> <li>Surname: <code>CXXXX SXXX</code></li> <li>Groups: <code>email_alert</code></li> </ul> <p><code>SXXX</code> is needed only for customers billed by Criticalcase SAGL (C1362)</p>"},{"location":"services/monitoring/customer/#media-tab","title":"Media tab","text":"<ul> <li>Click on <code>Add</code></li> <li>Type: <code>Email (HTML)</code></li> <li>Send to: <code>customer@email.com</code></li> </ul>"},{"location":"services/monitoring/customer/#permission-tab","title":"Permission tab","text":"<p>Role: <code>Guest role</code></p>"},{"location":"services/monitoring/customer/#add-user-group","title":"Add user group","text":"<ul> <li><code>Administration</code> &gt; <code>User groups</code>: https://monitoring.rz2.criticalcase.cloud/zabbix.php?action=usergroup.list</li> <li>Click on <code>Create user group</code></li> </ul>"},{"location":"services/monitoring/customer/#user-group-tab","title":"User group tab","text":"<ul> <li>Group name: <code>CustomerFullName CXXXX SXXX</code></li> <li>Users: select <code>CustomerName</code></li> <li>Permissions: select HostGroup with <code>CustomerName CRXXXXXX</code></li> </ul> <p><code>SXXX</code> is needed only for customers billed by Criticalcase SAGL (C1362)</p>"},{"location":"services/monitoring/customer/#permission-tab_1","title":"Permission tab","text":"<ul> <li>Select: <code>CustomerName</code></li> <li>Permissions: Read</li> <li>Click on the first <code>Add</code></li> <li>Click on the second <code>Add</code></li> </ul>"},{"location":"services/monitoring/customer/#add-host","title":"Add host","text":""},{"location":"services/monitoring/customer/#host-tab","title":"Host tab","text":"<ul> <li><code>Configuration</code> &gt; <code>Hosts</code>: https://monitoring.rz2.criticalcase.cloud/hosts.php</li> <li>Click on <code>Create Host</code></li> <li>Groups: <code>CustomerName</code></li> <li><code>Interfaces</code> &gt; Click on <code>Add</code> &gt; Click on <code>Agent</code></li> <li>Monitored by proxy: select the right proxy</li> </ul> <p>Fill all the required values</p>"},{"location":"services/monitoring/customer/#templates","title":"Templates","text":"<p>Link new templates: Select all necessary templates</p>"},{"location":"services/monitoring/zabbix/","title":"CCD Zabbix","text":""},{"location":"services/monitoring/zabbix/#drawio","title":"Drawio","text":""},{"location":"services/monitoring/zabbix/#install-postgres","title":"Install Postgres","text":"Bash<pre><code>wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nRELEASE=$(lsb_release -cs)\necho \"deb http://apt.postgresql.org/pub/repos/apt/ ${RELEASE}\"-pgdg main | sudo tee  /etc/apt/sources.list.d/pgdg.list\napt update -y\napt -y install postgresql-12\n\nsudo -u postgres createuser --pwprompt zabbix\nsudo -u postgres createdb -O zabbix zabbix\n</code></pre>"},{"location":"services/monitoring/zabbix/#install-zabbix","title":"Install Zabbix","text":"Bash<pre><code>wget https://repo.zabbix.com/zabbix/5.2/ubuntu/pool/main/z/zabbix-release/zabbix-release_5.2-1+ubuntu18.04_all.deb\ndpkg -i zabbix-release_5.2-1+ubuntu18.04_all.deb\napt -y update\n</code></pre>"},{"location":"services/monitoring/zabbix/#server","title":"Server","text":"Bash<pre><code>apt -y install zabbix-server-pgsql zabbix-frontend-php php7.2-pgsql zabbix-nginx-conf zabbix-agent\nzcat /usr/share/doc/zabbix-server-pgsql*/create.sql.gz | sudo -u zabbix psql zabbix\n</code></pre>"},{"location":"services/monitoring/zabbix/#proxy","title":"Proxy","text":"Bash<pre><code>apt -y install zabbix-proxy-pgsql zabbix-agent snmp-mibs-downloader\nzcat /usr/share/doc/zabbix-proxy-pgsql/schema.sql.gz | sudo -u zabbix psql zabbix\n</code></pre> <p>Create PSK</p> Text Only<pre><code>openssl rand -hex 32 &gt;&gt; /etc/zabbix/zabbix_proxy.psk\n</code></pre> <p>Edit /etc/zabbix/zabbix_proxy.conf</p> Bash<pre><code>DBPassword=\nConfigFrequency=120\nServer=monitoring.rz2.criticalcase.cloud\nHostname=ccd-zbxpxy02.rz1.cc.local\n\nTLSConnect=psk\nTLSAccept=psk\nTLSPSKFile=/etc/zabbix/zabbix_proxy.psk\nTLSPSKIdentity=ZBX-PSK-01\n</code></pre> <p>Edit /etc/zabbix/zabbix_agentd.conf</p> Bash<pre><code>Hostname=ccd-zbxpxy01.rz2.cc.local\n</code></pre> <p>Restart proxy</p> Bash<pre><code>systemctl restart zabbix-proxy\nsystemctl enable zabbix-proxy\nzabbix_proxy -R config_cache_reload\n</code></pre>"},{"location":"services/monitoring/zabbix/#install-zabbix-agent-on-centos8","title":"Install zabbix agent on Centos8","text":"Bash<pre><code>rpm -Uvh https://repo.zabbix.com/zabbix/5.2/rhel/8/x86_64/zabbix-release-5.2-1.el8.noarch.rpm\ndnf clean all\ndnf install zabbix-agent\n</code></pre> <p>Edit /etc/zabbix/zabbix_agentd.conf like this</p> Text Only<pre><code>#List of comma delimited IP addresses, optionally in CIDR notation, or DNS names of Zabbix servers and Zabbix proxies.\nServer=10.3.0.4\n#List of comma delimited IP:port (or DNS name:port) pairs of Zabbix servers and Zabbix proxies for active checks.\nServerActive=10.3.0.4\n#Required for active checks and must match hostnames as configured on the server.\nHostname=b2b-frontend\n</code></pre> <p>Enable and start service</p> Bash<pre><code>systemctl enable zabbix-agent.service\nsystemctl start zabbix-agent.service\n</code></pre>"},{"location":"services/monitoring/zabbix/#trap","title":"Trap","text":"<p>ToDo</p> Bash<pre><code>apt -y install snmptrapd snmptt\n#/etc/snmp/snmptrapd.conf\n#/etc/snmp/snmptt.ini\n#/etc/zabbix/zabbix_proxy.conf\n</code></pre>"},{"location":"services/monitoring/zabbix/#windows-client","title":"Windows Client","text":"<ul> <li>Download: https://cdn.zabbix.com/zabbix/binaries/stable/5.0/5.0.7/zabbix_agent2-5.0.7-windows-amd64-openssl.msi</li> </ul>"},{"location":"services/monitoring/zabbix/#report","title":"Report","text":"Bash<pre><code>apt -y install git php7.2-curl\nsystemctl restart php7.2-fpm\ncd /opt\ngit clone https://github.com/martinm76/zabbix-pdf-report.git\ncd /opt/zabbix-pdf-report\ncp config.inc.php.dist config.inc.php\n./fixrights.sh\nln -sf /opt/zabbix-pdf-report /usr/share/zabbix/\n</code></pre>"},{"location":"services/monitoring/zabbix/#docs","title":"Docs","text":"<ul> <li>https://bestmonitoringtools.com/install-zabbix-proxy-on-ubuntu/#Step_11_Understanding_Zabbix_proxy_Active_vs_Passive_mod</li> </ul>"},{"location":"services/monitoring/zabbix/#how-to-install-and-configure-zabbix-agent","title":"How to install and configure zabbix-agent","text":"Bash<pre><code>wget https://repo.zabbix.com/zabbix/5.2/ubuntu/pool/main/z/zabbix/zabbix-agent_5.2.7-1+ubuntu20.04_amd64.deb\ndpkg -i zabbix-agent_5.2.7-1+ubuntu20.04_amd64.deb\n</code></pre> <p>se da errore con i nuovi os di librerie:</p> Bash<pre><code>wget http://archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2_amd64.deb\nsudo dpkg -i libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n\nsudo wget http://http.us.debian.org/debian/pool/main/o/openldap/libldap-2.4-2_2.4.47+dfsg-3+deb10u7_amd64.deb\nsudo dpkg -i ./libldap-2.4-2_2.4.47+dfsg-3+deb10u7_amd64.deb\n</code></pre> <p>se si ha un arm64:</p> Bash<pre><code>wget http://ports.ubuntu.com/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2_arm64.deb\nsudo dpkg -i libssl1.1_1.1.1f-1ubuntu2_arm64.deb\n\nsudo wget http://http.us.debian.org/debian/pool/main/o/openldap/libldap-2.4-2_2.4.47+dfsg-3+deb10u7_arm64.deb\nsudo dpkg -i ./libldap-2.4-2_2.4.47+dfsg-3+deb10u7_arm64.deb\n\nwget https://repo.zabbix.com/zabbix/5.2/ubuntu/pool/main/z/zabbix/zabbix-agent_5.2.2-2+ubuntu20.04_arm64.deb\ndpkg -i zabbix-agent_5.2.2-2+ubuntu20.04_arm64.deb\n</code></pre> <p>Configure the zabbix_agentd.conf based on the following configuration :</p> <p>/etc/zabbix/zabbix_agentd.conf</p> Bash<pre><code>Server=monitoring.rz2.criticalcase.cloud,176.221.49.181\nServerActive=monitoring.rz2.criticalcase.cloud,176.221.49.181\nHostname=bastion_host_example\n</code></pre> <p>If there are already servers configured for the same client, simply copy the contents of the zabbix_agentd.conf file from the already monitored server and add it to the new server, changing the server name in the parameter Hostname:</p> Bash<pre><code>### option: hostname\n#       unique, case sensitive hostname.\n#       required for active checks and must match hostname as configured on the server.\n#       value is acquired from hostnameitem if undefined.\n#\nHostname=Name_of_host\n</code></pre> <p>If necessary, restart the zabbix agent service and check its status using the commands:</p> Bash<pre><code># Restart\nsudo systemctl restart zabbix-agent\n\n# Status\nsudo systemctl status zabbix-agent\n</code></pre>"},{"location":"services/monitoring/zabbix/#how-to-add-the-server-to-zabbix-monitoring","title":"How to add the server to zabbix monitoring","text":"<p>Access the zabbix console, inserting the admin user and password recovered from the vault:</p> <p>Steps:</p> <ol> <li>Go to the left side menu under Monitoring &gt; Hosts</li> <li>Click on the Select button and choose the host group related to the client to which you are adding the server.</li> <li> <p>Click on the Apple button to filter only the servers in that host group.    </p> </li> <li> <p>Choose any of the servers and right-click, choosing the Configuration option.    </p> </li> <li>With the Configuration open, click on the Clone button to create a new server with the same settings.    </li> <li>Change the Host Name to the name of the server that is being included in the monitoring and then change the Agent, including the private IP of this server. Then click on the Add button to create this new server in zabbix monitoring.    </li> <li>Check the list of servers again by re-running step 3. The list will show the server name created, probably with a red Availability ZBX, but after a few minutes it will turn green, indicating that the server has been successfully included in the monitoring.    </li> </ol>"},{"location":"services/monitoring/zabbix/#how-to-uninstall-zabbix-agent","title":"How to uninstall zabbix-agent","text":"<p>Check that the agent is installed:</p> Bash<pre><code>dpkg -l | grep zabbix\n</code></pre> <p>The result should be something like:</p> <p>ii zabbix-agent 1:5.2.7-1+ubuntu20.04 amd64 Zabbix network monitoring solution - agent</p> <p>Remove the agent:</p> Bash<pre><code>sudo dpkg -r zabbix-agent\nsudo dpkg --purge zabbix-agent\n</code></pre> <p>Remove residual dependencies:</p> Bash<pre><code>sudo apt-get autoremove\n</code></pre> <p>Confirm that the agent has been removed:</p> Bash<pre><code>dpkg -l | grep zabbix\n</code></pre> <p>The result should not give any information.</p>"},{"location":"services/monitoring/zabbix/zabbix-proxy/","title":"zabbix proxy","text":"<p>Zabbix proxy is a process that may collect monitoring data from one or more monitored devices and send the information to the Zabbix server, essentially working on behalf of the server.</p>"},{"location":"services/monitoring/zabbix/zabbix-proxy/#flux-pipeline-for-zabbix-proxy","title":"Flux pipeline for zabbix proxy","text":"<p>Based on the needs, the varianles below should be changed: Helm repo package address: url: https://cdn.criticalcase.com/helm/zabbix-proxy version: 0.1.0 zabbix_host_name: tp.brandsdistribution.criticalcasecloud.com zabbix_server_name: monitoring.rz2.criticalcase.cloud</p> Text Only<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: zabbix\n  labels:\n    name: zabbix\n---\napiVersion: source.toolkit.fluxcd.io/v1beta1\nkind: HelmRepository\nmetadata:\n  name: zabbix-proxy-helm\n  namespace: zabbix\nspec:\n  url: https://cdn.criticalcase.com/helm/zabbix-proxy\n  interval: 10m\n\n---\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: zabbix-proxy-helm\n  namespace: zabbix\nspec:\n  interval: 5m\n  chart:\n    spec:\n      chart: zabbix-proxy-helm\n      version: 0.1.0\n      sourceRef:\n        kind: HelmRepository\n        name: zabbix-proxy-helm\n        namespace: zabbix\n      interval: 1m\n  values:\n    zabbix_host_name: tp.brandsdistribution.criticalcasecloud.com\n    zabbix_server_name: monitoring.rz2.criticalcase.cloud\n</code></pre>"},{"location":"services/morpheus/Morpheus/","title":"Morpheus","text":"<p>In order to force Azure scan, go into cloud -&gt; edit (Azure) and enable:</p> <ul> <li>INVENTORY EXISTING INSTANCES</li> <li>INVENTORY LEVEL: Full (API Heavy)</li> </ul>"},{"location":"services/rancher/","title":"Rancher","text":"<p>Rancher is a tool to manage from a single UI many Kubernetes clusters.</p> <p>It is available at the following address: https://rancher.rz1.criticalcase.cloud/</p>"},{"location":"services/rancher/#authentication","title":"Authentication","text":"<p>It is possible to log in to the dashboard via AzureAD.</p> <p>To interact with the kubernetes cluster where Rancher is deployed, it is possible to use the kubeconfig stored on Sherlock</p>"},{"location":"services/rancher/#management","title":"Management","text":"<p>Rancher is installed in 3 VMs in HA mode:</p> <ul> <li>rancher-m01.ds1.criticalcase.cloud</li> <li>rancher-m02.ds1.criticalcase.cloud</li> <li>rancher-m03.ds1.criticalcase.cloud</li> </ul> <p>It is installed via helm chart in a RKE2 cluster managed by flux in the flux-rancher repository</p> <p>When rancher server is updated, all the rancher agents of the connected kubernetes clusters will be updated automatically.</p>"},{"location":"services/rancher/#backup","title":"Backup","text":"<p>A daily backup is scheduled at 3AM through the rancher-backup operator. The backup files are stored in a S3 bucket in the <code>cc-digital</code> account. Each backup is retained for 30 days. No encryption has been configured rancher side, but encryption at rest has been configured on the bucket.</p>"},{"location":"services/rke2/","title":"RKE2","text":"<p>RKE2 is Rancher's enterprise-ready next-generation Kubernetes distribution. It has also been known as RKE Government.</p> <p>It is a fully conformant Kubernetes distribution that focuses on security and compliance within the U.S. Federal Government sector.</p> <p>It is the Kubernetes distribution used for Criticalcase on-premises clusters.</p>"},{"location":"services/rke2/#dsk1","title":"DSK1","text":"<p>Il cluster DSK1 \u00e8 situato su vc8-is1 rz1.</p> <p>\u00c8 costituito da 8 nodi</p> <ul> <li>3 nodi master     Text Only<pre><code>dsk1-m01\ndsk1-m02\ndsk1-m03\n</code></pre></li> <li>5 nodi worker     Text Only<pre><code>dsk1-w01\ndsk1-w02\ndsk1-w03\ndsk1-w04\ndsk1-w05\n</code></pre></li> </ul>"},{"location":"services/rke2/#dsk2","title":"DSK2","text":"<p>Il cluster DSK2 \u00e8 situato su vc10-is2 rz2.</p> <p>\u00c8 costituito da 6 nodi</p> <ul> <li>3 nodi master     Text Only<pre><code>dsk2-m01\ndsk2-m02\ndsk2-m03\n</code></pre></li> <li>3 nodi worker     Text Only<pre><code>dsk2-w01\ndsk2-w02\ndsk2-w03\n</code></pre></li> </ul>"},{"location":"services/rke2/#dsk-playground","title":"DSK Playground","text":"<p>Il cluster playground \u00e8 situato su vc10-is2 rz2.</p> <p>\u00c8 costituito da 7 nodi</p> <ul> <li>3 nodi master     Text Only<pre><code>playground-m01\nplayground-m02\nplayground-m03\n</code></pre></li> <li>4 nodi worker     Text Only<pre><code>playground-w01\nplayground-w02\nplayground-w03\nplayground-w04\n</code></pre></li> </ul>"},{"location":"services/rke2/monitoring/","title":"Monitoring","text":""},{"location":"services/rke2/monitoring/#monitor-kubernetes-cluster-services","title":"Monitor kubernetes cluster services","text":"<p>By default, when the prometheus stack is used, the following resources can not be monitored because of bind address:</p> <ul> <li>kube-controller-manager</li> <li>kube-scheduler</li> <li>kube-proxy</li> <li>etcd</li> </ul> <p>To solve this issue, the following snippet must be added in the rke2 config files.</p> <p>In the master nodes config files:</p> Text Only<pre><code>etcd-expose-metrics: true\nkube-controller-manager-arg:\n  - \"bind-address=0.0.0.0\"\nkube-scheduler-arg:\n  - \"bind-address=0.0.0.0\"\nkube-proxy-arg:\n  - \"metrics-bind-address=0.0.0.0:10249\"\n  - \"proxy-mode=ipvs\"\n  - \"ipvs-strict-arp=true\"\nkube-proxy-extra-mount:\n  - \"/lib/modules:/lib/modules:ro\"\n</code></pre> <p>In the agent nodes config files:</p> Text Only<pre><code>kube-proxy-arg:\n  - \"metrics-bind-address=0.0.0.0:10249\"\n  - \"proxy-mode=ipvs\"\n  - \"ipvs-strict-arp=true\"\nkube-proxy-extra-mount:\n  - \"/lib/modules:/lib/modules:ro\"\n</code></pre> <p>In the kube-stack-prometheus configuration (in the values of the helm chart), also this snippet must be configured:</p> Text Only<pre><code>...\nkubeProxy:\n   service:\n     selector:\n        component: kube-proxy\n...\n</code></pre>"},{"location":"services/rke2/rke2-upgrade/","title":"RKE2 Upgrade","text":"<p>Upgrade from 1.27 to 1.28</p> <p>Get control plane node list</p> Bash<pre><code>kubectl get nodes | grep control-plane\n</code></pre> Bash<pre><code>NAME       STATUS   ROLES                       AGE     VERSION\ndsk2-m01   Ready    control-plane,etcd,master   220d   v1.27.11+rke2r1\ndsk2-m02   Ready    control-plane,etcd,master   220d   v1.27.11+rke2r1\ndsk2-m03   Ready    control-plane,etcd,master   220d   v1.27.11+rke2r1\n</code></pre> <p>Check the cluster status</p> Bash<pre><code>kubectl get pods -A -o wide --field-selector status.phase!=Running | grep -v Completed\n</code></pre>"},{"location":"services/rke2/rke2-upgrade/#in-place-upgrade","title":"In place upgrade","text":"<p>Run the following command from the VMs. It is possible to reach connect to the VMs using:</p> <ul> <li>teleport<ul> <li>cc-ds1 bh-ds1</li> <li>cc-ds1 bh-ds2</li> </ul> </li> <li>ssh     Bash<pre><code>ssh ubuntu@dsk2-m01.ds2.criticalcase.cloud\nsudo su\n</code></pre></li> </ul> <p>Then use the following commands to upgrade:</p> <p>Server Bash<pre><code># ssh ubuntu\ndeclare -x INSTALL_RKE2_CHANNEL=\"v1.28\"\ncurl -sfL https://get.rke2.io | sh -\nsystemctl restart rke2-server\n</code></pre></p> <p>Agent</p> Bash<pre><code>declare -x INSTALL_RKE2_CHANNEL=\"v1.28\"\ncurl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=\"agent\" sh -\nsystemctl restart rke2-agent\n</code></pre> <p>After the procedure, if the upgrade is from a major to the next one, create an issue and a branch and edit the <code>rke2_version_*</code> local: * here for dsk1 * here for dsk2 * here for playground</p>"},{"location":"services/rke2/rke2-upgrade/#terraform-upgrade","title":"Terraform upgrade","text":"<ul> <li>Delete the node</li> </ul> Bash<pre><code>export NODE_TO_REMOVE=dsk2-m03\n\nkubectl drain $NODE_TO_REMOVE --ignore-daemonsets --delete-emptydir-data\nkubectl get node -o name $NODE_TO_REMOVE | xargs -I {} kubectl patch {} -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge\nkubectl delete node $NODE_TO_REMOVE\n</code></pre> <ul> <li>Remove the virtual machine from vmware</li> <li>If you are upgrading one of the clusters managed in the vdc-digitalsolutions repository, create an issue and a branch and edit the <code>rke2_version_*</code> local as for the in place procedure</li> <li>Execute <code>terraform apply</code></li> <li>Repeat for each node, than push and close the issue</li> </ul>"},{"location":"services/rke2/rke2-upgrade/#verify-the-upgrade","title":"Verify the upgrade","text":"Bash<pre><code>kubectl get nodes | grep control-plane\n</code></pre> Bash<pre><code>NAME       STATUS   ROLES                       AGE     VERSION\ndsk2-m01   Ready    control-plane,etcd,master   220d    v1.27.11+rke2r1\ndsk2-m02   Ready    control-plane,etcd,master   220d    v1.27.11+rke2r1\ndsk2-m03   Ready    control-plane,etcd,master   7m41s   v1.28.10+rke2r1\n</code></pre> <p>Check the cluster status</p> Bash<pre><code>kubectl get pods -A -o wide --field-selector status.phase!=Running\n</code></pre>"},{"location":"services/rke2/rke2-upgrade/#known-issues","title":"Known issues","text":""},{"location":"services/rke2/rke2-upgrade/#etcd-cluster-member-is-not-removed","title":"etcd cluster member is not removed","text":"Bash<pre><code>export NODE_ALIVE=dsk2-m01\nexport NODE_TO_REMOVE=dsk2-m03\n\nexport READY_ETCD_MEMBER=\"etcd-$NODE_ALIVE\"\nexport ETCDCTL=\"etcdctl --cert=/var/lib/rancher/rke2/server/tls/etcd/server-client.crt --key=/var/lib/rancher/rke2/server/tls/etcd/server-client.key --cacert=/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt --endpoints=https://127.0.0.1:2379\"\n\n# New container version without sh\nkubectl -n kube-system exec -it \"$READY_ETCD_MEMBER\" -- $ETCDCTL member list\n\n# Old container version with sh\nkubectl -n kube-system exec -it \"$READY_ETCD_MEMBER\" -- /bin/sh -c \"$ETCDCTL member list\"\n</code></pre> Bash<pre><code>fc53013efca8e7d, started, dsk2-m01-7c6c0b69, https://10.156.240.21:2380, https://10.156.240.21:2379, false\n6813a62a3f9d8ca9, started, dsk2-m02-8cd6d99b, https://10.156.240.17:2380, https://10.156.240.17:2379, false\n86493933b16a63f5, started, dsk2-m03-35b38f3c, https://10.156.240.22:2380, https://10.156.240.22:2379, false\n</code></pre> <p>If you need to remove the member list replace <code>member list</code> with <code>member remove ID</code>.</p> <p>Check ETCD containers</p> Bash<pre><code>kubectl -n kube-system get pod --selector=tier=control-plane,component=etcd --output=custom-columns=\"NAME\":\".metadata.name\",\"READY\":\".status.conditions[?(@.type==\\\"Ready\\\")].status\"\n</code></pre> Bash<pre><code>NAME            READY\netcd-dsk2-m01   True\netcd-dsk2-m02   True\netcd-dsk2-m03   True\n</code></pre>"},{"location":"services/rke2/rke2-upgrade/#node-password-was-not-removed","title":"Node password was not removed","text":"Text Only<pre><code>NodePasswordValidationFailed: Deferred node password secret validation failed\n</code></pre> <p>Cleanup node password if is it present</p> Bash<pre><code>kubectl get secret -n kube-system | grep node-password\n</code></pre> Bash<pre><code>NAME                                                 TYPE                 DATA   AGE\ndsk-m01.node-password.rke2                           Opaque               1      220d\ndsk-m02.node-password.rke2                           Opaque               1      220d\ndsk-m03.node-password.rke2                           Opaque               1      220d\n</code></pre> Bash<pre><code>kubectl remove secret dsk-m03.node-password.rke2 -n kube-system\n</code></pre>"},{"location":"services/rundeck/aws-credentials/","title":"AWS Credentials","text":""},{"location":"services/rundeck/aws-credentials/#update-aws-credentials","title":"Update AWS credentials","text":"<p>To update the AWS credentials used by Rundeck, a secret on the ccd-c2 kubernetes cluster must be updated.</p> <ol> <li>Clone the repository https://github.com/criticalcase/flux-ccd-c2</li> <li>Create the secret <code>secret-aws.yaml</code> in the <code>flux-ccd-c2/kubify/rundeck</code> path</li> <li>Copy from Rancher the credentials actually present in the cluster https://rancher.rz1.criticalcase.cloud/dashboard/c/c-m-k2qdflkj/explorer/secret/rundeck/aws-credentials#data    Important: don't copy the first line (credentials)</li> <li>Put in the <code>secret-aws.yaml</code>the following content</li> </ol> YAML<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-credentials\n  namespace: rundeck\ntype: Opaque\ndata:\n  credentials: |\n    # content copied from rancher encoded in base64\n</code></pre> <ol> <li>From the <code>flux-ccd-c2/kubify/rundeck</code>path, exec the following command to seal the secret</li> </ol> Bash<pre><code>kubeseal --format=yaml --cert=../../kubeseal-ccd-c2.pem &lt; ./secret-aws.yaml &gt; ./sealed-secret-aws.yaml\n</code></pre> <ol> <li>Push the <code>sealed-secret-aws.yaml</code> file generated from the kubeseal command to GitHub. Flux will automatically reconcile the cluster status in maximum 10 minutes.</li> </ol>"},{"location":"services/rundeck/rundeck/","title":"Rundeck","text":"<ul> <li>Login to Rundeck</li> </ul>"},{"location":"services/rundeck/rundeck/#descrizione","title":"Descrizione","text":"<p>Rundeck \u00e8 un orchestratore. Nel nostro caso viene impiegato per poter spegnere e accedendere EC2 ed RDS all'interno degli account dei clienti. Per farlo si utilizza l'AWS CLI installata nel container con delle api key di user con sufficienti permissions per compiere queste azioni. Volendo sar\u00e0 possibile farlo multicloud, installando le varie CLI. L'idea \u00e8 spegnere solo le risorse che hanno determinati tag forniti dal cliente.</p>"},{"location":"services/techdata/TECHDATA/","title":"TD SYNNEX (ex techdata)","text":""},{"location":"services/techdata/TECHDATA/#ion-customer-url","title":"ION Customer url","text":"<ul> <li>https://ion.tdsynnex.com/v2c/dashboard?accountName=criticalcase&amp;lang=en</li> </ul>"},{"location":"services/techdata/TECHDATA/#office365-licenses","title":"Office365 Licenses","text":"<ul> <li>https://intouch.tdsynnex.com/</li> <li>StreamOne Marketplace &gt; Microsoft NCE</li> <li>Portale Reseller &gt; Digital Locker &gt; Criticalcase srl (Techdata@criticalcase.com) &gt; SaaS</li> </ul>"},{"location":"services/techdata/TECHDATA/#microsoft-365-nce","title":"Microsoft 365 NCE","text":"<p>New Commerce Experience</p> <p>Acquistare caselle: Microsoft NCE</p> <p>Autorizzare tdsynnex da questo link: https://admin.microsoft.com/Adminportal/Home?invType=ResellerRelationship&amp;partnerId=35fbd38b-7fdb-48d1-92ba-75adb4d76b2d&amp;msppId=0&amp;DAP=true#/BillingAccounts/partner-invitation</p> <ul> <li>Se le licenze sono legacy deve chiedere la sospensione Insight dopo che abbiamo fatto l'ordine. (ti danno nota credito). Non c'\u00e8 la scadenza.</li> <li>Se le licenze sono NCE (c'\u00e8 la scritta Expires on in Subscription stauts) bisonga mettere l'autorinnovo a OFF.</li> </ul> <p>Documentazione passaggio da legacy a NCE: https://cloud.also.mp/microsoft-new-commerce-experience/#:~:text=From%20January%202023%20Microsoft%20incentives,moved%20to%20new%20commerce%20experience.</p>"},{"location":"services/techdata/TECHDATA/#margini-rivendita-aws","title":"Margini rivendita AWS","text":"<ul> <li>Sconto base: 3%</li> <li>Advance: 4% solo se abbiamo ottenuto una competenza AWS (Migration, mi gira il link). Diventerebbe 7%</li> <li>POD: 5% aggiuntivo di questo programma. New business per AWS nuovo che parte da zero. Un cliente gi\u00e0 esistente che aveva la carta di credito collegata. Ma gli account devono essere inferiori a 5k$ mese per singolo account AWS non per il cliente. Il cliente lo gestiamo noi e facciamo la rivendita. Come secondary partner \u00e8 TD SYNNEX o Techdata.</li> </ul>"},{"location":"services/teleport/","title":"Teleport ITO Service (ssh login)","text":"<p>ITO centralized teleport server is a single access point that groups all the other customer teleport clusters. Access is possible with github credentials belonging to specific groups.</p> <ul> <li>https://cc-ds1.criticalcasecloud.com</li> <li>https://cc-ds2.criticalcasecloud.com</li> </ul> <p>If github is not available, please use the emergency credentials.</p>"},{"location":"services/teleport/#access-to-teleport","title":"Access to teleport","text":""},{"location":"services/teleport/#by-browser","title":"By browser","text":"<p>Login to ITO centralized telport server with github credentials at https://cc-ds1.criticalcasecloud.com or https://cc-ds2.criticalcasecloud.com </p> <p>List nodes, sub-clusters </p> <p>and login </p> <p>scp button </p> <p>scp from server to local: </p> <p>scp from local to server: </p>"},{"location":"services/teleport/#by-tsh-client","title":"By tsh client","text":"<p>Login to ITO centralized telport server with github credentials</p> Bash<pre><code>tsh login --proxy=cc-ds1.criticalcasecloud.com:443 --auth=github\n\n# Fallback\ntsh login --proxy=cc-ds2.criticalcasecloud.com:443 --auth=github\n</code></pre> <p>List sub-clusters and login to other sub-cluster</p> Bash<pre><code>tsh clusters\ntsh login aws-buffetti-b2c\n</code></pre> <p>List nodes</p> Bash<pre><code>    tsh ls\n</code></pre> <p>TSH / SSH to a node</p> Bash<pre><code>    tsh ssh root@VM-EU-WEB-04\n</code></pre> <p>Join to an existing session</p> Bash<pre><code>    tsh join -l root SESSION_ID\n</code></pre> <p>scp from server to local:</p> Bash<pre><code>tsh -l root scp -r VM-EU-WEB-04:/var/log/apache2/*.log .\n</code></pre> <p>scp from local to server:</p> Bash<pre><code>tsh -l root scp access.log.1 VM-EU-WEB-04:/var/log/apache2/access.log\n</code></pre>"},{"location":"services/teleport/#teleport-tsh-ssh-login","title":"Teleport TSH (ssh login)","text":"<p>In kubify environments, root cannot be accessed directly using a password or ssh-key. If you are working on a snapshot you could reset the root password following:</p> <ul> <li>https://askubuntu.com/questions/24006/how-do-i-reset-a-lost-administrative-password</li> <li>https://itsfoss.com/how-to-hack-ubuntu-password/</li> </ul> <p>Create User from access.to2.k4y</p> Text Only<pre><code>tsh ssh root@access.to2.k4y\ntctl users add a.sosso root\n</code></pre> <p>Login</p> Text Only<pre><code>tsh login --proxy=access.to2.k4y.it --user a.sosso\n</code></pre> <p>TSH / SSH to a server</p> Text Only<pre><code>tsh ssh root@gw.to2.k4y\n</code></pre> <p>Join to an existing session</p> Text Only<pre><code>tsh join -l root SESSION_ID\n</code></pre>"},{"location":"services/teleport/#setup","title":"Setup","text":"<p>Create default user:</p> Bash<pre><code>tctl users add criticalcase-admin --roles=digital-solution --logins=root\n</code></pre>"},{"location":"services/teleport/#manage-configuration-on-cc-ds1","title":"Manage configuration on cc-ds1","text":""},{"location":"services/teleport/#via-ec2-instance-connect","title":"Via EC2 Instance connect","text":"<ul> <li>Login to AWS console</li> <li>Search Instances TeleportRole=auth</li> <li>Connect to instance via Session Manager</li> </ul> Bash<pre><code>tctl tokens add --type=trusted_cluster --ttl=15m\n</code></pre>"},{"location":"services/teleport/#via-bastion-host-if-available","title":"Via bastion host if available","text":"Bash<pre><code>export BASTION_IP=18.197.119.186\nexport AUTH_IP=172.31.1.23\nssh -i ${TF_VAR_key_name}.pem -o ProxyCommand=\"ssh -i ${TF_VAR_key_name}.pem -W '[%h]:%p' ec2-user@${BASTION_IP}\" ec2-user@${AUTH_IP}\n</code></pre> Bash<pre><code>export BASTION_IP=18.197.119.186\nexport AUTH_IP=172.31.3.66\nssh -i ${TF_VAR_key_name}.pem -o ProxyCommand=\"ssh -i ${TF_VAR_key_name}.pem -W '[%h]:%p' ec2-user@${BASTION_IP}\" ec2-user@${AUTH_IP}\n</code></pre>"},{"location":"services/teleport/#renew-expired-certificate","title":"Renew expired certificate","text":"<ul> <li>Login to AWS console</li> </ul> <p>On the Auth server</p> <ul> <li>Search Instances TeleportRole=auth</li> <li>Connect to instance via Session Manager</li> </ul> Bash<pre><code>sudo -i\n. /etc/teleport.d/conf\n\n/usr/local/bin/certbot certonly -n --agree-tos --email ${TELEPORT_DOMAIN_ADMIN_EMAIL} --dns-route53 -d \"${TELEPORT_DOMAIN_NAME}\" -d \"*.${TELEPORT_DOMAIN_NAME}\"\n\n/usr/local/bin/teleport-upload-cert\n</code></pre> <p>On the Proxy server</p> <ul> <li>Search Instances TeleportRole=proxy</li> <li>Connect to instance via Session Manager</li> </ul> Bash<pre><code>sudo -i\n/usr/local/bin/teleport-check-cert\nsystemctl restart teleport\n</code></pre>"},{"location":"services/teleport/#how-to-add-the-leaf-cluster-to-the-root-cluster","title":"How to add the leaf cluster to the root cluster","text":"<p>When you created the leaf cluster.</p> <ol> <li>you should log in to the considered leaf cluster.</li> <li>on the left side of the panel, click on Clusters &gt; trust</li> <li>click on the Connect To Root Cluster</li> </ol> <p></p> <ol> <li> <p>now, you should login to the root cluster:</p> </li> <li> <p>Login to AWS console</p> </li> <li>Search Instances TeleportRole=auth</li> <li> <p>Connect to instance via Session Manager</p> </li> <li> <p>you should generate a token to allow you connect to from leaf to root cluster.</p> </li> </ol> Bash<pre><code>tctl tokens add --type=trusted_cluster --ttl=15m\n</code></pre> <ol> <li>Copy the generated token then insert the copied token in the template below:</li> </ol> Bash<pre><code>kind: trusted_cluster\nmetadata:\n  name: cc-ds1\nspec:\n  enabled: true\n  role_map:\n  - local:\n    - admin\n    remote: admin\n  - local:\n    - digital-solution\n    - admin\n    remote: digital-solution\n  - local:\n    - critical-service\n    remote: critical-service\n  - local:\n    - teleport-backup-role\n    remote: teleport-backup-role\n    token: #copy the token here\n  tunnel_addr: cc-ds1.criticalcasecloud.com:443\n  web_proxy_addr: cc-ds1.criticalcasecloud.com:443\nversion: v2\n</code></pre> <ol> <li>copy the template to the step 3 and save it.</li> </ol> <p></p>"},{"location":"services/teleport/#how-to-fix-node-connetion-rejection-from-main-cluster","title":"How to fix node connetion rejection from main cluster","text":"<p>If you receive the error below when you want to access a node from main cluster, you should remove the leaf cluster from main and rejoin it again. </p> <ul> <li>Login to AWS console</li> <li>Search Instances TeleportRole=auth</li> <li> <p>Connect to instance via Session Manager</p> </li> <li> <p>run the command below to connect the kubernetes cluster and see the clusters list:</p> </li> </ul> Bash<pre><code>tsh login --proxy=tp.example.criticalcasecloud.com:443 --auth=local --user=criticalcase-admin\n</code></pre> <ol> <li> <p>Enter the password of cluster which you want to remove the remote trust cluster. for example (tp.example.criticalcasecloud.com)</p> </li> <li> <p>you can check the remote cluster that you have already logged in by the following commnad:</p> </li> </ol> Text Only<pre><code>tsh clusters\n</code></pre> <ol> <li>you should remove the remote trust cluster by the following command:</li> </ol> Bash<pre><code>tctl get rc\n\ntctl rm rc/tp.example.criticalcasecloud.com\n</code></pre> <ol> <li> <p>login to the leaf cluster and recreate the trust cluster configuration like previous configurations.</p> </li> <li> <p>you can see the teleport proxy logs on this VM by the following command:</p> </li> <li> <p>Login to AWS console</p> </li> <li> <p>Search Instances i-0618a0d4da17495ed</p> </li> <li>Connect to instance via Session Manager</li> </ol> Text Only<pre><code>journalctl -u teleport-proxy.service -b -r\n</code></pre>"},{"location":"services/teleport/#how-to-fix-login-unsccessful-on-httpscc-ds1criticalcasecloudcom","title":"how to fix Login unsccessful on https://cc-ds1.criticalcasecloud.com/","text":"<ol> <li> <ul> <li>Login to AWS console</li> </ul> </li> <li> <ul> <li>Search Instances TeleportRole=auth</li> </ul> </li> <li> <ul> <li>Connect to instance via Session Manager</li> </ul> </li> <li> <p>Use the following commands to fix the issue:</p> </li> </ol> Text Only<pre><code>tctl users ls\ntctl usres rm johnny depp\n</code></pre>"},{"location":"services/teleport/#tsh-installation-on-windows-tsh-client-only","title":"Tsh installation on windows (tsh client only)","text":"<ol> <li>Open the PowerShell(administrator)</li> <li>Running the following commands:</li> </ol> Bash<pre><code>md c:\\teleport\ncd c:\\teleport\\\ncurl https://get.gravitational.com/teleport-v7.3.2-windows-amd64-bin.zip.sha256\ncurl -O teleport-v7.3.2-windows-amd64-bin.zip https://get.gravitational.com/teleport-v7.3.2-windows-amd64-bin.zip\nExpand-Archive .\\teleport-v7.3.2-windows-amd64-bin.zip\ncertUtil -hashfile teleport-v7.3.2-windows-amd64-bin.zip SHA256\n</code></pre> <ol> <li>click on the search icon on the task bar and type the \u201cEdit the system environment variables\u201d, then click on the \u201cEdit the system environment variables\u201d.</li> </ol> <ol> <li>In the \u201cSystem Properties\u201d, click on the \u201cEnvironment Variables\u2026\u201d</li> </ol> <ol> <li>In the \u201cSystem variables\u201d section, select the \u201cPath\u201d and click on the Edit.</li> </ol> <ol> <li>click on the \u201cnew\u201d and paste the following path and then click on the OK.</li> </ol> Text Only<pre><code>C:\\teleport\\teleport-v7.3.2-windows-amd64-bin\\teleport\n</code></pre>"},{"location":"services/teleport/#how-to-creating-the-roles-for-customer","title":"How to creating the roles for customer","text":"<ol> <li>login to the teleport dashboard</li> <li>navigate to the Team &gt; Roles</li> <li>click on the CREATE NEW ROLE and paste the below rules there. change the Name and env according to the customer name and env label of servers:     &gt; the env: .... should be same as the env label of servers.         node_labels: env: prod</li> </ol> Text Only<pre><code>kind: role\nmetadata:\n  name: name_of_customer\nspec:\n  allow:\n    app_labels:\n      '*': '*'\n    db_labels:\n      '*': '*'\n    kubernetes_labels:\n      '*': '*'\n    logins:\n    - root\n    node_labels:\n      env: prod\n    rules:\n    - resources:\n      - session\n      verbs:\n      - list\n      - read\n  deny: {}\n  options:\n    cert_format: standard\n    enhanced_recording:\n    - command\n    - network\n    forward_agent: true\n    max_connections: 5\n    max_session_ttl: 8h0m0s\n    permit_x11_forwarding: true\n    port_forwarding: true\nversion: v3\n</code></pre>"},{"location":"services/teleport/#remove-corrputed-sessions","title":"Remove corrputed sessions","text":"<p>Run only on agents:</p> Bash<pre><code>rm /var/lib/teleport/log/upload/streaming/default/*.tar\n</code></pre>"},{"location":"services/teleport/event-handler/","title":"Teleport Event Handlers","text":"<p>Teleport event handler plugin is a tool to export audit events. These logs are then sent to fluentd to process them and finally show the output data on a Kibana dashboard through a Elasticsearch cluster.</p>"},{"location":"services/teleport/event-handler/#event-handler","title":"Event handler","text":"<p>The official helm chart is used to deploy the event handler plugin in production.</p>"},{"location":"services/teleport/event-handler/#identity","title":"Identity","text":"<p>The first step constists on connecting to the customer teleport bastion host to retrieve the identity file of the user with sufficient privileges to get audit events.</p> <p>If not present, the following role and user must be created:</p> <p>teleport-event-handler-role-user.yaml</p> Text Only<pre><code>kind: role\nmetadata:\n  name: teleport-plugin-event-handler\nspec:\n  allow:\n    logins:\n    - teleport-plugin-event-handler\n    rules:\n    - resources:\n      - event\n      - session\n      - sessions\n      verbs:\n      - list\n      - read\n  deny: {}\n  options:\n    cert_format: standard\n    create_desktop_user: false\n    create_host_user: false\n    desktop_clipboard: true\n    desktop_directory_sharing: true\n    enhanced_recording:\n    - command\n    - network\n    forward_agent: false\n    idp:\n      saml:\n        enabled: true\n    max_session_ttl: 8760h0m0s\n    pin_source_ip: false\n    port_forwarding: false\n    record_session:\n      default: best_effort\n      desktop: true\n    ssh_file_copy: true\nversion: v6\n---\nkind: user\nmetadata:\n  name: teleport-plugin-event-handler\nspec:\n  roles: ['teleport-plugin-event-handler']\nversion: v2\n</code></pre> <p>Then apply the following commands:</p> Text Only<pre><code># create the role and user\ntctl create -f teleport-event-handler-role-user.yaml\n# get the identity file\ntctl auth sign --out identity-teleport-plugin-event-handler --user teleport-plugin-event-handler --ttl 87600h\n# encode identity file\ncat identity-teleport-plugin-event-handler | base64 -w0\n</code></pre>"},{"location":"services/teleport/event-handler/#kubernetes-resources","title":"Kubernetes resources","text":"<p>Create a new folder in the git repository using the customer name.</p> <p>The first resource to create is the secret containing the identity file generated in the previous step:</p> <p>Do not change the names of the following file</p> <p>Modify the values in the angle brackets ('&lt;' '&gt;')</p> <p>secret-identity-teleport-plugin-event-handler.yaml</p> Text Only<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: identity-plugin-tp-&lt;customer-name&gt;\n  namespace: teleport-handlers\ntype: Opaque\ndata:\n  identity: |\n        &lt;base64 encoded identity file&gt;\n</code></pre> <p>In the customer directory, seal the secret with</p> Text Only<pre><code>kubeseal --format=yaml --cert=../../../kubeseal-ccd-c2.pem  &lt; ./secret-identity-teleport-plugin-event-handler.yaml &gt; ./sealed-secret-identity-teleport-plugin-event-handler.yaml\n</code></pre> <p>The following file permits to deploy the event handler plugin</p> <p>teleport-plugin-event-handler.yaml</p> Text Only<pre><code>apiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: teleport-plugin-event-handler-&lt;customer-name&gt;\n  namespace: teleport-handlers\nspec:\n  interval: 5m\n  chart:\n    spec:\n      chart: teleport-plugin-event-handler\n      version: 12.3.1\n      sourceRef:\n        kind: HelmRepository\n        name: teleport-plugin-event-handler\n        namespace: flux-system\n      interval: 10m\n  values:\n    teleport:\n      address: \"&lt;customer teleport address&gt;:&lt;customer teleport port&gt;\"\n      identitySecretName: \"identity-plugin-tp-&lt;customer-name&gt;\"\n      identitySecretPath: \"identity\"\n\n    eventHandler:\n      storagePath: \"/var/lib/teleport/plugins/event-handler/storage\"\n      timeout: \"10s\"\n      batch: 20\n      namespace: \"default\"\n\n    fluentd:\n      url: \"https://fluentd:8888/test.log\"\n      sessionUrl: \"https://fluentd:8888/session\"\n      certificate:\n        secretName: \"secret-certifications-fluentd-eh\"\n        caPath: \"ca.crt\"\n        certPath: \"client.crt\"\n        keyPath: \"client.key\"\n\n    persistentVolumeClaim:\n      enabled: true\n      size: 1Gi\n      storageClassName: \"nfs-client\"\n      volumeName: \"storage\"\n</code></pre> <p>Pushing the code on GitHub will apply the resources on the ccd-c2 kubernetes cluster automatically thanks to Flux.</p>"},{"location":"services/teleport/event-handler/#fluentd","title":"Fluentd","text":"<p>Fluentd has been deployed on the ccd-c2 kubernetes cluster using the official helm chart.</p> <p>A custom image of Fluentd is generated via GitHub Action to add some plugins like geoip and elasticsearch. The image is hosted on criticalcase ECR: 474512620407.dkr.ecr.eu-central-1.amazonaws.com/ci-fluentd</p> <p>The resources deployed to run Fluentd on the kubernetes cluster are hosted on GitHub and are synchronized via Flux</p> <p>The data received from the Teleport event handlers are processed and then sent in output to elasticsearch-elastic3-ccd-c2.rz1.criticalcase.cloud</p>"},{"location":"services/teleport/event-handler/#kibana","title":"Kibana","text":"<p>The data received by Elasticsearch from Fluentd are shown in this Kibana dashboard</p>"},{"location":"services/teleport/flux-k3s-bastions/","title":"K3s Lightweight Kubernetes","text":"<p>K3s is a highly available, certified Kubernetes distribution designed for production workloads in unattended, resource-constrained, remote locations or inside IoT appliances.</p>"},{"location":"services/teleport/flux-k3s-bastions/#how-to-install-k3s-on-remote-vm-machine","title":"How to Install K3s on remote VM machine","text":"Bash<pre><code>export CUSTOMER_NAME=customer-example\n\nexport BASTION_PUBLIC_IP=$(curl -s ipconfig.io/ip) # export BASTION_PUBLIC_IP=3.64.70.32\n\nexport BASTION_HOSTNAME=tp.${CUSTOMER_NAME}.criticalcasecloud.com\n\necho 'export PS1=\"\\[\\e]0;\\u@$(hostname -f): \\w\\a\\]${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@$(hostname -f)\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ \"' &gt;&gt; ~/.bashrc\n\nsudo echo 'export PS1=\"\\[\\e]0;\\u@$(hostname -f): \\w\\a\\]${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@$(hostname -f)\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ \"' &gt;&gt; /root/.bashrc\n\nsudo hostnamectl set-hostname ${BASTION_HOSTNAME}\n\ncurl -sfL https://get.k3s.io | sh -s - --tls-san ${BASTION_PUBLIC_IP} --tls-san ${BASTION_HOSTNAME} --disable traefik --node-name ${BASTION_HOSTNAME}\n</code></pre>"},{"location":"services/teleport/flux-k3s-bastions/#how-k3s-works","title":"How K3s works","text":""},{"location":"services/teleport/flux-k3s-bastions/#install-kubectl-on-local-machine","title":"Install Kubectl on local machine","text":"Bash<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre>"},{"location":"services/teleport/flux-k3s-bastions/#homebrew","title":"Homebrew","text":"<p>The Homebrew package manager may be used on Linux and Windows Subsystem for Linux (WSL). It can be installed in your home directory, in which case it does not use sudo.</p>"},{"location":"services/teleport/flux-k3s-bastions/#how-to-install-homebrew-on-local-machine","title":"How to install Homebrew on local machine","text":"Bash<pre><code>$ sudo apt-get install build-essential curl file git\n\ntest -d ~/.linuxbrew &amp;&amp; eval $(~/.linuxbrew/bin/brew shellenv)\ntest -d /home/linuxbrew/.linuxbrew &amp;&amp; eval $(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\ntest -r ~/.bash_profile &amp;&amp; echo \"eval \\$($(brew --prefix)/bin/brew shellenv)\" &gt;&gt;~/.bash_profile\necho \"eval \\$($(brew --prefix)/bin/brew shellenv)\" &gt;&gt;~/.bashrc\n</code></pre>"},{"location":"services/teleport/flux-k3s-bastions/#kubie","title":"Kubie","text":"<p>kubie is an alternative to kubectx, kubens and the k on prompt modification script. It offers context switching, namespace switching and prompt modification in a way that makes each shell independent from others. It also has support for split configuration files, meaning it can load Kubernetes contexts from multiple files.</p>"},{"location":"services/teleport/flux-k3s-bastions/#how-to-install-kubie-on-local-machine","title":"How to install Kubie on local machine","text":"Bash<pre><code>brew install kubie\n</code></pre>"},{"location":"services/teleport/flux-k3s-bastions/#bash-autocomplete","title":"Bash autocomplete","text":"Bash<pre><code>sudo cp ./completion/kubie.bash /etc/bash_completion.d/kubie\n</code></pre>"},{"location":"services/teleport/flux-k3s-bastions/#settings","title":"Settings","text":"<p>create a directory and settings file in this path ====&gt; ~/.kube/kubie.yaml</p> <p>then, copy the kubie settings contents to kubie.yaml file from below :</p> Text Only<pre><code># Force kubie to use a particular shell, if unset detect shell currently in use.\n# Possible values: bash, dash, fish, zsh\n# Default: unset\nshell: bash\n\n# Configure where to look for kubernetes config files.\nconfigs:\n\n    # Include these globs.\n    # Default: values listed below.\n    include:\n        - ~/.kube/config\n        - ~/.kube/*.yml\n        - ~/.kube/*.yaml\n        - ~/.kube/configs/*.yml\n        - ~/.kube/configs/*.yaml\n        - ~/.kube/kubie/*.yml\n        - ~/.kube/kubie/*.yaml\n\n    # Exclude these globs.\n    # Default: values listed below.\n    # Note: kubie's own config file is always excluded.\n    exclude:\n        - ~/.kube/kubie.yaml\n\n# Prompt settings.\nprompt:\n    # Disable kubie's custom prompt inside of a kubie shell. This is useful\n    # when you already have a prompt displaying kubernetes information.\n    # Default: false\n    disable: true\n\n    # When using recursive contexts, show depth when larger than 1.\n    # Default: true\n    show_depth: true\n\n    # When using zsh, show context and namespace on the right-hand side using RPS1.\n    # Default: false\n    zsh_use_rps1: false\n\n    # When using fish, show context and namespace on the right-hand side.\n    # Default: false\n    fish_use_rprompt: false\n\n# Behavior\nbehavior:\n    # Make sure the namespace exists with `kubectl get namespaces` when switching\n    # namespaces. If you do not have the right to list namespaces, disable this.\n    # Default: true\n    validate_namespaces: true\n\n    # Enable or disable the printing of the 'CONTEXT =&gt; ...' headers when running\n    # `kubie exec`.\n    # Valid values:\n    #   auto:   Prints context headers only if stdout is a TTY. Piping/redirecting\n    #           kubie output will auto-disable context headers.\n    #   always: Always prints context headers, even if stdout is not a TTY.\n    #   never:  Never prints context headers.\n    # Default: auto\n    print_context_in_exec: auto\n</code></pre>"},{"location":"services/teleport/flux-k3s-bastions/#configure-access-to-multiple-kubernetes-clusters","title":"Configure Access to Multiple Kubernetes Clusters","text":"<p>For accessing to your K8s or K3s cluster from your local machine, you should copy the remote cluster config contents on your local machine in this path and copy to a yaml file (config-example.yaml) =====&gt; ~/.kube/ by command below, it shows the cluster config contents.</p> Bash<pre><code>kubectl config view --raw\n</code></pre> <p>Replacing the IP address 127.0.0.1 with the VM machine IP in the config file on your local machine ===&gt; server: https://VM-IP:6443</p>"},{"location":"services/teleport/flux-k3s-bastions/#helm-charts","title":"Helm Charts","text":"<p>Helm is a package manager for Kubernetes. Helm is the K8s equivalent of yum or apt. Helm deploys charts, which you can think of as a packaged application. It is a collection of all your versioned, pre-configured application resources which can be deployed as one unit.</p>"},{"location":"services/teleport/flux-k3s-bastions/#install-helm","title":"Install Helm","text":"Bash<pre><code>brew install helm\n</code></pre>"},{"location":"services/teleport/flux-k3s-bastions/#flux-v2","title":"Flux V2","text":"<p>Flux is a tool for keeping Kubernetes clusters in sync with sources of configuration (like Git repositories), and automating updates to configuration when there is new code to deploy.</p>"},{"location":"services/teleport/flux-k3s-bastions/#install-flux-v2-on-local-machine","title":"Install Flux v2 on local machine","text":"Bash<pre><code>brew install fluxcd/tap/flux\n. &lt;(flux completion bash)\n</code></pre> <p>Before creating repository by Flux, you must use this command kubie ctx config-example.yaml (cluser config file) to work under kubernetes cluster workspace.</p>"},{"location":"services/teleport/flux-k3s-bastions/#create-the-repository-under-a-github-organization","title":"Create the repository under a GitHub organization:","text":"Bash<pre><code>flux bootstrap github \\\n  --owner=criticalcase \\\n  --repository=flux-k3s-bastions \\\n  --branch=main \\\n  --team=digital-solution \\\n  --path=./customers/brandsdistribution\n</code></pre>"},{"location":"services/teleport/flux-k3s-bastions/#teleport","title":"Teleport","text":"<p>Teleport is an identity-aware, multi-protocol access proxy which understands SSH, HTTPS, Kubernetes API, MySQL and PostgreSQL wire protocols.</p> <p>On a server side, Teleport is a single binary which enables convenient secure access to behind-NAT resources such as:</p> <ul> <li>SSH nodes - SSH works in browsers too!</li> <li>Kubernetes clusters</li> <li>PostgreSQL and MySQL databases</li> <li>Internal Web apps</li> </ul> <p>Teleport is trivial to setup as a Linux daemon or in a Kubernetes pod and it's rapidly replacing legacy sshd based setups at organizations.</p> <p>source : https://github.com/gravitational/teleport/tree/master/examples/chart/teleport</p>"},{"location":"services/teleport/flux-k3s-bastions/#prerequisites","title":"Prerequisites","text":"<ul> <li>Helm v3</li> <li>Kubernetes 1.14+</li> <li>Teleport license for Enterprise deployments</li> <li>TLS Certificates or optionally use self-signed certificates</li> </ul>"},{"location":"services/teleport/flux-k3s-bastions/#deploying-teleport-on-kubernetes","title":"Deploying Teleport on Kubernetes","text":"<p>The values : variables should be changed based on our needs.</p> Text Only<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: teleport\n---\napiVersion: source.toolkit.fluxcd.io/v1beta1\nkind: HelmRepository\nmetadata:\n  name: teleport\n  namespace: teleport\nspec:\n  url: https://cdn.criticalcase.com/helm/teleport\n  interval: 10m\n\n---\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: teleport\n  namespace: teleport\nspec:\n  interval: 5m\n  chart:\n    spec:\n      chart: teleport\n      version: 0.0.18\n      sourceRef:\n        kind: HelmRepository\n        name: teleport\n        namespace: teleport\n      interval: 1m\n  values:\n    config:\n      # used for cluster name, advertise_ip, and public addresses\n      # If high availability is set it is only used for the proxy\n      public_address: tp.brandsdistribution.criticalcasecloud.com\n      auth_public_address: tp.brandsdistribution.criticalcasecloud.com\n\n      teleport:\n        auth_service:\n          enabled: yes\n\n          authentication:\n            second_factor: \"off\"\n\n\n          # We recommend to use tools like `pwgen` to generate sufficiently random\n          # tokens of 32+ byte length.\n          tokens:\n          - node:hohquaidee9aiv0Ool5Eequequaijeec\n\n        ssh_service:\n          enabled: yes\n          public_addr: tp.brandsdistribution.criticalcasecloud.com\n\n        proxy_service:\n          enabled: enable\n          ssh_public_addr: tp.brandsdistribution.criticalcasecloud.com\n          tunnel_public_addr: tp.brandsdistribution.criticalcasecloud.com\n\n          kubernetes:\n            enabled: no\n\n    clusterName: tp.brandsdistribution.criticalcasecloud.com\n    acme: true\n    acmeEmail: digitalerts@criticalcase.com\n</code></pre>"},{"location":"services/teleport/flux-k3s-bastions/#controllare-lendpoint-di-teleport","title":"Controllare l'endpoint di Teleport","text":"<p>E' stato creato un HOST in zabbix dove aggiungere regole per il monitoraggio degli endpoints di Teleport dei clienti sotto ITO</p> <p>Per aggiungere un nuovo controllo, andare sotto Web Scenarios, clonarne uno e cambiare i parametri quali Name (Cliente-CXXX), nel tabs Steps, cambiare Name (CRXXXX) e URL (Nel caso della cina aumentare il Timeout), cliccare Add</p> <p>Nel Trigger, clonarne uno e modificare il Name e l'espressione con i nuovi valori, \u00e8 sufficiente sostituire il Cliente-CXXX,CRXXX, esempio: Wevee-C2404,CR230202. Cliccare Add</p>"},{"location":"services/teleport/teleport-backup/","title":"Teleport backup","text":""},{"location":"services/teleport/teleport-backup/#image","title":"Image","text":"<p>This dockerfile creates a minideb image teleport and awscli installed. Through a github action, the image is pushed on S3.</p>"},{"location":"services/teleport/teleport-backup/#helm-chart","title":"Helm chart","text":"<p>The teleport-backup helm chart creates the cronjob that will perform the teleport backup and push the output file to S3. The operations performed by the jobs are contained in a configmap.</p>"},{"location":"services/teleport/teleport-backup/#dsk1-jobs","title":"DSK1 jobs","text":"<p>The backup jobs are executed on dsk1 kubernetes cluster, in the namespace teleport-backup. The definition are in the flux-dsk repository.</p>"},{"location":"services/teleport/teleport-backup/#s3","title":"S3","text":"<p>The backup are stored in a S3 bucket. Every customer has a folder.</p>"},{"location":"services/teleport/teleport-backup/#lambda","title":"Lambda","text":"<p>A lambda function has been created in the cc-digital account. An alert mail is sent if:</p> <ul> <li>no file more recent than 24h are present, because the backup job runs every 24h</li> <li>the file is not bigger than 1kB, because sometimes a failed backup can upload an empty tar file.</li> </ul>"},{"location":"services/teleport/teleport-backup/#disable-teleport-backup","title":"Disable Teleport Backup","text":"<ol> <li>Remove the backup job configuration from the flux-dsk repository</li> <li>Move the customer folder from the teleport-backup-secret bucket to the teleport-backup-secret-no-ito bucket</li> </ol>"},{"location":"services/vault/","title":"Cloud Vault (only for customer exposed secrets)","text":"<p>This installation is AWS based, Vault is a single server instance on AWS EC2 (cloud account) with an S3 encrypted backend and an open-source UI. Vault path is https://cccloudvault.criticalcasecloud.com:8200 The ui path is https://cccloudvault.criticalcasecloud.com (exposed with HAProxy reverse proxy SSL termination)</p>"},{"location":"services/vault/#setup","title":"Setup","text":""},{"location":"services/vault/#setting-up-the-s3-bucket","title":"Setting up the s3 bucket","text":"<p>First, we will create the s3 bucket to use as storage for our Vault instance</p> <ul> <li>Navigate to the s3 console from the AWS management console</li> <li>Click the create bucket button</li> <li>Give it a name. Leave the other settings as they are</li> <li>Remember the bucket ARN for later</li> <li>You can find the bucket ARN by selecting the bucket, then going to Permissions \u2192 Bucket Policy</li> </ul>"},{"location":"services/vault/#setting-up-the-iam-policies-and-roles","title":"Setting up the IAM policies and Roles","text":"<p>We will be using an IAM user and role to authenticate with Vault once it is deployed.</p> <ul> <li>Set up the IAM Policy</li> <li>Navigate to the IAM console from the AWS management console</li> <li>Click policies on the side tab</li> <li>Click create policy</li> <li>Select the visual editor for creating the policy</li> <li>Choose s3 as the service</li> <li>In the list section, select ListBucket</li> <li>In the read section, select GetObject</li> <li>In the write section, select PutObject</li> <li>In the Resources section: For Bucket enter in the ARN for the bucket you created in the previous section</li> <li>Also in Resources section: For Object enter in the ARN for the bucket you created in the previous section for the bucket option, select any for the Object option</li> <li>Click Review Policy</li> <li>Give the policy a name and a description</li> <li>Click create policy</li> <li>The JSON of the policy should look like this at the end</li> </ul> Text Only<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:ListBucket\"\n             ],\n            \"Resource\": [\n                \"&lt;BUCKET_ARN&gt;/*\",\n                \"&lt;BUCKET_ARN&gt;\"\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"services/vault/#create-iam-role","title":"Create IAM role","text":"<p>We will now create an IAM role that will allow an ec2 instance to connect to this bucket</p> <ul> <li>Click on Roles on the side tab</li> <li>Click Create Role</li> <li>Select EC2 as the use case</li> <li>Attach the policy you just created to the role</li> <li>Click next until you get to the Review step</li> <li>Give your role a name and a description</li> <li>Click Create Role</li> <li>Create IAM user</li> <li>Lastly, we will create an IAM user and assign the policy to it. We will generate access keys for this user when initializing Vault</li> <li>Click on Users on the side tab</li> <li>Click add user</li> <li>Give the user a name</li> <li>Select programmatic access for the access type</li> <li>Click Next: Permissions</li> <li>Attach the s3 policy from the previous section to the user</li> <li>Click next until you get to the Review step</li> <li>Review the information and click create user</li> <li>Save/download the user credentials on the next screen</li> <li>Create another policy that will allow this iam user to authenticate with the AWS sdk while using Vault. Navigate to the permissions tab and click Add Permissions</li> <li>Select add existing policies directly</li> <li>Click create policy</li> <li>Put in the following policy definition in the json tab:</li> </ul> Text Only<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:DescribeInstances\",\n                \"iam:GetInstanceProfile\",\n                \"iam:GetUser\",\n                \"iam:GetRole\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sts:AssumeRole\"\n            ],\n            \"Resource\": [\n                \"&lt;USER_ARN&gt;\"\n            ]\n        }\n    ]\n}\n</code></pre> <ul> <li>Click Review Policy</li> <li>Give your policy a name and description</li> <li>Click create policy</li> <li>Attach this policy to the new user just created</li> </ul>"},{"location":"services/vault/#setting-up-the-ec2-instance","title":"Setting up the ec2 instance","text":"<p>We will now launch the ec2 instance that will host our vault server</p> <ul> <li>Navigate to the ec2 console from the AWS management console</li> <li>Select Instances from the right side tab</li> <li>Click launch instance</li> <li>Select the AWS Linux 2 ami (the topmost option)</li> <li>Select an instance class, for this tutorial we will use a nano size instance</li> <li>Click Next: Configure Instance Details</li> <li>Under IAM role, select the IAM role we created in the previous step</li> <li>Click Next: Add Storage</li> <li>The defaults are fine. Click Next: Add Tags</li> <li>Give your instance a Name tag so you can identify it after launch</li> <li>Click Next: Configure Security Group</li> <li>Create a new security group</li> <li>Give your security group a name and a description</li> <li>Click Review and Launch</li> <li>Click Launch</li> <li>If you have an existing key pair you would like to use select it, otherwise download a new key pair</li> <li>Last click Launch Instance then View Instance</li> </ul>"},{"location":"services/vault/#installing-vault","title":"Installing Vault","text":"<p>Next, we will install vault onto the instance.</p> <ul> <li>If you are not already ssh\u2019ed into the machine, ssh now using the key pair generated in the launch ec2 step</li> <li>If this is a new key you may get a permission error when you attempt to ssh in. If this happens modify the permissions on the key and try again</li> </ul> Text Only<pre><code>chmod 400 &lt;path/to/key.pem&gt;\n</code></pre> <ul> <li>update the instance</li> </ul> Text Only<pre><code>sudo yum update\n</code></pre> <ul> <li>Install vault (find other versions/binaries on the releases page)</li> </ul> Text Only<pre><code>wget https://releases.hashicorp.com/vault/1.4.2/vault_1.4.2_linux_amd64.zip\n</code></pre> <ul> <li>Unzip it</li> </ul> Text Only<pre><code>unzip vault_1.4.2_linux_amd64.zip\n</code></pre> <ul> <li>Move the binary to a location on $PATH</li> </ul> Text Only<pre><code>sudo mv vault /usr/local/bin/vault\n</code></pre> <ul> <li>Verify that vault installed properly</li> </ul> Text Only<pre><code>vault version\n</code></pre>"},{"location":"services/vault/#install-vault-as-a-service","title":"Install Vault as a Service","text":"<p>Next, we will install Vault as a service to ensure it is available after a server reboot. We will use systemd for this.</p> <ul> <li>Create a vault service file</li> </ul> Text Only<pre><code>sudo touch /etc/systemd/system/vault.service\n</code></pre> <ul> <li>Add the following to the service file</li> </ul> Text Only<pre><code>[Unit]\nDescription=\"HashiCorp Vault \"\nDocumentation=https://www.vaultproject.io/docs/\nRequires=network-online.target\nAfter=network-online.target\nConditionFileNotEmpty=/etc/vault.d/vault-config.hcl\nStartLimitIntervalSec=60\nStartLimitBurst=3\n#Environment=SYSTEMD_LOG_LEVEL=debug\n[Service]\nExecStart=/usr/local/bin/vault server -config=/etc/vault.d/vault-config.hcl\nExecReload=/bin/kill --signal HUP $MAINPID\nStandardOutput=/var/log/vault-output.log\nStandardError=/var/log/vault-error.log\nKillMode=process\nKillSignal=SIGINT\nRestart=on-failure\nRestartSec=5\nTimeoutStopSec=30\nLimitMEMLOCK=infinity\n[Install]\nWantedBy=multi-user.target\n</code></pre> <ul> <li>Create a directory for Vault configuration file</li> </ul> Text Only<pre><code>sudo mkdir --parents /etc/vault.d\n</code></pre> <ul> <li>Create the vault configuration file (I use vim here but feel free to use the text editor of your choice)</li> </ul> Text Only<pre><code>sudo vi /etc/vault.d/vault-config.hcl\n</code></pre> <ul> <li>Define the configuration</li> </ul> Text Only<pre><code>listener \"tcp\" {\n    address = \"0.0.0.0:8200\"\n    tls_disable = 0\n    tls_cert_file=\"/etc/letsencrypt/live/cccloudvault.criticalcasecloud.com/fullchain.pem\"\n    tls_key_file=\"/etc/letsencrypt/live/cccloudvault.criticalcasecloud.com/privkey.pem\"\n}\n\napi_addr = \"https://cccloudvault.criticalcasecloud.com:8200\"\n\nbackend \"s3\" {\n    bucket = \"cccloud-vault\"\n    region = \"eu-south-1\"\n}\ndisable_mlock=true\n</code></pre> <p>Note. For the certificates managemnet look at the next section</p> <ul> <li>Give ownership of the vault folder to the vault user</li> </ul> Text Only<pre><code>sudo chown --recursive vault:vault /etc/vault.d\n</code></pre> <ul> <li>Change permissions on the config file</li> </ul> Text Only<pre><code>sudo chmod 640 /etc/vault.d/vault-config.hcl\n</code></pre> <p>Enable and start the service. Check the status of the service once it has been started</p> Text Only<pre><code>sudo systemctl enable vault\nsudo systemctl start vault\nsudo systemctl status vault\n</code></pre>"},{"location":"services/vault/#configurazione-letsencrypt","title":"Configurazione letsencrypt","text":""},{"location":"services/vault/#on-aws","title":"ON AWS","text":"<ul> <li>Create an Elastic IP for the VM</li> <li>Create a DNS A record that points to the EIP for the VM</li> </ul>"},{"location":"services/vault/#on-vm","title":"ON VM","text":"<p>Change the hostname:</p> Text Only<pre><code>sudo hostname cccloudvault.criticalcasecloud.com\n</code></pre>"},{"location":"services/vault/restore/","title":"Vault restore procedure","text":""},{"location":"services/vault/restore/#backup","title":"Backup","text":"<p>First, make a backup</p> Bash<pre><code>\u279c vault operator raft snapshot prod.snapshost\n</code></pre>"},{"location":"services/vault/restore/#create-a-new-vault-server","title":"Create a new vault server","text":"Bash<pre><code>\u279c mkdir -p /opt/vault-restore\n\u279c touch /var/log/vault_audit.log\n</code></pre> <p>Add a new configuration file: /opt/vault-restore/default.hcl</p> Bash<pre><code>ui = true\nlistener \"tcp\" {\n  address = \"127.0.0.1:8200\"\n  cluster_address = \"127.0.0.1:8201\"\n  tls_disable = 1\n}\n\ncluster_addr = \"https://127.0.0.1:8201\"\napi_addr = \"https://127.0.0.1:8200\"\n\nstorage \"raft\" {\n  path    = \"/opt/vault-restore/raft\"\n  node_id = \"vaultds\"\n}\n</code></pre> <p>Start vault</p> Bash<pre><code>\u279c vault server -config /opt/vault-restore/default.hcl\n</code></pre> <p>Use another tab</p> Bash<pre><code>\u279c vault operator init\n\nUnseal Key 1: K1vw31fX2HiBploXFYQkQxHL/IVnNfuxmQYbhu5/eUm5\nUnseal Key 2: W3ORxDVdC7nZeYC5GixhLiTCn1HrnD5kAouFh3mEiXkJ\nUnseal Key 3: leL7nzZQZFjs/X7584sVM9bgytIrC5G9ibUBsClZUcf6\nUnseal Key 4: 0YZ/XRBkXF42TJZ9Mi1mPQ31KqmrUXLAa52rPG8D4QgG\nUnseal Key 5: P6Jr8pJaLu6HcPUf0sjc1BwTFq8CSHZBkcrVEj8odjOY\n\nInitial Root Token: hvs.DvUmRjcXBYC1PMd7OMikeOpH\n\n# Do not store these temporary keys\n# Use 3 keys to unseal vault\n\u279c vault operator unseal K1vw31fX2HiBploXFYQkQxHL/IVnNfuxmQYbhu5/eUm5\n\u279c vault operator unseal W3ORxDVdC7nZeYC5GixhLiTCn1HrnD5kAouFh3mEiXkJ\n\u279c vault operator unseal leL7nzZQZFjs/X7584sVM9bgytIrC5G9ibUBsClZUcf6\n\n# Use Initial Root Token to login\n\u279c vault login hvs.DvUmRjcXBYC1PMd7OMikeOpH\n</code></pre>"},{"location":"services/vault/restore/#restore-vault","title":"Restore vault","text":"Bash<pre><code># Restore the snapshot\n\u279c vault operator raft snapshot restore -force prod.snapshot\n\n# Use the token from the original instance\n\u279c vault operator unseal\n\n# Use Initial Root Token from the original instance\n\u279c vault login\n</code></pre>"},{"location":"services/vault/sherlock/","title":"Sherlock - Criticalcase Vault","text":"<p>This installation is Criticalcase Cloud based, Vault is a single server instance on VMware in Rozzano DC with a Raft encrypted backend (1 peer only) and the official Hashicrp UI. Vault itself if on machine</p> Text Only<pre><code>vaultds.rz2.cc.local\n</code></pre> <p>exposed on port 8200 The ui path to reach te service is</p> Text Only<pre><code>https://cccloudvault.criticalcasecloud.com\n</code></pre> <p>(exposed with HAProxy reverse proxy)</p> <p>UI is reachable on the path</p> Text Only<pre><code>https://cccloudvault.criticalcasecloud.com/ui\n</code></pre>"},{"location":"services/vault/sherlock/#access","title":"Access","text":"<p>Users must access with LDAP autentication (freeIPA), just in case of sealed Vault or main configuration changes the keys and root tocken are available inside AWS vault.</p>"},{"location":"services/vault/sherlock/#ui-access-use-freeipa-credentials","title":"UI Access use FreeIPA credentials","text":""},{"location":"services/vault/sherlock/#cli","title":"CLI","text":"Text Only<pre><code>export VAULT_ADDR=\"https://sherlock.criticalcase.cloud/\"\nvault login -method=ldap username=&lt;your ldap user&gt;\n</code></pre> <p>Remember to export vault addr even on vault server otherwise vault will point localhost not validating the Certificate</p>"},{"location":"services/vault/sherlock/#keys-structure","title":"Keys Structure","text":"<ul> <li>DS is the main root for digital solution stuffs</li> <li>Customers contains all cutomers related data</li> <li>Services contains the internal services stuffs</li> <li>[environment]-[customer] are the keys referred to all the customers</li> </ul>"},{"location":"services/vault/sherlock/#installing-vault","title":"Installing Vault","text":""},{"location":"services/vault/sherlock/#configuration","title":"Configuration","text":""},{"location":"services/vault/sherlock/#freeipa-config","title":"FreeIPA config","text":""},{"location":"services/vault/sherlock/#dr-procedure","title":"DR Procedure","text":""},{"location":"services/vault/tips/","title":"Tips","text":"Bash<pre><code># List KV\n\u279c vault kv list kv/ds\nKeys\n----\ncustomers/\nservices/\n\n# List TOTP\n\u279c vault list /totp/keys\nKeys\n----\nmy-key\nmy-key2\n\n# List secret\n\u279c vault secrets list\n\nPath          Type         Accessor              Description\n----          ----         --------              -----------\ncubbyhole/    cubbyhole    cubbyhole_ac3251aa    per-token private secret storage\nidentity/     identity     identity_7aef978e     identity store\nkv/           kv           kv_3d9f9abd           n/a\nsys/          system       system_e42e2530       system endpoints used for control, policy and debugging\ntotp/         totp         totp_74f9c304         n/a\n\n# Create TOTP\n\u279c export KEYNAME=my-key\n\u279c export ACCOUNTNAME=aws-key\n\u279c export SECRET=hk64l4vxbkptn3vsbtacwgcrz5rgai5d5wscvo6kteriq6oe34k3a6gx\n\u279c vault write totp/keys/$KEYNAME url=\"otpauth://totp/Vault:$ACCOUNTNAME?secret=$SECRET\"\n\nSuccess! Data written to: totp/keys/my-key\n\n# Get TOTP\n\u279c vault read /totp/code/my-key\nKey     Value\n---     -----\ncode    171876\n\n# Read key data\n\u279c vault read /totp/keys/my-key\nKey             Value\n---             -----\naccount_name    aws-key\nalgorithm       SHA1\ndigits          6\nissuer          Vault\nperiod          30s\n</code></pre>"},{"location":"services/vmware/","title":"VMWare","text":""},{"location":"services/vmware/#manuale-operativo-vdc","title":"Manuale operativo VDC","text":"<p>Questo \u00e8 il manuale operativo che utilizzano i colleghi di supporto/infrastruttura per l'ambiente on-premise. E' utile per capire su quale storage fisico si trovano i clienti nelle varie isole.</p> <p>Non eseguire operazioni in autonomia, scalare sempre su supporto/infrastruttura.</p> <ul> <li>https://criticalcaseazure.sharepoint.com/:w:/s/ICT/EcO13MCuccpAoFy2X0K8jwkBVqjsc1Fc3A6lv8CzveYYkg?e=4Y016o</li> </ul>"},{"location":"services/vmware/#ricerca-vm-cross-datacenter-ex-boro","title":"Ricerca VM Cross Datacenter (ex Boro)","text":"<ul> <li>URL: https://10.156.0.45:5601/s/ito/app/dashboards#/view/c858cb4a-b48e-447e-9eab-791a4073c284?_g=(filters:!(),refreshInterval:(pause:!t,value:60000),time:(from:now-15m,to:now))</li> <li>Credenziali: https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/kv/ds%2Fservices%2Fboro-vmware/details</li> </ul>"},{"location":"services/vmware/#cli","title":"CLI","text":"Bash<pre><code>brew install govc\n\n#\u00a0Configure the CLI\nexport GOVC_URL='https://microgame-terraform@vsp:PASSWORD@10.154.67.75/sdk'\nexport GOVC_INSECURE=true\n\n# Search VM\n\u276f govc find /VC5-IS3-RZ1/vm/Microgame@C2311/Microsoft_ITO@230146/packer -type m -p=false\n/VC5-IS3-RZ1/vm/Microgame@C2311/Microsoft_ITO@230146/packer/mg-ubuntu-20.04.5-1696086210\n\n# Get INFO\n\u276f govc vm.info /VC5-IS3-RZ1/vm/Microgame@C2311/Microsoft_ITO@230146/packer/mg-ubuntu-20.04.5-1696086210\n\n#\u00a0Delete VM\n\u276f govc vm.destroy /VC5-IS3-RZ1/vm/Microgame@C2311/Microsoft_ITO@230146/packer/mg-ubuntu-20.04.5-1696086210\n</code></pre>"},{"location":"services/vmware/#bugfix-for-multipathd-on-linux-vm","title":"Bugfix for multipathd on Linux VM","text":"<p>In the syslog you see a lot of multipathd error like:</p> Bash<pre><code>multipathd[651]: sda: add missing path\nmultipathd[651]: sda: failed to get udev uid: Invalid argument\nmultipathd[651]: sda: failed to get sysfs uid: Invalid argument\nmultipathd[651]: sda: failed to get sgio uid: No such file or directory\nmultipathd[651]: sda: add missing path\nmultipathd[651]: sda: failed to get udev uid: Invalid argument\nmultipathd[651]: sda: failed to get sysfs uid: Invalid argument\nmultipathd[651]: sda: failed to get sgio uid: No such file or directory\n</code></pre> <p>The problem is that VMWare by default doesn't provide information needed by udev to generate /dev/disk/by-id entries.</p> <p>You can fix in this way:</p> <ul> <li>power off the VM, and then go to Edit Settings -&gt; VM Options -&gt; Advanced -&gt; EDIT CONFIGURATION</li> <li>add the following parameter</li> </ul> Bash<pre><code>disk.EnableUUID = \"TRUE\"\n</code></pre> <ul> <li>Save and power on the VM</li> </ul>"},{"location":"services/vmware/#retrieve-vsphere-server-ip","title":"Retrieve vSphere server IP","text":"<p>Use the following command to retrieve vSphere server IP:</p> Bash<pre><code>dig @10.154.67.1 &lt;vsphere-url&gt;\n#example\n# dig @10.154.67.1 vc6-is2.rz2.vdc.ccws.it\n</code></pre>"},{"location":"services/vmware/pfsense-ds/","title":"pfsense: DS-RZ1 and DSRZ2","text":"<p>Documentazione cloudflared su pfsense: https://www.reddit.com/r/CloudFlare/comments/sqgygk/cloudflared_on_freebsd/</p> <p>Configurazione pfsense su vc6-is2.rz2:</p> Text Only<pre><code>Hostname: vpn-gw02.criticalcase.cloud\n\nLAN\nIP: 10.156.240.1\nSubnet: 10.156.240.0/20\n\nWAN\nIP: 109.233.121.45\nSubnet 109.233.121.0/24\nGW: 109.233.121.1\nNetwork: Ito-109.233.121.0/24.pub3036@C0001\n</code></pre> <p>Configurazione pfsense su vc8-is1.rz1:</p> Text Only<pre><code>Hostname: vpn-gw01.criticalcase.cloud\n\nLAN\nIP: 10.154.240.1\nSubnet: 10.154.240.0/20\n\nWAN\nIP: 109.233.121.47\nSubnet 109.233.121.0/24\nGW: 109.233.121.1\nNetwork: Ito-109.233.121.0/24.pub3036@C0001\n</code></pre>"},{"location":"services/vmware/pfsense-ds/#configure-vmware","title":"Configure VMWare","text":"<p>VM Name: vpn-gw02.criticalcase.cloud</p> <p></p>"},{"location":"services/vmware/pfsense-ds/#pfsense-first-boot","title":"PFSense first boot","text":"<ul> <li>Install pfsense and use ZFS as Filesystem</li> <li>Unmount the CD-ROM and reboot the server</li> <li>Do not setup VLAN: press N</li> <li>Enter WAN interface: vmx1</li> <li>Enter LAN interface: vmx0</li> <li>Press: y</li> <li>Wait a couple of minutes</li> <li>Press 2 to set interfaces ip address</li> <li>Press 1 for vlan</li> <li>Press n to not use dhcp</li> <li>Enter the ip address: 109.233.121.45</li> <li>Enter subnet: 24</li> <li>Enter gateway: 109.233.121.1</li> <li>Press y to set as default gateway</li> <li>Press n to not configure DHCP6</li> <li>Pres n to disable ipv6</li> <li>Press n to not enable dhcpd on WAN</li> <li>Press y to revert webconfigurator</li> <li>Press 8 to open shell</li> <li>Disable the firewall: <code>pfctl -d</code></li> <li>Go to http://109.233.121.45/</li> <li>Sign in with: admin / pfsense</li> <li>Next &gt; Next &gt; Set the hostname: vpn-gw02</li> <li>Domain: ccds2</li> <li>Primary DNS: 8.8.8.8</li> <li>Secondary DNS: 1.1.1.1</li> <li>Override DNS: uncheck</li> <li>Next &gt; Next</li> <li>Configure LAN Interface: 10.156.240.1/20</li> <li>Set the new admin password</li> <li>Press reload</li> <li>Disable the firewall: <code>pfctl -d</code></li> <li>Go to http://109.233.121.45/system_advanced_admin.php and switch Protocol to HTTPS. Add Alternate Hostnames: i.e. ui-vpn-gw02.criticalcase.cloud vpn-gw02.criticalcase.cloud. Enable Secure Shell Server. SSHd Key Only = Public Key only</li> <li>Add your SSH key here http://109.233.121.45/system_usermanager.php?act=edit&amp;userid=0 and enable: Keep Command History</li> <li>Go to http://109.233.121.45/firewall_rules.php</li> <li>Add rules to enable HTTP / HTTPS on WAN and apply changes</li> <li>Reboot the firewall</li> </ul>"},{"location":"services/vmware/pfsense-ds/#cloudflared","title":"cloudflared","text":"<p>cloudflared is managed by \"manager\" vms, do not install in pfsense!</p> Bash<pre><code>pkg install pfSense-pkg-Shellcmd pfSense-pkg-acme pfSense-pkg-Open-VM-Tools\npkg lock pkg\n\n# Enable FreeBSD repo:\nvi /usr/local/etc/pkg/repos/FreeBSD.conf\nvi /usr/local/etc/pkg/repos/pfSense.conf\n\npkg update\n# Ignore the mismatch and continue? [y/N]: y\n\npkg install cloudflared\n# Installed packages to be UPGRADED:\n#   pkg: 1.19.1_1 -&gt; 1.20.5 [FreeBSD]\n# Proceed with this action? [y/N]:\n# Press: N\n\n# Remove FreeBSD repo:\nvi /usr/local/etc/pkg/repos/FreeBSD.conf\nvi /usr/local/etc/pkg/repos/pfSense.conf\n\npkg unlock pkg\npkg update\n\n# exit the shell and login again\nexit\n\n# Open the shell again, press: 8\n\n# Add the tunnel token\nvi /usr/local/etc/cloudflared/config.yml\ntoken: ey...\n\n# Try the configuration\ncloudflared tunnel run\n\n# Set\nvi /usr/local/etc/rc.d/cloudflared\n\n: ${cloudflared_enable:=\"YES\"}\n: ${cloudflared_conf:=\"/usr/local/etc/cloudflared/config.yml\"}\n: ${cloudflared_mode:=\"tunnel run\"}\n\nservice cloudflared enable\nservice cloudflared restart\n\n# Install the service\nln -sf /usr/local/etc/rc.d/cloudflared /usr/local/etc/rc.d/cloudflared.sh\n</code></pre>"},{"location":"services/vmware/veeam/","title":"VEEAM","text":"<ul> <li>Credenziali: https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/show/ds/services/vdc/ds-veeam-rz1</li> </ul>"},{"location":"team/","title":"Team","text":""},{"location":"team/#onboarding","title":"Onboarding","text":"<ul> <li>VPN (TO1, TO2, RZ1, RZ2)</li> <li>FreeIPA + OTP cc.local</li> <li>FreeIPA kooomo.local</li> <li>VSP</li> <li>Jira:</li> <li>Cloud account</li> <li>Jira ticket notification</li> <li>Pingdom SMS (High) + Mail (High &amp; Low)</li> <li>Github</li> <li>Gitlab</li> <li>.ssh/config</li> <li>Generate SSH public key</li> <li>Vault</li> <li>Zabbix</li> <li>AWS criticalcase-access</li> <li>GCP gcp.criticalcase.com</li> <li>Azure</li> <li>3CX Centralino</li> <li>Diffusione Tessile</li> </ul>"},{"location":"team/#microsoft-365","title":"Microsoft 365","text":"<p>Add members to Groups:</p> <ul> <li>Microsoft 365 Group: Digital Solution</li> <li>Distribution list group: digitalerts@criticalcase.com</li> </ul> <p>Add members to Shared Mailboxes:</p> <ul> <li>digital@criticalcase.com &gt; Delegation &gt; Read and manage (Full access)</li> </ul> <p>Add alias for @gcp.criticalcase.com:</p> <ul> <li>Go to the user: a.sosso@criticalcase.com</li> <li>Aliases &gt; Manage username and email</li> <li>Username: <code>a.sosso</code></li> <li>Domain: <code>gcp.criticalcase.com</code></li> <li>Press Add</li> <li>Press Save</li> </ul>"},{"location":"team/#gcp","title":"GCP","text":"<ul> <li>Create the e-mail alias on Microsoft 365</li> <li>Add new user: https://admin.google.com/u/8/ac/users</li> <li>Primary email: <code>a.sosso@gcp.criticalcase.com</code></li> <li> <p>Secondary email: personal email account (not <code>a.sosso@criticalcase.com</code>)</p> </li> <li> <p>Add the user to Group: Criticalcase ITO https://admin.google.com/u/8/ac/groups/02afmg282jt3w5x</p> </li> </ul>"},{"location":"team/#jira-ticket-notification","title":"JIRA Ticket notification","text":"<p>Add new member to group ito-service-notifications):</p>"},{"location":"team/#reperibilita-digital-calendar","title":"Reperibilit\u00e0 Digital calendar","text":"<p>Add members to: Andare sul calendario &gt; Reperibilit\u00e0 Digital &gt; Condivisione e autorizzazioni &gt; Aggiungere partecipante &gt; Possono modificare.</p>"},{"location":"team/#vault","title":"Vault","text":"<ul> <li>https://github.com/criticalcase/azuread-criticalcase-com/commit/e98022f9b40d0a250c4833fc0c27f74157f68e18</li> </ul>"},{"location":"team/#aws-iam","title":"AWS IAM","text":"<ul> <li>https://github.com/criticalcase/aws-iam-identity-center/pull/2/files</li> </ul>"},{"location":"team/#todo","title":"ToDo","text":"<ul> <li>Mange azuread groups via terraform: https://portal.azure.com/#view/Microsoft_AAD_UsersAndTenants/UserProfileMenuBlade/~/Groups/userId/898a5b5c-5005-4385-82b4-762930c3c261/hidePreviewBanner~/true</li> <li>Manage github groups via terraform</li> <li>Manage google groups via terraform</li> <li>Manage atlassian via terraform</li> </ul>"},{"location":"team/baseline/","title":"Baseline","text":"<p>Best practices del Team Digital Solutions</p>"},{"location":"team/baseline/#infrastructure-as-code","title":"Infrastructure as code","text":"<p>Usiamo terraform per gestire le infrastrutture sui cloud provider</p>"},{"location":"team/baseline/#container-orchestration","title":"Container Orchestration","text":"<p>Usiamo Kubernetes come sistema di orchestrazione per i container</p>"},{"location":"team/baseline/#on-premise","title":"on-premise","text":"<p>Tutte le VM on-premise del servizio ITO devono:</p> <ul> <li>essere messe sotto backup nakivo</li> <li>avere la SWAP disabilitata. I database possono avere una swap minore di 256MB con swapiness a 10.</li> </ul>"},{"location":"team/baseline/#credentials","title":"Credentials","text":"<p>Le credenziali condivise con il team e H24 devono essere riportate su Vault interno: https://sherlock.criticalcase.cloud</p>"},{"location":"team/baseline/#aws-account-cc-digital","title":"AWS Account cc-digital","text":"<ul> <li>Usiamo sempre la region eu-central-1 quando possibile. Evitiamo di installare nat gateway in pi\u00f9 VPC in modo da ottimizzare il costi.</li> <li>Ogni risorsa deve avere il TAG del creatore, in questo modo sappiamo a chi chiedere infomrazioni.</li> <li>Ricerca risorse senza tag: https://eu-central-1.console.aws.amazon.com/resource-groups/tag-editor/find-resources?region=eu-central-1#query=regions:!%28%27AWS::AllSupported%27%29,resourceTypes:!%28%27AWS::AllSupported%27%29,tagFilters:!%28%29,type:TAG_EDITOR_1_0</li> </ul>"},{"location":"team/cloudflare/","title":"Cloudflare","text":"<ul> <li>Portal: https://criticalcase.cloudflareaccess.com</li> <li>Team name: <code>criticalcase</code></li> </ul>"},{"location":"team/cloudflare/#warp-client","title":"Warp client","text":"<ul> <li>Turn off any other vpn before start</li> </ul>"},{"location":"team/cloudflare/#linux","title":"Linux","text":"<ul> <li>Install the package from this repo: https://pkg.cloudflareclient.com/</li> </ul> Bash<pre><code># Install\nsudo apt-get update &amp;&amp; sudo apt-get install cloudflare-warp\n\n# Join to criticalcase group\nwarp-cli teams-enroll criticalcase\n\n# Check the status\nwarp-cli account\n\n# If it doesn't work try again with: warp-cli delete\n\n# Connect to the VPN\nwarp-cli enable-always-on\n\n# On demand connect\nwarp-cli connect\n</code></pre>"},{"location":"team/discover/","title":"Discovered tools","text":""},{"location":"team/discover/#ansible","title":"Ansible","text":"<ul> <li>Collection Explorer allows you browse and read documentation for any Ansible collections: https://github.com/ansible-community/collection-explorer/tree/master/electron</li> <li>Use pulp_ansible to create a private Galaxy: https://docs.pulpproject.org/pulp_ansible/</li> </ul>"},{"location":"team/discover/#dashboards","title":"Dashboards","text":"<ul> <li>Redash helps you make sense of your data: https://redash.io/</li> <li>Unified monitoring wallboard: https://monitoror.com/</li> </ul>"},{"location":"team/discover/#status-page","title":"Status page","text":"<ul> <li>https://lambstatus.github.io/</li> <li>https://marquez.co/statusfy</li> <li>https://staytus.co/</li> <li>https://github.com/statping/statping</li> <li>https://freshstatus.io</li> </ul>"},{"location":"team/discover/#database","title":"Database","text":"<ul> <li>Version control for your database: https://flywaydb.org/</li> <li>Fully managed database-as-a-service https://www.compose.com/why-compose</li> </ul>"},{"location":"team/discover/#git","title":"Git","text":"<ul> <li>A painless self-hosted Git service: https://gitea.io</li> </ul>"},{"location":"team/discover/#logs","title":"Logs","text":"<ul> <li>Loki is a horizontally-scalable, highly-available, multi-tenant log aggregation system: https://grafana.com/oss/loki/</li> </ul>"},{"location":"team/discover/#terraform","title":"Terraform","text":"<p>Terratest is a Go library that makes it easier to write automated tests for your infrastructure code: https://github.com/gruntwork-io/terratest</p>"},{"location":"team/discover/#vpn-zerotrust","title":"VPN / Zerotrust","text":"<ul> <li>Secure Application Access: https://www.pomerium.com/</li> <li>Authenticate and authorize all traffic with a Zero Trust security model: https://www.ory.sh/oathkeeper</li> <li>Access any system from anywhere based on user identity.: https://www.boundaryproject.io/</li> </ul>"},{"location":"team/discover/#ssh","title":"SSH","text":"<ul> <li>The easy way for teams to SSH: https://smallstep.com/sso-ssh/</li> <li>A powerful yet lightweight security platform that provides insightful observability, proactive controls, threat detection and response for your Linux infrastructure in the cloud or datacenter: https://cmd.com</li> </ul>"},{"location":"team/discover/#backup","title":"Backup","text":"<ul> <li>Small and simple wrapper application for Restic + Prometheus https://github.com/Southclaws/restic-robot</li> <li>provides secure and efficient way to backup data remotely https://github.com/restic/rest-server</li> </ul>"},{"location":"team/keys/","title":"SSH Keys","text":""},{"location":"team/keys/#generate-the-ssh-key","title":"Generate the SSH-Key","text":"Bash<pre><code>ssh-keygen -t ed25519 -C \"n.cognome@criticalcase.com\"\n</code></pre>"},{"location":"team/keys/#add-ssh-keys-for-root-user","title":"Add SSH keys for root user","text":"Bash<pre><code>sudo -i\nmkdir -p /root/.ssh\ncat &gt;&gt; /root/.ssh/authorized_keys &lt;&lt;EOF\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOrQ+BmDshUGsaOZ9NHYr0hrsfeUUiXaJUYwwUbcAdhK a.sosso@criticalcase.com\nssh-rsa AAAAB3NzaC1yc2EAAAABJQAAAQEAmSEGE/5YKstvnU5J95HXy4wuq3Ho2DbudPZnUpHrY4r90d1weaCEhbiVwLk8y6avAKWxWocbu4UG6zIpuSBEerDFo7Hsoa+ycad9/KXdZ25+TdNwiIZxLdNic9JPt2EVCpEKIj9ynha9t7ejnya3AlvcKpTXZsFfure/e/BQ29XUCLQMt5DaOcoEoytMPq42hOUzpfo7nLxPKGjRhfMpw7xBmVZZyRofY0MuZuuX1Kh7M3HyXLbNrElWXGlUZR04f3+HUTPlnNEkmuetFWsBzW6EsbsFCGkRZhTqsh2HdhayPJw1v0pcu75FJ/EFia99urlw4+KsZmKkFCG8Mp1X8Q== a.barbaglia@criticalcase\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDJ9TpqR3xT6zeVaMvguTEph0ww8gchp+LWf/uXfOHiYOluaqa/5w61Ajo/Ukfk9ZTVy+tS1OK9JBvUtIgwo2oOPFe7V0juCpRh3F51VAkYpdk9l/ehfUey+kY7jMWxVJGBeHB16KhXO8LK0CYFFrI+NfhtKR67BUql26mUZPGyIq52DoB9qh4cVDzsuUOLpUkMtdnp9ZIWzuYlpSaDsR8V5sVE+YWmbKzimNz6K1qMSqSDYS0jacAoBuuZOkGtm/w8eHYHE4VSmnEZFjC+zacmAx3R52QDR9Huj0VUUM+hTkf7PaPvEXAB+UUmZMP5UFeOwhfkfJXEBKR9W//FjY57 j.garia@criticalcase.com\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC9S40+3602OASDmagQDVb9FrgoMgvgEY8uzavIhV4xdmeLFKjIwbbnEbO/kkli13CV6ADBU9POeguFLpYYjXTeYfz7McClTY1T3ujiuYHqBHNnbKuD7Z7GD2DMOf3cn4EiHGSQeHku5Anvagt4ZTcN5ZOAh6lETxR5RbFNVBQGM9Z5R9OfrD7G2y5yxTBt6ptlYYgf58hv0oPc+fUrSzSiceZXNM/y+6GbyRLdKpip/KHFB9NnS6EzWuVWH+5Kl0DOTYIYgv0rIAM4t59aBcLGvvmQaT2BU4azfQVjja1BJRoSylxIobkeZbFtZsEnjb3HT9QJCrdMft/e3mCeEgQD/7Ld8k95k4Hw+syuOpWygJEwg10rfTQ+LtUl9fNntXK2KrqV60lNuO7bbxelV5ts+MnhK/B2I1or1cyjTJ62nHi9hwNJ0GeIlKrup2k4zpqK3bl9HqVqd4/dpGEk1DX2oMiiBcA/ho5xAlV+oOgtIUKK1BEMdATZjvVwddDgFF8= s.russo@criticalcase\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC6Oa4Ej9UfaOwRFIQcsLia75nxw06KnWP0Y+ubJU8EqZb516WSec+pg3YJ+cLdOW5KSY2Z03RRzO8yCxMiUcoBDPhOPey1t7udW4EagXDLoiT0gp84/3pdhV6tKDs2eK7uYwrWhAsY/QJ0IBExBuUwwICCSpeUZRX/xrjRsBUvsD7uktV0VghW6mLR9qT7QTwmcGjShRFS6Rd3x1b1P+6x/3qPF1fUVJ0lI5IU3QUFLMUBrpEENm/9X+g/gn0EkmrnO5ILZiVV/YSrYEMMy1K63BRGTyt6HRLGokmmgo0TqjpQxnL9pTGDsC47O8/2AmzU7zcVm7eqxx5DkrU/leXVUv6dmdPoajt6S5D1J4yH6yHYDMVZUpE/jjBWnWPTVt8JeM6du8ecrqm4kIcpCcJs8kOq1QJM6aNhNnRicrOLi38ZV/LDzfJm38wQzZzLRrZyxgH4HRvwm6JzbehTlbMrFCUTb1SIRIxTsOHQNktnd68ubD6xQ6hXkrSguPEvuTk= l.farinazzo@criticalcase.com\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCSFL+QRlAdyzi4oDRm9rHoYWxn4RHc2IfqKsHXts8PUE50WgSyCJ+vumAUgi/6y8ohYUxzzsZ7XPWUGVzfUMdjTih7oNczg+GIkjqOijTSo/qnsZNAnSYIkrNBaco8GpbHvLk9qujv6djO85JGv/hPVJJmN22exTpCmXBmJfW8TaP4GHvb7i6CUOUW+eAb5baX4e/qfJlZVoKlXTJfr6BHZ+gSnmL6BSsUxi1Ws24eGoy6G2mGHECW7N22JIoHbzUmsxPmlgtOZ7jgK9PcNCG7LZpLV4KU0lblmmdRT/eBwETfgaR3Jzh7/t6bT4BdwpFoAOA8N4zj0vRmKTUb77737bNPOovLTsZu6ZcN6tkOqQGYqnhKFYFT/oAlganyxWe1EkRrmPYNXvNMmJ5v6sS1NPcMO0i8o+/xCqZIlsp0PhUbw/vNOiPv3x/l3AopJ3eB+Yzm6LkGFH+yYImUR6t3+bSLLBoPgJZPMUa0ZvcBwS0VY6d06KOhrMPfdfVGSmM= l.tetti@criticalcase.com\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQChFnhlmenjVVsbnufRALytj8eekse5sOAqyay+PmiEqUEOofMRSALASVn/bRy4yhiz8JmnMDCH5TPq7gvfpgSy28r2FmGy9Q2MUv4rkCvYjYM2cnADde7RQ4Qujnia1IUBbSWrK0TXQaM9G+20TpVNJ034yUFG1O5tx9QQuUEY5QriPM5MVTOaYdFZt3X6HhulgsebU7q9CqAXHcSD21Tmzru0U54pBOlSR/mgNH8a36vUGkXknZSvr0V1cK3cszmYWUGb5ICkSwF47y9Fs4i7ZScNc9+UgViWeePeZrgrO+5w/U6CvpPZUzDm8jcYDijwP2aVW5ZnR8HYNoD9M0x/HoLVzBQycMvdon9Kszlwp15cjf2TE1ogvzLv2/V3+9ZBVkWn190e6o6VEJW+LPtG98fjdY8xfwgjS7NEvjerJb27DrktC5OnlOIyHIpc3lutSrXitB9jlZdbE6JQN+F+JSwPOJ/M2FEl4oW6x11noH8vr9vbwIz4jmibeG7hfFjXffxFyeV6PcA3E+yLbxrvslTwqEbpJX48ccgeR94if2TC7IRsxnZpvLGFJcpwEYm1khHsQDQFglHn6YWo0jG0yr2wd53dIqoEJn8wc3x5K0SzmQLJ7Z7oD3pJQ7a6g6kZqQrd8MLLBfqKGEzNdPTVdkvLLNU5FpjPASgbURcV1Q== f.pasand@criticalcase.com\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCxddv0l+8lgU4559UvyXVMCJaOzAFRfsyF3YoJla10MrBMYxPht/WkKRkME7QlNNCQ8NxzJhrt4MxrVyxPvFm9rYdRpJeJKGDcxd2k0ykJ1vvvxpX+A9tlBRVd82S6UdvUTbHTiAB0yi/cbY97UFI6tVsVIzEL0zO0GZn/tzkVQxFtgSKn5xsf1mNc8cFNoUuxT3FtHsY5XMcYJYqZMxKfJ0wxVXjNZ9x9PLf1vsjQGOgiP4e4R1qBX4PMfsPQ5qVEGRVzJxQ/C+txuAHcWBNWtNXEKNyxgRSVBYwVkPc/NcfHdpgF4foQvv6f6gCeCJTwds4RnVXliRY7Nl3I9/ZXf3IQw0tEfO+yMfhHO83eRKPfktufTQqhRT7/4256P407938p6SGB5YFyVe0yFd1x2ickiz3YCF20Rs9QEv1oZ7FSlO3Jm3jSszAmPS8QL35HNba18urYriKnQt7bt8YV3SLTPiplZ3sN/WHzggSRo1Mrs7lK6kalAdm8TvVPbbc= s.passaro@criticalcase.com\nEOF\n</code></pre>"},{"location":"team/keys/#update-the-keys-everywhere","title":"Update the keys everywhere","text":"<ul> <li>vdc-digitalsolutions user-data (develop branch)</li> <li>vdc-digitalsolutions rz1 (develop branch)</li> <li>vdc-digitalsolutions rz2 (develop branch)</li> </ul>"},{"location":"team/on-call/","title":"On Call - Reperibilit\u00e0","text":"<p>Ogni luned\u00ec il reperibile della settimana precedente deve:</p> <ul> <li>Assicurarsi di aver cambiato le impostazioni del centralino con il reperibile successivo</li> <li>Avvisare il nuovo reperibile</li> <li>Aggiornare il il foglio excel con il riepilogo della reperibilit\u00e0.</li> </ul> <p>Questo \u00e8 l'ordine della reperibilit\u00e0 con cui viene creato il calendario.</p> <ul> <li>Andrea S</li> <li>Salvatore</li> <li>Andrea B</li> <li>Joel</li> <li>Luana</li> </ul> <p>Ultimo aggiornamento Andrea Sosso dal 31/03/2025 al 06/04/2025</p> <p>Fare riferimento sempre al calendario per impostare la reperibilit\u00e0.</p> <p>Ogni luned\u00ec mattina (Appuntamento 9.00-9.05 Switch Reperibile) il reperibile uscente dalla settimana precedente e quello entrante per la successiva, avranno il compito CONGIUNTO di modificare e verificare il corretto passaggio della reperibilit\u00e0 sul centralino come da procedura sottostante.</p>"},{"location":"team/on-call/#centralino","title":"Centralino","text":"<p>Per cambiare il destinatario delle chiamate fuori orario lavorativo e' necessario accedere via browser all'interfaccia del centralino:</p> <p>https://criticalcase.my3cx.it:5001/</p> <p>Dalla dashboard iniziale usare il men\u00f9 di sinistra, cliccare su \"Ring groups\" e su \"811 RG-ITO-ONCALL-2ND\"; nella scheda che si apre modificare nella sezione \"Membri del gruppo\" il tecnico reperibile togliendo l'esistente ed inserendo quello nuovo. Alla fine salvare.</p> <p>Non inserire il proprio numero di interno dell'ufficio, ma quello specifico chiamato \"Reperibile\", es. \"Andrea S Reperibile\": in questo modo le chiamate non arriveranno sull'app 3CX, ma direttamente sul proprio numero di cellulare.</p>"},{"location":"team/on-call/#numeri-di-emergenza","title":"Numeri di emergenza","text":"<p>I numeri di emergenza vengono usati quando 3CX non \u00e8 disponibile e sono impostati direttamente dal fornitore.</p> <p>In questo momento sono:</p> <ul> <li>Andrea Sosso: +39 3489234879</li> <li>Tito Petronio: +39 3316884096</li> </ul> <p>Per cambiare i numeri occorre inviare una mail a Andrea Bodrati bodrati@all3.biz</p> <p>Buongiorno Andrea,</p> <p>Potresti aggiornare la regole di inoltro quando il trunk \u00e8 KO sulle seguenti numerazioni?</p> <ul> <li>0110888761</li> <li>0110888762</li> </ul> <p>La chiamata deve essere inoltrata a:</p> <ul> <li>+39 XXXXXXXXXX</li> <li>+39 XXXXXXXXXX</li> </ul> <p>Mi potesti dare conferma quando fatto?</p> <p>Grazie mille ancora, buona giornata</p>"},{"location":"team/openvpn/","title":"OpenVPN","text":""},{"location":"team/openvpn/#openvpn-community","title":"OpenVPN Community","text":"<p>Il client OpenVPN Connect NON funziona correttamente, installare la versione community.</p> <ul> <li>Scaricare OpenVPN Community: https://openvpn.net/community-downloads/</li> <li>Importare il file di configurazione</li> <li>Connettersi con credenziali di FreeIPA <code>cc.local</code></li> </ul>"},{"location":"team/openvpn/#file-di-configurazione","title":"File di configurazione","text":"<ul> <li>vpn-to1.ovpn</li> <li>vpn-to2.ovpn (raccomandata)</li> <li>vpn-rz1.ovpn</li> <li>vpn-rz2.ovpn</li> </ul>"},{"location":"team/openvpn/#indirizzi-ip-che-devono-passare-tramite-la-vpn","title":"Indirizzi IP che devono passare tramite la VPN","text":"Text Only<pre><code>176.221.51.52/32 #cc-energie-lesac\n18.185.214.111/32,54.247.98.245/32,3.66.74.207/32 #cc-ds1,cc-ds2,cc-on-premise\n</code></pre>"},{"location":"team/openvpn/#openvpn-connect-for-winodws-deprecated","title":"OpenVPN Connect for Winodws (deprecated)","text":"<p>Usando OpenVPN Connect i server DNS non vengono risolti correttamente.</p> <ul> <li>Scaricare OpenVPN Connect for Winodws https://openvpn.net/client-connect-vpn-for-windows/</li> <li>Creare un file di configurazione come amministratore sotto C:\\Program Files\\conf\\client.ovpn con queste entry:</li> </ul> Bash<pre><code># TO2\nnetsh interface ip set dns \"Local Area Connection\" static 10.152.8.8\nnetsh interface ip set dns \"Local Area Connection\" static 10.152.8.4\n\n# RZ1\nnetsh interface ip set dns \"Local Area Connection\" static 10.154.0.3\nnetsh interface ip set dns \"Local Area Connection\" static 10.154.0.4\n\n# RZ2\nnetsh interface ip set dns \"Local Area Connection\" static 10.156.0.3\nnetsh interface ip set dns \"Local Area Connection\" static 10.156.0.4\n</code></pre> <ul> <li>Lanciare questa stringa in esplora risorse: <code>C:\\Program Files\\OpenVPN Connect\\OpenVPNConnect.exe C:\\Program Files\\conf\\client.ovpn</code></li> <li>Connettersi alla VPN</li> </ul>"},{"location":"team/trasferte/","title":"Trasferte","text":"<p>Dal 1 febbraio 2025 \u00e8 necesssario prenotare Hotel, Treni e Aerei da questa piattaforma:</p> <ul> <li>https://criticalcase.bizaway.com</li> </ul>"},{"location":"tools/AWS/RI/","title":"Report RI","text":""},{"location":"tools/AWS/RI/#discoveryec2region","title":"DiscoveryEC2(region)","text":"<p>Customer, Family, Size, OS</p> <ul> <li>Ottenere tutte le istanze EC2 accese per regione</li> </ul> <p>Aggregare le informazioni per ottenere un output del tipo: OS, Family, Number of EC2</p> <p>(normalized to small): Linux, T3, 50</p> <ul> <li>Togliere il nome del cliente</li> <li>Dividere le istanze linux da quelle windows</li> <li>Normalizzare il size dell\u2019istanza a SMALL</li> </ul>"},{"location":"tools/AWS/RI/#discoveryriregion","title":"DiscoveryRI(region)","text":"<p>Family, Size, OS</p> <ul> <li>Ottenere tutte le istanze EC2 Riservate per regione</li> </ul> <p>Aggregare le informazioni per ottenere un output del tipo: OS, Family, Number of EC2 reserved instances (normalized to small): Linux, T3, 20</p>"},{"location":"tools/AWS/RI/#discoveryri3rdpartyregion","title":"DiscoveryRI3rdparty(region)","text":"<p>Mesi residui, Payment Options (upfront), Offering class (convertible), Quantity available, Normalized units, Effective Rate, Uprfront Price, Hourly Rate</p> <ul> <li>Ottenere tutte le RI di terze parti per ciascuna famiglia con tutti i size disponibili ordinate per mesi residui ASC</li> </ul>"},{"location":"tools/AWS/RI/#ottenere-le-istanze-riservate-da-acquistareregion","title":"Ottenere le istanze riservate da acquistare(region)","text":"<p>DiscoverEC2-DiscoverRI</p> <p>OS, Family, Number of EC2 (normalize to small to buy)</p>"},{"location":"tools/AWS/Route53toBind/","title":"Export Route53 zone to Bind zone","text":"<p>Create a bash script with this content:</p> Bash<pre><code>#!/bin/bash\n# r53_export\n\nusage() {\n  local cmd=$(basename \"$0\")\n  echo -e &gt;&amp;2 \"\\nUsage: $cmd {--id ZONE_ID|--domain ZONE_NAME}\\n\"\n  exit 1\n}\n\nwhile [[ $1 ]]; do\n  if   [[ $1 == --id ]];     then shift; zone_id=\"$1\"\n  elif [[ $1 == --domain ]]; then shift; zone_name=\"$1\"\n  else usage\n  fi\n  shift\ndone\n\nif [[ $zone_name ]]; then\n  zone_id=$(\n    aws route53 list-hosted-zones --output json \\\n      | jq -r \".HostedZones[] | select(.Name == \\\"$zone_name.\\\") | .Id\" \\\n      | head -n1 \\\n      | cut -d/ -f3\n  )\n  echo &gt;&amp;2 \"+ Found zone id: '$zone_id'\"\nfi\n[[ $zone_id ]] || usage\n\naws route53 list-resource-record-sets --hosted-zone-id $zone_id --output json \\\n  | jq -jr '.ResourceRecordSets[] | \"\\(.Name) \\t\\(.TTL) \\t\\(.Type) \\t\\(.ResourceRecords[]?.Value)\\n\"'\n</code></pre> <p>Run the script:</p> Bash<pre><code>./r532bind.sh --id Z1043117388PELEU1ETYT &gt; brunobarbieri.blog.zone\n</code></pre>"},{"location":"tools/AWS/Sostituire_disco_su_AWS/","title":"Sostituire disco VM su AWS","text":""},{"location":"tools/AWS/Sostituire_disco_su_AWS/#best-practice","title":"Best-practice","text":"<p>Sul portale AWS, dopo aver fatto accesso alla pagina della EC2, cliccare sul disco dall'interfaccia dopo aver selezionato il server (copiare l'istance ID e il path in cui si trova il volume, in questo caso /dev/xvda). Cliccare su Actions e Stop (NON TERMINARE L'ISTANZA) </p> <p>Copiare il Volume Id dalla pagina dei volumi, annotarsi la Availability Zone e se il disco \u00e8 criptato anche la KMS Key Id, serviranno per il prossimo passaggio. Aprire in una nuova scheda Snapshots dal menu a sinistra sotto Volumes </p> <p>Incollare il Volume Id nella barra di ricerca e mettere in ordine di data per trovare l'ultimo backup disponibile </p> <p>Cliccare sotto Actions e poi Create Volume</p> <p></p> <p>Rispettare il Tipo di volume, la Dimensione, la Availability Zone e se il disco era criptato, inserire la stessa chiave, non criptarlo se non lo era in precedenza. Tutte queste informazioni si possono recuperare dalla vecchia scheda del volume</p> <p></p> <p>Aspettare che il disco sia Available. Tornare alla schermata del vecchio volume, cliccare Actions e Detach Volumes e confermare il detach</p> <p></p> <p>Spostarsi nella scheda del nuovo volume Actions, poi Attach Volume, indicare l'Instance ID e il patch di dove si trovata il vecchio volume</p> <p></p> <p>Tornare alla schermata delle EC2 e avviare l'istanza Actions e Start Instance</p>"},{"location":"tools/AWS/VPN-S2S/","title":"AWS VPN S2S - pfsense","text":"<ul> <li>https://easycloud.ws/2020/04/setup-site-to-site-vpn-to-aws-with-pfsense/</li> <li>https://getlabsdone.com/aws-site-to-site-vpn-to-pfsense-with-dynamic-routing/</li> </ul>"},{"location":"tools/AWS/VPN-S2S/#aws-side","title":"AWS side","text":""},{"location":"tools/AWS/VPN-S2S/#create-vpn-connection","title":"Create VPN connection","text":"<ul> <li>Routing options: Static</li> <li>Local IPv4 network CIDR (it's the pfsense subnet)</li> <li>Remote IPv4 network CIDR - optional (it's the AWS VPC/subnet)</li> <li>DPD timeout action: Restart</li> <li>Startup action: start</li> </ul>"},{"location":"tools/AWS/VPN-S2S/#routing","title":"Routing","text":"<ul> <li>Remember to set the routing table to the Virtual private gateway.</li> </ul>"},{"location":"tools/AWS/VPN-S2S/#pfsense-side","title":"pfsense side","text":""},{"location":"tools/AWS/VPN-S2S/#firewall-rules","title":"Firewall rules","text":"<ul> <li>Allow IPSEC from the AWS source IP Address in firewall rules</li> </ul>"},{"location":"tools/AWS/VPN-S2S/#ipsec","title":"IPsec","text":"<ul> <li>Use IKEv1</li> <li>Child SA Close Action: restart reconnect</li> <li>Dead Peer Detection: Enable DPD</li> </ul>"},{"location":"tools/AWS/VPN-S2S/#routing_1","title":"Routing","text":"<ul> <li>Create a gateway for LAN</li> <li>Add a static route to the AWS destionation network and select the lan gateway</li> </ul>"},{"location":"tools/AWS/best-practices/","title":"Best Practice AWS","text":"<ul> <li>https deny s3 bucket policy</li> <li>bilanciatore default rule deny</li> <li>password root per accedere tramite serial console</li> <li>taggare le risorse con il tag Name e corrispettivo nome per aumentare la precisione nei discover</li> <li>Migrare/Utilizzare le nuove taglie disponibili, M5 \u2192 M6i, R5 \u2192 R6i, C5 \u2192 C6i (attenzione alle reservation)</li> <li>Abbassare la soglia giornaliera per il controllo dei costi</li> <li>Creare una soglia mensile (forse lo facciamo gi\u00e0 da Techdata)</li> <li>Impostare un allarme sulla percentuali di errori 5xx rilevati dalla CDN o dal LoadBalancer</li> <li>Stream dei logs (system.log,ecc) su Cloudwatch Logs</li> <li>Aumentare il numero di check per unhealthy sul bilanciatore (evitare falsi allarmi)</li> <li>Inserimento pagina statica Cloudfront errori 502/503</li> <li>Notifiche aggiornamento sistema operativo RDS: https://aws.amazon.com/about-aws/whats-new/2022/10/amazon-rds-events-operating-system-updates/</li> </ul>"},{"location":"tools/AWS/disco/","title":"Estendere disco VM su AWS","text":""},{"location":"tools/AWS/disco/#best-practice","title":"Best-practice","text":"<ul> <li> <p>Sul portale AWS, dopo aver fatto accesso alla pagina della EC2, cliccare sul disco dall'interfaccia dopo aver selezionato il server   </p> </li> <li> <p>Cliccare Actions e selezionare Modify Volume, compilando i campi richiesti.    </p> </li> </ul> <p>Seguire la guida ufficiale per estendere il disco a caldo. Linux: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html?icmpid=docs_ec2_console Windows: https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/recognize-expanded-volume-windows.html?icmpid=docs_ec2_console</p>"},{"location":"tools/AWS/disco/#installing-the-growpart-package-on-old-operating-systems","title":"Installing the growpart package on old operating systems","text":"<ol> <li>first, we need to download the source package:</li> </ol> Text Only<pre><code>curl -O -L -k https://launchpad.net/cloud-utils/trunk/0.32/+download/cloud-utils-0.32.tar.gz\n</code></pre> <ol> <li>Extract files:</li> </ol> Text Only<pre><code>tar -xzvf cloud-utils-0.32.tar.gz\n</code></pre> <ol> <li>navigate to the extracted directory:</li> </ol> Text Only<pre><code>cd /cd cloud-utils-0.32/\n</code></pre> <ol> <li>install the source package:</li> </ol> Text Only<pre><code>make insatall\n</code></pre>"},{"location":"tools/AWS/Alarms/AWS_ABUSE_MALWARE_WEBSITE_REPORT/","title":"AWS_ABUSE_MALWARE_WEBSITE_REPORT","text":""},{"location":"tools/AWS/Alarms/AWS_ABUSE_MALWARE_WEBSITE_REPORT/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, c'\u00e8 una risorsa all'interno dell'account AWS del cliente che sta effettuando traffico anomalo o non permesso. AWS \u00e8 stata avvisata di questo e chiede al cliente di porre rimendo, altrimenti sospender\u00e0 le risorse o l'account</p> <p>E' possibile scrivere una email ad ec2-abuse@amazon.com per spiegare la situazione e rispondere al caso</p> <p>Bisogner\u00e0 capire qualche risorsa ha causato questo problema e risolverlo insieme al cliente</p> <p>https://aws.amazon.com/it/premiumsupport/knowledge-center/aws-abuse-report/</p> <p></p>"},{"location":"tools/AWS/Alarms/AWS_LIGHTSAIL_INSTANCE_MAINTENANCE_SCHEDULED/","title":"AWS_LIGHTSAIL_INSTANCE_MAINTENANCE_SCHEDULED","text":""},{"location":"tools/AWS/Alarms/AWS_LIGHTSAIL_INSTANCE_MAINTENANCE_SCHEDULED/#best-practice","title":"Best-practice","text":"<p>E' stato pianificato da AWS uno spegnimento del server, probabilmente per degradazione dell'hardware. Questo implica che bisogna spegnere e accendere il server manualmete in una data precedente a quella di AWS concordata con il cliente. Altrimenti il server verr\u00e0 spento da AWS, ma non verr\u00e0 riacceso.</p> <p>Questo allarme \u00e8 specifico per risorse AWS LIGHTSAIL</p> <p>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instances-status-check_sched.html</p> <p></p>"},{"location":"tools/AWS/Alarms/BurstBalance/","title":"BurstBalance","text":""},{"location":"tools/AWS/Alarms/BurstBalance/#best-practice","title":"Best-practice","text":"<p>Non \u00e8 necessario svegliare il reperibile di secondo livello</p> <p>In questo caso se \u00e8 in orario lavorativo bisogna contattare il cliente in modo proattivo prima che termini il credito. In orario NON H24 \u00e8 consigliato interrompere i cron che si trovano sul server Admin</p> <p>Se si riceve una notifica via email come in questa immagine, significa che la risorsa ha terminato i crediti BurstBalance del volume.</p> <p>Anche se il valore attuale corrispondesse a 0, la risorsa continuerebbe a funzionare ma con prestazioni inferiori. Questo \u00e8 dovuto al fatto che di base un volume gp2 possiede un determinato numero di IOPS dati da dimensione del volume in GB _ 3 IOPS (Esempio: 500GB _ 3 = 1500 IOPS). Se il disco supera questo valore di IOPS utilizza dei crediti forniti da AWS che gli permettono avere prestazioni temporanee migliori soccombendo a questa mancanza di IOPS.</p> <p>AWS DOCUMENTATION: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2</p> <p></p>"},{"location":"tools/AWS/Alarms/CPUUtilization/","title":"CPUUtilization","text":""},{"location":"tools/AWS/Alarms/CPUUtilization/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, che la risorsa AWS non ha pi\u00f9 CPU disponibile. Contattare il cliente per capire se qualcosa sta esaurendo la cpu o \u00e8 il caso di aumentare la taglia dell'istanza</p> <p>AWS DOCUMENTATION: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html</p> <p>IMMAGINE DELL'ALLARME ESATTA NON ANCORA DISPONIBILE</p> <p></p>"},{"location":"tools/AWS/Alarms/DatabaseConnections/","title":"DatabaseConnections","text":""},{"location":"tools/AWS/Alarms/DatabaseConnections/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, significa \u00e8 stato raggiunto il numero massimo di connessioni che l'RDS riuscir\u00e0 a gestire (probabile rallentamento dell'applicazione). Bisogner\u00e0 capire con il cliente come mai ci sono cos\u00ec tante connessioni e se \u00e8 il caso di aumentare la taglia del database</p> <p>AWS DOCUMENTATION: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html</p> <p>IMMAGINE DELL'ALLARME ESATTA NON ANCORA DISPONIBILE</p> <p></p>"},{"location":"tools/AWS/Alarms/EbsAvailable/","title":"Budget","text":""},{"location":"tools/AWS/Alarms/EbsAvailable/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, significa che il budget giornaliero che il cliente \u00e8 disposto a spendere \u00e8 stato superato.</p> <p></p> <p>Prima di svegliare il reperibile di secondo livello, \u00e8 necessario fare una breve analisi almeno per capire quale servizio a superato la soglia. Nell'email sono presenti tutti i link per accedere al cost explorer del cliente</p> <p>Cost explorer \u00e8 il servizio utilizzato per monitorare i cost in AWS (link per accedere nell'email ricevuta).</p> <ul> <li>Generalmente come primo screening si pu\u00f2 raggruppare per SERVICE, impostare il DAILY come frequenza. Sotto i grafici ci sono i costi totali</li> </ul> <p></p> <p>Come si pu\u00f2 vedere da questo esempio, il 17 Agosto ci sono 100 euro in pi\u00f9 nei costi giornalieri, se \u00e8 arrivato l'allarme, significa che questo costo non \u00e8 previsto. Per identificare il servizio \u00e8 sufficiente passarci sopra con il cursore del mouse sul grafico che \u00e8 maggiore rispetto agli altri, o controllare nei singoli costo sotto quale servizio \u00e8 costato di pi\u00f9</p> <p></p> <p>In questo caso il servizio che \u00e8 esploso \u00e8 EC2-Other. Normalmente tutti i servizi in AWS hanno una propria sezione come RDS, EC2 ecc... EC2-Other invece \u00e8 un inglobatore di pi\u00f9 risorse. Per vedere cosa sta succedendo esattamente, \u00e8 utile selezionare come servizi EC2-Other in alto a destra e raggruppare per Usage Type:</p> <p></p> <p>Il costo \u00e8 dato da EU-EBS:VolumeUsage, ci \u00e8 dimenticato di cancellare dei volumi</p> <p>Una volta identificato il servizio svegliare il secondo livello e riportargli che cosa \u00e8 stato identificato</p>"},{"location":"tools/AWS/Alarms/FailedAWSBackup/","title":"FailedAWSBackup","text":""},{"location":"tools/AWS/Alarms/FailedAWSBackup/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, significa che c'\u00e8 un job di AWS Backup dell'account del cliente che \u00e8 fallito. La movitazione \u00e8 scritta nello \"Status Message\" che aiuter\u00e0 a risolvere il problema</p> <p>AWS DOCUMENTATION: https://docs.aws.amazon.com/aws-backup/latest/devguide/troubleshooting.html#troubleshooting-create-backup</p> <p></p>"},{"location":"tools/AWS/Alarms/FreeStorageSpace/","title":"FreeStorageSpace","text":""},{"location":"tools/AWS/Alarms/FreeStorageSpace/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, che la risorsa AWS non ha pi\u00f9 spazio del volume disponibile. Contattare il cliente per capire se qualcosa cancellare dati o espandere la dimensione del disco</p> <p>AWS DOCUMENTATION: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/monitoring-cloudwatch.html</p> <p>IMMAGINE DELL'ALLARME ESATTA NON ANCORA DISPONIBILE</p> <p></p>"},{"location":"tools/AWS/Alarms/FreeableMemory/","title":"FreeableMemory","text":""},{"location":"tools/AWS/Alarms/FreeableMemory/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, che la risorsa AWS non ha pi\u00f9 memoria disponibile. Contattare il cliente per capire se qualcosa sta esaurendo la memoria o \u00e8 il caso di aumentare la taglia dell'istanza</p> <p>AWS DOCUMENTATION: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_BestPractices.html</p> <p>IMMAGINE DELL'ALLARME ESATTA NON ANCORA DISPONIBILE</p> <p></p>"},{"location":"tools/AWS/Alarms/LostSSMConnection/","title":"LostSSMConnection","text":""},{"location":"tools/AWS/Alarms/LostSSMConnection/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, significa che alcune istanze registrate con l'agente SSM sono disconnesse dal System Manager del nostro account. Questa email riguarda l'architettura SSM</p> <p>Indagare all'interno dei servers come mai l'agent non contatta pi\u00f9 AWS</p> <p>AWS DOCUMENTATION: https://aws.amazon.com/it/premiumsupport/knowledge-center/ssm-connection-lost-status/</p> <p></p>"},{"location":"tools/AWS/Alarms/NoIpsAvailableSubnets/","title":"NoIpsAvailableSubnets","text":""},{"location":"tools/AWS/Alarms/NoIpsAvailableSubnets/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, significa gli IPs Address in determinate subnets del cliente sono terminati o vicino allo 0. E' un problema grave del cliente dato che non \u00e8 possibile espandere subnets dopo averle create. Bisogna smettere di creare risorse su quelle subnets, contattare il cliente per pianificare una soluzione (Esempio: creare nuove subnets e creare le risorse su quelle)</p> <p>AWS DOCUMENTATION: https://aws.amazon.com/it/premiumsupport/knowledge-center/subnet-insufficient-ips/</p> <p></p>"},{"location":"tools/AWS/Alarms/RootOperation/","title":"RootOperation","text":""},{"location":"tools/AWS/Alarms/RootOperation/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email simile a questa immagine, il root account ha effettuato operazioni iam</p> <p>Da best practices, l'account root non deve mai essere utilizzato, se non per poche e delicate operazioni in quanto \u00e8 un account amministratore con full access. Bisogna indagare sul perch\u00e9 \u00e8 stata fatta questa operazione</p> <p>AWS DOCUMENTATION: https://docs.aws.amazon.com/accounts/latest/reference/best-practices-root-user.html</p> <p></p>"},{"location":"tools/AWS/Alarms/RootSignIn/","title":"RootSignIn","text":""},{"location":"tools/AWS/Alarms/RootSignIn/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, significa c'\u00e8 stato l'accesso da parte di un root account in un account di un cliente</p> <p>Da best practices, l'account root non deve mai essere utilizzato, se non per poche e delicate operazioni in quanto \u00e8 un account amministratore con full access. Bisogna indagare sul perch\u00e8 \u00e8 stato fatto l'accesso</p> <p>AWS DOCUMENTATION: https://docs.aws.amazon.com/accounts/latest/reference/best-practices-root-user.html</p> <p></p>"},{"location":"tools/AWS/Alarms/S3BucketPublic/","title":"S3BucketPublic","text":""},{"location":"tools/AWS/Alarms/S3BucketPublic/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, significa che il cliente ha all'interno del proprio account dei bucket s3 pubblici. Come best practices, AWS sconsiglia di utilizzare bucket pubblici, ma esporli per esempio attraverso Cloudfront</p> <p>AWS DOCUMENTATION: https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html</p> <p></p>"},{"location":"tools/AWS/Alarms/SNSSubscription/","title":"SNS Subscription","text":""},{"location":"tools/AWS/Alarms/SNSSubscription/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, significa \u00e9 stata richiesta una sottoscrizione SNS da parte di un account (cliente ITO) alla nostra infrastruttura SES/Lambda per ricevere allarmi con una email custom permettendo una migliore analisi. Essendo che l'accettazione avviene in automatico dalle lambda, \u00e8 utile sempre controllare l'account che l'ha richiesto, VERIFICANDO CHE ABBIAMO EFFETTUATO NOI QUESTA OPERAZIONE</p> <p>AWS DOCUMENTATION: https://docs.aws.amazon.com/sns/latest/dg/sns-create-subscribe-endpoint-to-topic.html</p> <p></p>"},{"location":"tools/AWS/Alarms/SSMPatchFailed/","title":"Budget","text":""},{"location":"tools/AWS/Alarms/SSMPatchFailed/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, nell'account del cliente il System Manager del cliente ha un fallimento durante lo scan giornaliero delle patch.</p> <p>Bisogna andare nell'account del cliente e verificare come mai la patch \u00e8 fallita, si pu\u00f2 controllare l'output della scan direttamente dal LINK_RESOURCE all'interno dell'email</p> <p>AWS DOCUMENTATION: https://aws.amazon.com/it/premiumsupport/knowledge-center/ssm-state-manager-association-fail/</p> <p></p>"},{"location":"tools/AWS/Alarms/StatusCheckFailed_Instance/","title":"StatusCheckFailed_Instance","text":""},{"location":"tools/AWS/Alarms/StatusCheckFailed_Instance/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, significa che un server ha avuto un problema di sistema operativo, AWS non interverr\u00e0 automaticamente con un recover del server non essendo sua responsabilit\u00e0. L'instanza probabilmente non sar\u00e0 pi\u00f9 contattabile tramite ssh/teleport/SSM.</p>"},{"location":"tools/AWS/Alarms/StatusCheckFailed_Instance/#logs-del-server-o-screenshot-del-server","title":"LOGS DEL SERVER O SCREENSHOT DEL SERVER","text":"<ul> <li>Instance Actions</li> <li>Monitor and troubleshoot</li> <li>Get instance screenshot</li> <li>Get system log</li> </ul> <p>Probabilmente \u00e8 sufficiente riavviare il server</p>"},{"location":"tools/AWS/Alarms/StatusCheckFailed_Instance/#restart-del-server","title":"Restart del server","text":"<ul> <li>Instance State</li> <li>Reboot Instance</li> <li>Reboot</li> </ul> <p>AWS DOCUMENTATION: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-system-instance-status-check.html</p> <p></p>"},{"location":"tools/AWS/Alarms/StatusCheckFailed_System/","title":"StatusCheckFailed_System","text":""},{"location":"tools/AWS/Alarms/StatusCheckFailed_System/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, significa che un server ha avuto un problema di hardware, se \u00e8 configurato l'azione di recover automatico AWS effettuer\u00e0 il ripristino del server in autonomia (si ricever\u00e0 un'altra email nel giro di pochi minuti con il problema risolto). Altrimenti \u00e8 sufficiente SPEGNERE e RIACCENDERE il server (AWS si occuper\u00e0 di cambiare l'hardware che ospita il server)</p>"},{"location":"tools/AWS/Alarms/StatusCheckFailed_System/#spegnere-il-server","title":"SPEGNERE IL SERVER","text":"<ul> <li>Selezionare il server</li> <li>Instance State</li> <li>Stop Instance</li> <li>Stop</li> </ul>"},{"location":"tools/AWS/Alarms/StatusCheckFailed_System/#accendere-il-server","title":"ACCENDERE IL SERVER","text":"<ul> <li>Selezionare il server</li> <li>Instance State</li> <li>Start Instance</li> <li>Start</li> </ul> <p>AWS DOCUMENTATION: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-system-instance-status-check.html</p> <p>IMMAGINE DELL'ALLARME ESATTA NON ANCORA DISPONIBILE</p> <p></p>"},{"location":"tools/AWS/Alarms/UnHealthyHostCount/","title":"UnHealthyHostCount","text":""},{"location":"tools/AWS/Alarms/UnHealthyHostCount/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, significa che almeno un host attaccato al target group del bilanciatore non ha superato l'health check risultando down. Il bilanciatore non invier\u00e0 pi\u00f9 il traffico su di esso e quindi si avr\u00e0 un host in meno che ricever\u00e0 traffico (\u00e8 possibile che gli altri host diventino sovraccarichi). Bisogner\u00e0 indagare come mai il server non risponde correttamente all'interno del server</p> <p>AWS DOCUMENTATION: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</p> <p></p>"},{"location":"tools/AWS/Alarms/teleport-backup/","title":"Teleport backup","text":""},{"location":"tools/AWS/Alarms/teleport-backup/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, significa che il job di backup del teleport del cliente non si \u00e8 concluso con successo. Come best practices, \u00e8 possibile visualizzare i log del pod che ha eseguito il job di backup per identificare il problema riscontrato:</p> <ul> <li>accedere a rancher con credenziali vpn + otp</li> <li>individuare il pod pi\u00f9 recente corrispondente al cliente del quale si \u00e8 ricevuta la mail di alert</li> <li>click sui 3 punti a destra in corrispondenza del pod individuato -&gt; view logs</li> </ul> <p>Documentazione a teleport backup</p> <p></p>"},{"location":"tools/AWS/Alarms/tools/EbsAvailable/","title":"EbsAvailable","text":""},{"location":"tools/AWS/Alarms/tools/EbsAvailable/#best-practice","title":"Best-practice","text":"<p>Se si riceve una notifica via email come in questa immagine, significa che ci sono dei volumi non attaccati alla macchina che potrebbero non essere necessari e portare a dei costi inutili. Se necessario contattare il cliente per capire se si possono eliminare</p> <p>AWS DOCUMENTATION: https://aws.amazon.com/it/blogs/mt/controlling-your-aws-costs-by-deleting-unused-amazon-ebs-volumes/</p> <p></p>"},{"location":"tools/CMS/magento/","title":"Magento","text":""},{"location":"tools/CMS/magento/#deploy-script-example","title":"Deploy script example","text":"Bash<pre><code>#!/bin/bash\n\nset -x\nset -e\n\nENVNAME=drvranjes\nGITBRANCH=env/40_prod\n\nNOW=$(date +'%F_%Hh%Mm%Ss')\nPRODDIR=/opt/$ENVNAME/releases/production\nBUILDDIR=/opt/$ENVNAME/releases/build_$NOW\nMEDIADIR=/opt/$ENVNAME/releases/media\nURAPIDFLOW_DIR=/opt/$ENVNAME/releases/urapidflow\nENVCONF=/opt/$ENVNAME/releases/env.php\nENVCONFIG=/opt/$ENVNAME/releases/config.php\n\ndate\n\nmkdir -p $BUILDDIR\ngit clone --depth=1 -b $GITBRANCH git@bitbucket.org:aliassrl/drvranjes2.git $BUILDDIR\ncd $BUILDDIR\ncomposer install\n#bin/magento module:disable -c --all\n#bin/magento module:enable --all --clear-static-content\nln -s $ENVCONF $BUILDDIR/app/etc/env.php\nln -s $ENVCONFIG $BUILDDIR/app/etc/config.php\nmv $BUILDDIR/pub/media $BUILDDIR/pub/media_old\nln -s $MEDIADIR $BUILDDIR/pub/media\nln -s $URAPIDFLOW_DIR $BUILDDIR/var/urapidflow\n\nbin/magento setup:di:compile\nbin/magento setup:static-content:deploy it_IT en_US es_ES fr_FR de_DE -j -f\nbin/magento deploy:mode:set production --skip-compilation\nbin/magento maintenance:enable\ndate\n# bin/magento app:config:import --no-interaction\nbin/magento setup:upgrade --keep-generated\nbin/magento cache:clean\nbin/magento maintenance:disable\nln -sfn $BUILDDIR $PRODDIR\nsudo /bin/systemctl reload php7.2-fpm\n\ndate\n</code></pre>"},{"location":"tools/DNS/internal_dns_change/","title":"Internal DNS change","text":"<p>This is a critical server that manages the DNS of all criticalcase clients, be careful what you do</p> <p>The DNS zone must already exist, contact support if not</p> <p>Connect to DNS server</p> Bash<pre><code>ssh root@pxc1.to1.ccws.it\n# or other servers in clusters\nssh root@pxc2.to2.ccws.it\nssh root@pxc3.mi1.ccws.it\n</code></pre> <p>edit the dns zone (default editor is vim)</p> Bash<pre><code>pdnsutil edit-zone example.com\n</code></pre> <p>validate the certificate o insert a new record. Example: </p> <p>insert these records if not present</p> Text Only<pre><code>example.com.  86400   IN      NS      ns1.criticalcase.com\nexample.com.  86400   IN      NS      ns2.criticalcase.com\nexample.com.  86400   IN      NS      ns3.criticalcase.com\nexample.com.  86400   IN      NS      ns4.criticalcase.com\n</code></pre>"},{"location":"tools/GCP/ExtendingDiskSpace/","title":"How to extend the VM disk space","text":"<p>It does not need to shutdown the VM for extending Disk space.</p> <ol> <li>Log in to the GCP account</li> <li>on the left top page, select the project name</li> <li>click on the navigation menu, route to Compute Engine, on the left side pane in the Storage section, click on the Disks &gt; VM names</li> </ol> <p></p> <ol> <li>Click on the Edit and in the Propertise section, we can change the Disk size.</li> </ol> <p></p>"},{"location":"tools/GCP/ExtendingDiskSpace/#debianubuntu","title":"Debian/Ubuntu","text":"Bash<pre><code>sudo apt install -y cloud-utils         # Debian jessie\nsudo apt install -y cloud-guest-utils   # Debian stretch, Ubuntu\nsudo growpart /dev/sda 1\nsudo resize2fs /dev/sda1\n</code></pre>"},{"location":"tools/GCP/ExtendingDiskSpace/#redhatfedoracentos","title":"RedHat/Fedora/CentOS","text":"Bash<pre><code>sudo dnf install -y cloud-utils-growpart\nsudo growpart /dev/sda 1\nsudo xfs_growfs -d /\n</code></pre>"},{"location":"tools/GCP/ExtendingDiskSpace/#windows","title":"Windows","text":"<ol> <li>Right-click the Windows Start button and select Disk Management to open the Disk Management tool.</li> </ol> <ol> <li>Refresh the Disk Management tool so that it recognizes the additional space on your zonal persistent disk. At the top of the Disk Management window, click Action and select Refresh.</li> </ol> <ol> <li>On the disk that you resized, right-click the formatted partition and select Extend Volume.</li> </ol> <ol> <li> <p>Follow the instructions in the Extend Volume Wizard to extend your existing partition to include the extra disk space. If the existing partition is formatted in NTFS, the maximum partition size is limited by its cluster size settings.</p> </li> <li> <p>After you complete the wizard and the volume finishes formatting, check the Status column on the list of attached disks to ensure that the new disk has a Healthy status.</p> </li> </ol> <p></p> <p>You do not need to restart your instance after you complete this process. You can now use the added disk space to store data.</p>"},{"location":"tools/VPN/Ipsec/","title":"Ipsec VPN","text":"<p>E' stata realizzata su una macchina Debian 10 sul nostro datacenter partendo da questo repo: https://github.com/hwdsl2/setup-ipsec-vpn</p> <p>E' una vpn Ipsec L2tp e permette di uscire con l'ip 176.221.51.59. Questo permette di accedere a risorse dei clienti anche se non ci si trova in uffico laddove non sia presente il tunnel di Cloudflare.</p> <p>Per aggiungere un utente:</p> Bash<pre><code>/root/add_vpn_user.sh username password\n</code></pre> <p>Per eliminare un utente</p> Bash<pre><code>/root/del_vpn_user.sh username\n</code></pre> <p>La chiave PSK si trova in /etc/ipsec.secrets</p> <p>Le guide sulla configurazione dei client si trovano qua: https://github.com/hwdsl2/setup-ipsec-vpn/blob/master/docs/clients.md</p>"},{"location":"tools/ansible/ANSIBLE/","title":"Ansible Best Practices","text":"<p>Una raccolta di best practices da seguire per tutti i progretti gestiti tramite ansible.</p>"},{"location":"tools/ansible/ANSIBLE/#nuovo-progetto","title":"Nuovo progetto","text":"<p>Quando viene creato un nuovo progetto \u00e8 raccomandato partire da un template presente in a questo indirizzo: https://github.com/criticalcase/ansible-template.git</p> <p>Procedere quindi con il clone dal template e l'inizializzazione del nuovo repository git.</p> Bash<pre><code>export NEWPROJECT=nome-progetto\ngit clone git@github.com:criticalcase/ansible-template.git ${NEWPROJECT}\ncd ${NEWPROJECT}\nrm -rf .git\ngit init\n</code></pre> <p>Creare il nuovo progetto su https://git.criticalcase.com/projects/new selezinando il progetto il gruppo OPS (Operations) e inserendo il nome del progetto: nome-progetto.</p> <p>Il progetto deve essere di tipo: Private.</p>"},{"location":"tools/ansible/ANSIBLE/#changelog","title":"Changelog","text":"<p>Il file di changelog deve essere aggiornato ogni volta che si cambia qualcosa nel progetto. Pu\u00f2 essere relativa ad un commit su git, oppure ad una riconfigurazione fatta fuori da ansible.</p> <p>Ad esempio \u00e8 utile inserire il numero di ticket o il codice ERP per un upgade.</p> <p>Qui potete trovare le istruzioni su come aggiornare un file changelog: https://keepachangelog.com/it-IT/1.0.0/</p>"},{"location":"tools/ansible/ANSIBLE/#ccreadme-documentazione-sulle-vm","title":"cc.readme: documentazione sulle VM","text":"<p>Le operazioni pi\u00f9 comuni devono essere documentate all'interno della VM nel file /root/README.md. Il ruolo https://git.criticalcase.com/ansible/cc.readme aiuta a inserire informazioni importanti su tutta l'infrastruttura, come il codice <code>ERP CRXXXXXX</code>, <code>IDOP XXXX</code>, <code>JIRA CC-XXXXX</code>.</p> <p>Un esempio del contenuto \u00e8 sicuramente quello delle istruzioni per aggiungere un certificato TLS all'ambiente <code>cc.local</code>.</p>"},{"location":"tools/ansible/ANSIBLE/#readme","title":"README","text":"<p>In README del progetto deve contenere i comandi utili e quelli usati pi\u00f9 frequentemente per eseguire il deployment.</p> Bash<pre><code>#\u00a0README\nansible-playbook -i hosts default.yml --tags facts,readme\n\n# DNS\nansible-playbook -i hosts -l pdns_recursor pdns-recursor.yml\n</code></pre> <p>Fare riferimento a cc.local/README.md.</p>"},{"location":"tools/ansible/ANSIBLE/#ruoli-ansible","title":"Ruoli ansible","text":"<p>Molte Best Practices sono gi\u00e0 integrate nei ruoli gestiti da noi. E' sempre preferibile usare un ruolo interno, migliorandolo di volta in volta, piuttosto che un ruolo esterno di dubbia qualit\u00e0.</p> <p>Lista completa dei ruoli interni: https://git.criticalcase.com/ansible</p>"},{"location":"tools/ansible/ANSIBLE/#infrastrutture-con-loadbalancer","title":"Infrastrutture con LoadBalancer","text":"<p>Come Load Balancer usiamo per tutti i nuovi progetti HAProxy 2.x con il ruolo cc.haproxy.</p> <p>Un esempio di architettura pu\u00f2 essere quella di buffetti</p>"},{"location":"tools/ansible/ANSIBLE/#haproxy","title":"HAProxy","text":"<p>Accesso SSH solo indirizzo ip privato</p> <ul> <li>buffetti-lb01.local.ccws.it 10.101.0.196</li> </ul> <p>Firewall che permette HTTP/HTTPS su ip pubblico</p> <ul> <li>buffetti-lb.ccws.it 176.221.53.56 80/443</li> </ul> <p>Esempio di interfacce di rete:</p> Text Only<pre><code>eth0: 10.101.0.X foreman\neth1: 10.12.0.1 rete privata del cliente\neth2: 176.221.X.X rete pubblica\n</code></pre> <p>Questa macchina far\u00e0 anche da gateway (con il nat MASQUERADE sull'interfaccia pubblica es. buffetti)</p>"},{"location":"tools/ansible/ANSIBLE/#vm-interne-non-esposte-su-internet","title":"VM interne non esposte su internet","text":"<p>Esempio di interfacce di rete:</p> Text Only<pre><code>eth0: 10.101.0.X foreman\neth1: 10.12.0.X rete privata del cliente\n</code></pre> <p>Tutti i servizi, in particolare quelli configurati senza autenticazione devono essere messi in ascolto solo su <code>localhost</code> e la rete privata del cliente <code>10.12.0.X</code>.</p> <p>Tutte le VM interne devo usare il load balancer come gateway sulla rete privata del cliente, es:</p> Text Only<pre><code>auto eth0\nallow-hotplug eth0\niface eth0 inet static\n    address 10.101.0.X\n    netmask 255.255.255.0\n    dns-nameservers 8.8.8.8 1.1.1.1\n    dns-search local.ccws.it\n\nauto eth1\nallow-hotplug eth1\niface eth1 inet static\n   address 10.12.0.X\n   netmask 255.255.255.0\n   gateway 10.12.0.1\n</code></pre> <p>Esempio di routing:</p> Text Only<pre><code>$\u00a0route -n\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n0.0.0.0         10.12.0.1       0.0.0.0         UG    0      0        0 eth1\n10.12.0.0       0.0.0.0         255.255.255.0   U     0      0        0 eth1\n10.101.0.0      0.0.0.0         255.255.255.0   U     0      0        0 eth0\n</code></pre>"},{"location":"tools/ansible/UPGRADE/","title":"Ansible upgrade","text":""},{"location":"tools/ansible/UPGRADE/#from-29x-to-210x","title":"From 2.9.x to 2.10.x","text":""},{"location":"tools/ansible/UPGRADE/#remove-old-ansible-version","title":"Remove old ansible version","text":"Bash<pre><code>\u279c pip3 list | grep ansible\nansible                         2.9.5\nansible-base                    2.9.5\n\n\u279c pip3 uninstall ansible ansible-base\n</code></pre>"},{"location":"tools/ansible/UPGRADE/#install-ansible-210x","title":"Install ansible 2.10.x","text":""},{"location":"tools/ansible/UPGRADE/#ansible-pip-version-is-340","title":"Ansible PIP version is 3.4.0","text":"Bash<pre><code>\u279c pip3 install ansible==3.4.0\n\u279c pip3 list | grep ansible\nansible                         3.4.0\nansible-base                    2.10.14\n</code></pre>"},{"location":"tools/ansible/UPGRADE/#update-the-ansiblecfg","title":"Update the ansible.cfg","text":"Text Only<pre><code>[defaults]\ncollections_path = collections\nstrategy_plugins = plugins/mitogen-0.3.0rc1/ansible_mitogen/plugins/strategy\n</code></pre>"},{"location":"tools/ansible/UPGRADE/#upgrade-mitogen","title":"Upgrade mitogen","text":"<p>Install the new mitogen plugin: https://github.com/mitogen-hq/mitogen/releases</p> Bash<pre><code>cd plugins\nwget https://github.com/mitogen-hq/mitogen/archive/refs/tags/v0.3.0rc1.tar.gz\ntar -xzvf v0.3.0rc1.tar.gz\nrm v0.3.0rc1.tar.gz\nrm -rf mitogen-0.2.9/\n</code></pre>"},{"location":"tools/azure/custompfsense/","title":"pfSense on Azure","text":"<p>Follow these steps to build your own pfSense custom image and upload it to Azure. Using, for example in KTM customer enviroment.</p>"},{"location":"tools/azure/custompfsense/#create-local-vm","title":"Create local vm","text":"<ol> <li>Download the iso from pfsense.org (x64 - last version)</li> <li>install Hyper-V on Windows 10 client</li> <li>In Hyper-V console create a new virtual switch and choose external network then check \"Allow managemente operating system to share this network adapter\"</li> <li>Create virtual hard disk with:    Virtual hard disk format: VHD    Virtual hard disk type: Fixed size    Virtual hard disk size: You can choose the size, but 16GB should be sufficient</li> <li>Create the VM with:    Virtual machine generation: generation 1    Amount of memory: 1GB should be sufficient    Dynamic memory: disabled    Network connection: connected to the virtual switch we prepared    Virtual hard disk: the exisiting disk we prepared</li> <li>Add DVD, Nic and disable checkpoint in the VM:    Add Network adapter to vm in vm settings then connected to the External virtual switch we prepared    In Ide controller 1, DVD drive, use a downloaded pfSense iso    Settings -&gt; Management -&gt; Checkpoint: disable</li> <li>Power on VM then install pfSense as usual, reboot and remove the iso from VM settings</li> <li>Configure interfaces from pfSense console:    WAN -&gt; hn0    LAN -&gt; hn1</li> <li>Assign a temporary fixed address (no gw) to the LAN port</li> <li>**enable ssh by option 14 from main menu</li> </ol>"},{"location":"tools/azure/custompfsense/#install-necessary-packages","title":"Install necessary packages","text":"<p>Using ssh (or menu 8 option)</p> <ol> <li>Upgrade the package</li> </ol> Bash<pre><code>pkg upgrade\n</code></pre> <ol> <li>Install sudo, bash and git and creating symlink</li> </ol> Bash<pre><code>pkg install -y sudo bash git\nln -s /usr/local/bin/python3.8 /usr/local/bin/python\n</code></pre> <ol> <li>Install the Linux Agent for Azure</li> </ol> Bash<pre><code># Clone the Git repository\ngit clone https://github.com/Azure/WALinuxAgent.git\n# Enter the WALinuxAgent directory\ncd WALinuxAgent\n# List all available versions\ngit tag\n# Checkout the latest (stable) version of the agent\ngit checkout v2.3.1.1\n# Install the agent\npython setup.py install\n</code></pre> <ol> <li>create waagent symlink</li> </ol> Bash<pre><code>ln -sf /usr/local/sbin/waagent /usr/sbin/waagent\n</code></pre>"},{"location":"tools/azure/custompfsense/#configure-the-lan-interface","title":"Configure the Lan interface","text":"<ol> <li>Go on web interface (default login is the same as the SSH login :admin/pfsense)</li> <li>Go to menu Interfaces &gt; LAN. Change \u201cIPv4 Configuration Type\u201d to DHCP</li> <li>Disable Block private networks and loopback addresses and Block bogon networks and save</li> </ol>"},{"location":"tools/azure/custompfsense/#optionally-grant-access-via-wan-to-the-web-interface-and-ssh","title":"OPTIONALLY: Grant access via WAN to the Web interface and ssh","text":"<ul> <li> <p>otherwise you must access on the LAN interface by vm place on the same subnet</p> </li> <li> <p>Go to Firewall -&gt; Rules -&gt; WAN</p> </li> <li>Insert 2 rules:</li> </ul> Bash<pre><code>action: pass\nprotocol: TCP\nSource: *\nSource Port: *\nDestination: WAN address\nDestination Port: 443 (HTTPS)\n</code></pre> Bash<pre><code>action: pass\nprotocol: TCP\nSource: *\nSource Port: *\nDestination: WAN address\nDestination Port: 22 (SSH)\n</code></pre>"},{"location":"tools/azure/custompfsense/#deploy-pfsense-in-azure","title":"Deploy pfSense in Azure","text":"<ul> <li> <p>Upload the VHD file in a blob container using the Microsoft Azure Storage Explorer (https://azure.microsoft.com/en-us/features/storage-explorer/) and select as blob type: \u201cPage blob\u201d</p> </li> <li> <p>On Windows PowerShell run as Administrator (ver 7 with \"Install-Module -Name Az\"):   Note: you can change name, ip, subnet, etc as you want</p> </li> <li> <p>Network Resources</p> </li> </ul> Bash<pre><code># Enable AzureRM alias\nEnable-AzureRmAlias\n\n# Login\nLogin-AzureRmAccount\n\n# List all subscriptions, available under your account\nGet-AzureRmSubscription\n\n# Select the right subscription in this example is Digital Solution\n Select-AzureRmSubscription dbe224e1-d261-4619-a880-b9f5b8c7b1d9\n\n# Create a new resource group\nNew-AzureRmResourceGroup -Name eu-network-rg -Location 'West Europe'\n$VirtualNetwork = New-AzureRmVirtualNetwork -ResourceGroupName eu-network-rg -Location 'West Europe' -Name eu-vdc-vnet -AddressPrefix \"10.0.0.0/24\"\n\n# Configure all subnets\nAdd-AzureRmVirtualNetworkSubnetConfig -Name frontend -VirtualNetwork $VirtualNetwork -AddressPrefix \"10.0.0.32/27\"\nAdd-AzureRmVirtualNetworkSubnetConfig -Name backend -VirtualNetwork $VirtualNetwork -AddressPrefix \"10.0.0.64/27\"\n$VirtualNetwork | Set-AzureRmVirtualNetwork\n</code></pre> <ol> <li>Disk Resources</li> </ol> Bash<pre><code>$storageType = \"Standard_LRS\"\n$location = \"West Europe\"\n$storageAccountId = \"/subscriptions/dbe224e1-d261-4619-a880-b9f5b8c7b1d9/resourceGroups/cloud-shell-storage-westeurope/providers/Microsoft.Storage/storageAccounts/pfsensevhdimage\"\n$sourceVhdUri = \"https://pfsensevhdimage.blob.core.windows.net/vhdimages/pfSense.vhd\"\nNew-AzureRmResourceGroup -Name eu-firewalls-rg -Location 'West Europe'\n\n# Create the disk configuration\n$diskConfig = New-AzureRmDiskConfig -AccountType $storageType -Location $location -CreateOption Import -StorageAccountId $storageAccountId -SourceUri $sourceVhdUri\n\n# Create the Managed Disk\nNew-AzureRmDisk -Disk $diskConfig -ResourceGroupName eu-firewalls-rg -DiskName eu-pfsense-1-os\n</code></pre> <ol> <li>pfSense Virtual Machine</li> </ol> Bash<pre><code># Get the object of the existing Managed Disk\n$disk = Get-AzureRmDisk -DiskName eu-pfsense-1-os -ResourceGroupName eu-firewalls-rg\n\n# Get the object for the existing Virtual Network\n$VirtualNetwork = Get-AzureRmVirtualNetwork -Name eu-vdc-vnet -ResourceGroupName eu-network-rg\n\n# Create a new Virtual Machine object\n$virtualMachine = New-AzureRmVMConfig -VMName eu-pfsense-1 -VMSize Standard_B2s\n\n# Attach the existing Managed Disk to the Virtual Machine\n$virtualMachine = Set-AzureRmVMOSDisk -VM $virtualMachine -ManagedDiskId $disk.Id -CreateOption Attach -Linux\n\n# Create the Public IP Address\n$pip = New-AzureRmPublicIpAddress -ResourceGroupName eu-firewalls-rg -Location 'West Europe' -Name eu-pfsense-pip -AllocationMethod Dynamic\n\n# Create the NIC's for the frontend (and assign public ip) and the backend\n$frontEndNic = New-AzureRmNetworkInterface -Name eu-pfsense-1-frontend-nic -ResourceGroupName eu-firewalls-rg -Location 'West Europe' -SubnetId $VirtualNetwork.Subnets[0].Id -PublicIpAddressId $pip.Id -PrivateIpAddress 10.0.0.36\n$backEndNic = New-AzureRmNetworkInterface -Name eu-pfsense-1-backend-nic -ResourceGroupName eu-firewalls-rg -Location 'West Europe' -SubnetId $VirtualNetwork.Subnets[1].Id -PrivateIpAddress 10.0.0.68\n\n# Add the NIC's to the Virtual Machine\n$virtualMachine = Add-AzureRmVMNetworkInterface -VM $virtualMachine -Id $frontEndNic.Id -Primary\n$virtualMachine = Add-AzureRmVMNetworkInterface -VM $virtualMachine -Id $backEndNic.Id\n\n# Create the Virtual Machine\nNew-AzureRmVM -VM $virtualMachine -ResourceGroupName eu-firewalls-rg -Location 'West Europe'\n</code></pre>"},{"location":"tools/azure/custompfsense/#optionally-test-virtual-machine","title":"OPTIONALLY Test Virtual Machine","text":"Bash<pre><code># Create the NIC for the Virtual Machine\n$nic = New-AzureRmNetworkInterface -Name eu-pfsense-mgmt-nic -ResourceGroupName eu-firewalls-rg -Location 'West Europe' -SubnetId $VirtualNetwork.Subnets[1].Id -PrivateIpAddress 10.0.0.69\n\n# Create the Virtual Machine Object\n$virtualMachine = New-AzureRmVMConfig -VMName eu-pfsense-mgmt -VMSize \"Standard_B1s\"\n\n# Prepare the Virtual Machine for Windows\n$virtualMachine | Set-AzureRmVMOperatingSystem -Windows -ComputerName eu-pfsense-mgmt -Credential (Get-Credential)\n\n# Configure the Virtual Machine for Windows 2016 Datacenter\n$virtualMachine | Set-AzureRmVMSourceImage -PublisherName MicrosoftWindowsServer -Offer WindowsServer -SKus 2016-Datacenter -Version latest\n\n# Add the NIC to the Virtual Machine\n$virtualMachine | Add-AzureRmVMNetworkInterface -Id $nic.Id\n\n# Create the Virtual Machine\nNew-AzureRmVM -ResourceGroupName eu-firewalls-rg -Location 'West Europe' -VM $virtualMachine\n</code></pre>"},{"location":"tools/azure/custompfsense/#settings-on-azure-portal-place-the-vm-in-powered-off-state","title":"Settings on Azure portal (place the vm in powered off state)","text":"<ol> <li>Edit WAN NIC and ensure both public and private IP is set to Static</li> <li>Edit LAN NIC and ensure private IP is set to Static</li> <li>Go into both fw's NICs and enable IP forwarding (IMPORTANT!)</li> <li>Remove the Network Security Groups that was assigned to your WAN and LAN NIC</li> <li>Open Azure's Route Tables and create one for the LAN subnet - set it to 0.0.0.0/0 and point it to the fw's LAN NIC and associate to backend subnet</li> <li>Connect to the public IP and run through the webconfig to setup the appliance</li> <li>Set Outbound NAT to Manual, delete all auto-generated rules, and create the following three:    Source: WAN net (ex: 10.0.0.32 /27) ; NAT Address: WAN address ; leave the rest set to defaults    Source LAN net (ex: 10.0.0.64 /27) ; NAT Address: WAN address ; leave the rest set to defaults    Source localhost (ex: 127.0.0/8) ; NAT Address: WAN address ; leave the rest set to defaults</li> <li>Setup any port forwarding/NAT rules you need to allow access / traffic to your VM(s) in the LAN for example open on WAN interface access rdp to test machine:    Interface: WAN    Protocol: TCP    Source Address: _    Source Port: _    Dest. Addr: WAN address    Dest. Ports: 3389    NAT Ip: 10.0.0.69    NAT Ports: 3389</li> <li>IMPORTANT you must replicate in Firewall -&gt; Rules -&gt; WAN:    Action: pass    Protocol: ipv4 TCP    Source: _    Port: _    Destination: 10.0.0.69    Port: 3389</li> <li>Test you TDP Port Forward!!!</li> </ol>"},{"location":"tools/azure/custompfsense/#assign-multiple-public-ip-to-wan-interface","title":"Assign multiple Public IP to WAN Interface","text":"<ul> <li> <p>On Azure portal:</p> </li> <li> <p>In the portal, click Create a resource &gt; Networking &gt; Public IP address</p> </li> <li>In the Create public IP address pane that appears, enter a Name, select an IP address assignment type, a Subscription, a Resource group, and a Location, then click Create</li> <li> <p>In the pfSense network interface (in our example eu-pfsense-1-frontend-nic):</p> </li> <li> <p>go in IP configurations</p> </li> <li> <p>click Add and:   Name: ip1   Allocation: Static   IP address: 10.0.0.40 #In our example, choose an private wan free ip   Public IP Address: Associate   Public IP Address: Choose a public ip previously created</p> </li> <li> <p>Add new ip to pfSense:    Go to pfSense Web Interface then Firewall -&gt; Virtual IP    Click Add:    Type: IP Alias    Interface: WAN    Address Type: Single Address    Address(es): 10.0.0.40 (in our example)    Description: your new public IP</p> </li> <li>Setup any port forwarding/NAT rules you need on new IP</li> <li>If you want to force a backand vm to exit with the new ip adjust the outbound rules</li> </ul> <p>Resources:</p> <ul> <li>https://www.christofvg.be/2019/01/12/pfSense-on-Azure-Part-3-Deploy-pfSense-in-Azure/</li> <li>https://www.cloudcorner.gr/microsoft/azure/custom-pfsense-on-azurerm-a-complete-guide/</li> <li>https://forum.netgate.com/topic/146832/pfsense-in-azure</li> <li>https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-multiple-ip-addresses-portal</li> </ul>"},{"location":"tools/azure/disco/","title":"Estendere disco VM su Azure","text":"<p>Azure non permette di ridimensionare i dischi esistenti senza spegnere la macchina. \u00c8 per\u00f2 possibile aggiungere dischi dati e sfruttare LVM.</p>"},{"location":"tools/azure/disco/#estendere-il-disco-principale","title":"Estendere il disco principale","text":"<p>Dal portale azure:</p> <ul> <li>Spegnere la VM</li> <li>Andare su Disks e selezionare il disco sotto OS disk</li> <li>Andare su Size + performance</li> <li>Selezionare il nuovo taglio del disco es. 64GB</li> <li>Impostare il performance Tier di default</li> <li>Premere save</li> <li>Accendere la VM</li> </ul>"},{"location":"tools/azure/disco/#console-della-vm","title":"Console della VM","text":"Bash<pre><code>parted /dev/sdb print free\n\nNumber  Start         End            Size          File system  Name                  Flags\n        17408B        1048575B       1031168B      Free Space\n14      1048576B      5242879B       4194304B                                         bios_grub\n15      5242880B      524288511B     519045632B    fat16        EFI System Partition  boot, esp\n        524288512B    525336575B     1048064B      Free Space\n 1      525336576B    1049624575B    524288000B    xfs\n 2      1049624576B   68719459839B   67669835264B  xfs\n        68719459840B  137438936575B  68719476736B  Free Space\n</code></pre> <p>Effettuare il resize usando tutto lo spazio disponibile (ultima linea della colonna End</p> Text Only<pre><code>parted resizepart 2 137438936575B\n</code></pre> <p>Estendere il disco in base al filesystem</p>"},{"location":"tools/azure/disco/#xfs","title":"XFS","text":"Text Only<pre><code>xfs_growfs /\n</code></pre>"},{"location":"tools/azure/disco/#ext4","title":"EXT4","text":"Text Only<pre><code>resize2fs /\n</code></pre>"},{"location":"tools/azure/disco/#aggiungere-un-disco-lvm","title":"Aggiungere un disco (LVM)","text":"<ul> <li>Sul portale Azure, dopo aver fatto accesso alla pagina della VM in esame, dal menu a sinistra selezionare Disks sotto Settings</li> </ul> <ul> <li>Sotto Data disks selezionare Create and attach a new disk, compilando i campi richiesti.</li> </ul> <p>Il numero di dischi creabili \u00e8 limitato e varia a seconda del modello di VM.</p> <p></p> <ul> <li>Selezionare Save per completare la creazione del disco.</li> <li>Seguire infine la normale procedura LVM per estendere lo spazio sulla macchina.</li> </ul>"},{"location":"tools/backup/","title":"Backup","text":""},{"location":"tools/backup/#bash-backup-scripts","title":"Bash Backup Scripts","text":"<p>various bash backup script, please see: https://github.com/criticalcase/bash-backup-scripts</p>"},{"location":"tools/backup/#nakivo-backup-add-vm-to-backup-job","title":"Nakivo Backup (add VM to backup job)","text":""},{"location":"tools/backup/#pay-attention","title":"PAY ATTENTION","text":"<p>**All VMs under Production@000000 folder will be added automatically under backup job. Anyway please check the backup job and follow this guide in case the VM are not found in the backup job **</p> <p>Credentials: https://sherlock.criticalcase.cloud/ui/vault/secrets/kv/show/ds/services/nakivo URL: https://nakivo1.rz1.vdc.ccws.it/c/mt</p> <ul> <li>login and go to CriticalCase tenant</li> </ul> <p></p> <ul> <li>click on Digital-solution job then Manage -&gt; edit</li> </ul> <p></p> <ul> <li>In edit settings, switch View = VMs &amp; Templates then select the VMs tha you want to put under backup (the ITO VMs are under Criticalcase@C0001 -&gt; Production@000000 folder)</li> </ul> <p></p> <ul> <li> <p>In Destination tab, click on advanced then expand all VMs.   Select Target destination:   _ ZFSR01DigitalSolution for VMs on RZ1 Datacener   _ ZFSR02DigitalSolution for VMs on RZ2 Datacener.</p> <p></p> </li> </ul> <p>Click Save</p> <p>Please, pay attention on remained used on lower left and used space on backup storage</p> <p>You can view license on lower left screen in job edit settings </p> <p>You can view used space on destination tab in the edit setting  and in the job info tab </p> <p>Now write an email to r.razzano@criticalcase.com with number of remained licenses. If the space is close to running out, ask to Roberto to increase it.</p>"},{"location":"tools/backup/restore/","title":"Restore","text":""},{"location":"tools/backup/restore/#vmware-on-premise","title":"VMWare on-premise","text":""},{"location":"tools/backup/restore/#veeam","title":"Veeam","text":"<p>In attesa di condivisione procedura di restore, scalare sul supporto interno.</p>"},{"location":"tools/backup/restore/#nakivo","title":"Nakivo","text":"<p>In attesa di condivisione procedura di restore, scalare sul supporto interno.</p>"},{"location":"tools/backup/restore/#snapshot","title":"Snapshot","text":"<p>Se non fosse possibile ripristinare la VM da un backup con Veeam o Nakivo scalare sul supporto interno infrastruttura.</p>"},{"location":"tools/backup/restore/#aws","title":"AWS","text":"<ul> <li>https://docs.aws.amazon.com/prescriptive-guidance/latest/backup-recovery/restore.html</li> </ul>"},{"location":"tools/backup/restore/#azure","title":"Azure","text":"<ul> <li>https://docs.microsoft.com/it-it/azure/backup/backup-azure-arm-restore-vms</li> </ul>"},{"location":"tools/backup/restore/#gcp","title":"GCP","text":"<ul> <li>https://cloud.google.com/compute/docs/disks/create-snapshots#restore-snapshots</li> </ul>"},{"location":"tools/backup/restore/#tencent","title":"Tencent","text":"<ul> <li>https://intl.cloud.tencent.com/document/product/362/5756</li> </ul>"},{"location":"tools/cloud/tencent/","title":"Tencent","text":"<p>Login to criticalcase access account:</p> <ul> <li>https://www.tencentcloud.com/login/subAccount/200030041042?type=subAccount&amp;username=a.sosso@criticalcase.com</li> </ul>"},{"location":"tools/github/actions-alerts/","title":"GitHub Action Alerts","text":"<p>By design, GitHub only sends alerts via mail if the action fails only to the user that applied the latest change to the action or who started it manually. There are no out of the box solutions to notify more than one user, as explaned in this discussion.</p>"},{"location":"tools/github/actions-alerts/#mattermost-channel","title":"Mattermost channel","text":"<p>As a solution, a mattermost channel called <code>\u2757Allarmi GitHub Actions\u2757</code> has been created where all alerts can be sent.</p> <p>A webhook has been created and can be used for all repos in the Criticalcase organization. It can be retrieved from Sherlock and can be used as a secret that must be added in the repo, for example here is called MATTERMOST_WEBHOOK.</p> <p>In order to configure the action to send the alert in case of failure, the following step must be added in the action:</p> Text Only<pre><code>    - name: Notify mattermost of failure\n      if: failure()\n      uses: mattermost/action-mattermost-notify@master\n      with:\n        MATTERMOST_WEBHOOK_URL: ${{ secrets.MATTERMOST_WEBHOOK }}\n        MATTERMOST_CHANNEL: allarmi-github-actions\n        TEXT: |\n          This is a message from ${{ github.repository }}.\n          [&lt;Name of the Action&gt;](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) has failed execution :x:\n        MATTERMOST_USERNAME: ${{ github.triggering_actor }}\n        MATTERMOST_ICON_URL: https://cdn3.iconfinder.com/data/icons/system-basic-vol-4-1/20/icon-note-attention-alt3-512.png\n</code></pre> <p>Example of real use</p> <p>If the notification channel should be used in a different organization shared with a customer, it is strongly suggested to create a new webhook that can be used only on that organization, even if linked to the same channel.</p>"},{"location":"tools/github/actions-runner-controller/","title":"GitHub Actions Runner Controller (ARC)","text":"<p>These resources are used to automatically create pods to exec github actions on premise.</p>"},{"location":"tools/github/actions-runner-controller/#operator","title":"Operator","text":"<p>The GitHub Actions Runner Controller is used to manage the scaling and the scheduling of the pod. Authentication to GitHub is managed via GitHub app, and credentials are stored in the github-runner-app secret.</p> <p>It is made of two main components:</p> <ul> <li>scale set controller: in charge of contacting GitHub to register runners and to deploy pods when actions start</li> <li>scale set: identify a pool of runners, it can be customized to define to which organization or repository the pool is bound, how many pods define the pool and which image should be used for the runners</li> </ul> <p>Both the components are installed via helm charts, here it is possible to find latest versions:</p> <ul> <li>scale set controller: https://github.com/actions/actions-runner-controller/tree/master/charts/gha-runner-scale-set-controller</li> <li>scale set: https://github.com/actions/actions-runner-controller/tree/master/charts/gha-runner-scale-set</li> </ul>"},{"location":"tools/github/actions-runner-controller/#runner-image","title":"Runner image","text":"<p>The image used for runners is built using the pipeline defined in the ci-github-action-runner repository, based on the official GitHub runner image. The output is stored in the public ECR of the <code>criticalcase</code> aws account.</p>"},{"location":"tools/github/actions-runner-controller/#management","title":"Management","text":""},{"location":"tools/github/actions-runner-controller/#charts-upgrade","title":"Charts upgrade","text":"<p>To upgrade the helm chart versions, if there are not breaking changes, it is possible to update the version in the controller.yaml file and in each of the scale set files, for example here.</p>"},{"location":"tools/github/actions-runner-controller/#image-upgrade","title":"Image upgrade","text":"<p>This is often mandatory, because GitHub stops compatibility with old images after few months.</p> <ol> <li>Retrieve the latest version from the Release page and check if there are breaking changes</li> <li>Update the version https://github.com/criticalcase/ci-github-action-runner/blob/main/Dockerfile#L1 and the readme</li> <li>Push the new tag to start the pipeline</li> <li>Update the version in all the scale sets, for example here</li> <li>use flux reconcile to restore the resources in all clusters</li> </ol> Text Only<pre><code>flux reconcile kustomization flux-system --with-source\n</code></pre> <ol> <li>go to the <code>arc-runners</code> namespace with</li> </ol> Text Only<pre><code>kubie ns arc-runners\n</code></pre> <ol> <li>IF still no pods are running, retrieve all the helmrelease resources and delete all of them, to clean all the scale set</li> </ol> Text Only<pre><code>kubectl get helmrelease\nkubectl delete &lt;insert all helmreleases&gt;\n</code></pre> <ol> <li>use flux reconcile to restore the resources</li> </ol> Text Only<pre><code>flux reconcile kustomization flux-system --with-source\n</code></pre>"},{"location":"tools/github/actions-runner-controller/#debug","title":"Debug","text":"<p>This procedure can help checking if the runners are no more working because of version incompatibility:</p> <ol> <li>go to the <code>arc-runners</code> namespace with</li> </ol> Text Only<pre><code>kubie ns arc-runners\n</code></pre> <ol> <li>check if there are no pods (if there are pods, it is not related to the version)</li> </ol> Text Only<pre><code>kubectl get pods\n</code></pre> <ol> <li>retrieve all the helmrelease resources and delete all of them, to clean all the scale set</li> </ol> Text Only<pre><code>kubectl get helmrelease\nkubectl delete &lt;insert all helmreleases&gt;\n</code></pre> <ol> <li>use flux reconcile to restore the resources</li> </ol> Text Only<pre><code>flux reconcile kustomization flux-system --with-source\n</code></pre> <ol> <li>wait a few seconds, than you should see the pod started to be created. The startup will fail, so it is important to check the logs before the pod is killed.</li> </ol> Text Only<pre><code>kubectl get pods\nkubectl logs &lt;pod name&gt;\n# Check if this error is present in the logs:\n# WRITE ERROR: An error occured: Runner version v2.317.0 is deprecated and cannot receive messages.\n</code></pre>"},{"location":"tools/lvm/LVM/","title":"LVM","text":""},{"location":"tools/lvm/LVM/#liberare-spazio","title":"Liberare spazio","text":"<p>Prima di estendere un disco in emergenza \u00e8 meglio controllare se \u00e8 possibile liberare spazio in altri modi.</p> <p>Si possono usare tool come ncdu. Oppure vedere se \u00e8 possibile cancellare qualche log di journalctl, es.:</p> Bash<pre><code>$ journalctl --disk-usage\nArchived and active journals take up 5.0G in the file system.\n\n$ journalctl --vacuum-size=2G\nVacuuming done, freed 2G of archived journals on disk.\n</code></pre>"},{"location":"tools/lvm/LVM/#estendere-lo-spazio-di-un-disco-esistente","title":"Estendere lo spazio di un disco esistente","text":""},{"location":"tools/lvm/LVM/#procedura-rapida","title":"Procedura rapida","text":"Bash<pre><code>echo 1 &gt; /sys/block/sda/device/rescan; parted /dev/sda unit b print free\nparted /dev/sda unit b resizepart 2 INSERT_LAST_END\nparted /dev/sda unit b resizepart 3 INSERT_LAST_END\n\npvresize /dev/sda5; lvextend -r -l+95%FREE /dev/vg00/root\n</code></pre>"},{"location":"tools/lvm/LVM/#procedura-dettagliata","title":"Procedura dettagliata","text":"<p>Si pu\u00f2 estendere il Logical Volume senza riavviare il sistema espandendo il disco esistente.</p> <ol> <li>Estendere il disco da VMWare</li> <li>Eseguire il comando sulla macchina per il disco sda: <code>echo 1 &gt; /sys/block/sda/device/rescan</code></li> <li>Installare parted</li> <li>Usa parted:</li> </ol> Bash<pre><code>$ parted\n$ unit b\n$ print free\n    Number  Start         End           Size          Type      File system  Flags\n                32256B        1048575B      1016320B                Free Space\n    1      1048576B      511705087B    510656512B    primary   ext2         boot\n                511705088B    512752639B    1047552B                Free Space\n    2      512752640B    42949672959B  42436920320B  extended\n    5      512753664B    32211206143B  31698452480B  logical                lvm\n                32211206144B  42949672959B  10738466816B            Free Space\n</code></pre> <ol> <li>Copy End column of last Free Space line (42949672959B)</li> </ol> Bash<pre><code>#\u00a0Extend extended partition\nresizepart 2 42949672959B\n\n#\u00a0Extend logical partition\nresizepart 5 42949672959B\n\nquit\n\n$ pvresize /dev/sda5\n</code></pre> <ol> <li>Estendere il logical volume</li> </ol> Bash<pre><code>$ lvextend -r /dev/mapper/vg00-root -L+5G\n  Size of logical volume vg00/root changed from 18.91 GiB (4840 extents) to 23.91 GiB (6120 extents).\n  Logical volume root successfully resized\n</code></pre> <p>6a. Estendere il logical volume in percentuale</p> Bash<pre><code>$ lvextend -r /dev/vg00/root -l +95%FREE\n    Size of logical volume vg00/root unchanged from &lt;34.76 GiB (8898 extents).\n  Logical volume vg00/root successfully resized.\n</code></pre> <ol> <li>Nel caso si usi gparted all'avvio \u00e8 necessario cancellare la swap, se presente, allargare il disco. Le VM in ITO non dovrebbero avere SWAP, pertanto non \u00e8 necessario riabilitarla. ~~ricreare la swap, riavviare, cambiare l'IUD del disco swap visibile con bkkid dentro /etc/fstab ed infine lanciare il comando swapon -a~~</li> </ol>"},{"location":"tools/lvm/LVM/#estendere-un-disco-secondario","title":"Estendere un disco secondario","text":"Bash<pre><code>echo 1 &gt; /sys/block/sdb/device/rescan\npvresize /dev/sdb\nlvextend -r -l+95%FREE /dev/vg00/root\n</code></pre>"},{"location":"tools/lvm/LVM/#aggiungere-un-disco-a-caldo","title":"Aggiungere un disco a caldo","text":"<p>Come estendere un disco a caldo su VMWare aggiungendo un disco. Di solito preferiamo aggiungere spazio ad un disco esistente, vedi: Estendere lo spazio di un disco esistente</p>"},{"location":"tools/lvm/LVM/#aggiungere-un-disco-su-vmware","title":"Aggiungere un disco su VMWare","text":"<p>Per estendere il Logical Volume senza riavviare il sistema \u00e8 necessario aggiungere un nuovo disco alla macchina. Estendendo un disco gi\u00e0 esistente l'operazione diventa pi\u00f9 complicata (vedi Estendere lo spazio di un disco esistente)</p>"},{"location":"tools/lvm/LVM/#in-case-of-two-hard-disks","title":"In case of two Hard Disks","text":"Bash<pre><code>echo 1 &gt; /sys/block/sdb/device/rescan\n$ pvresize /dev/sdb\n$ lvextend -r /dev/mapper/Kooomo--vg-root -L+25G\n</code></pre>"},{"location":"tools/lvm/LVM/#aggiornamento-devices-su-linux","title":"Aggiornamento devices su linux","text":"<p>Una volta aggiunto il disco, potrebbe non essere visibile dalla macchina virtuale. (Controllare con <code>fdisk -l</code>) Connettersi alla macchina e lanciare questi 3 comandi per aggiornare i devices:</p> Bash<pre><code>echo \"- - -\" &gt; /sys/class/scsi_host/host0/scan\necho \"- - -\" &gt; /sys/class/scsi_host/host1/scan\necho \"- - -\" &gt; /sys/class/scsi_host/host2/scan\n</code></pre> <p>Ora il devices dovrebbe visualizzarsi correttamente, es. <code>Disk /dev/sdb: 16 GiB, 17179869184 bytes, 33554432 sectors</code></p>"},{"location":"tools/lvm/LVM/#lvm_1","title":"LVM","text":"<p>Una macchina pu\u00f2 avere diversi Volume Group. Con i comandi <code>vgs</code> e <code>lvs</code> si possono vedere rispettivamente i VG presenti e la lista dei Logical Volume</p> Bash<pre><code>$ vgs\n  VG   #PV #LV #SN Attr   VSize  VFree\n  vg00   1   2   0 wz--n- 19.76g    0\n$ lvs\n  LV     VG   Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n  root   vg00 -wi-ao----  18.91g\n  swap_1 vg00 -wi-ao---- 872.00m\n</code></pre> <p>In questo caso abbiamo solo un VG</p>"},{"location":"tools/lvm/LVM/#estendere-un-vg","title":"Estendere un VG","text":"<p>Adesso vogliamo aggiungere il disco creato PV (Physical Volume) al VG <code>vg00</code> Per farlo basta dare questo comando</p> Bash<pre><code>$ vgextend vg00 /dev/sdb\n  Physical volume \"/dev/sdb\" successfully created\n  Volume group \"vg00\" successfully extended\n</code></pre> <p>Ora il VG ha dello spazio libero da allocare ad un LV</p> Bash<pre><code>$ vgs\n  VG   #PV #LV #SN Attr   VSize  VFree\n  vg00   2   2   0 wz--n- 35.75g 16.00g\n</code></pre>"},{"location":"tools/lvm/LVM/#estendere-un-lv","title":"Estendere un LV","text":"<p>Vogliamo quindi aggiungere 5G dei 16G disponibili al logical volume <code>root</code></p> Bash<pre><code>$ lvextend -r /dev/mapper/vg00-root -L+5G\n  Size of logical volume vg00/root changed from 18.91 GiB (4840 extents) to 23.91 GiB (6120 extents).\n  Logical volume root successfully resized\n</code></pre> <p>Controlliamo quindi la situazione e notiamo che</p> <ul> <li>sul VG ci sono 5G di spazio libero in meno</li> <li>l'LV root ha 5G di spazio allocato in pi\u00f9</li> </ul> Bash<pre><code>$ vgs\n  VG   #PV #LV #SN Attr   VSize  VFree\n  vg00   2   2   0 wz--n- 35.75g 11.00g\n$ lvs\n  LV     VG   Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n  root   vg00 -wi-ao----  23.91g\n  swap_1 vg00 -wi-ao---- 872.00m\n</code></pre> <p>NB Se si vuole dare tutto lo spazio aggiunto ad un volume:</p> Text Only<pre><code>lvextend -r -l +100%FREE /dev/mapper/vg00-root\n</code></pre> <p>Operazione completata</p>"},{"location":"tools/lvm/LVM/#estendere-filesystem","title":"Estendere FileSystem","text":"<p>SOLO nel caso in cui non si e' utilizzata l'opzione -r per il comando lvextend sara' necessario effettuare il resize anche del file system:</p> <p>Per tutti i file system tranne XFS:</p> Bash<pre><code>resize2fs /dev/mapper/vg00-root\n</code></pre> <p>in caso il filesystem sia di tipo xfs (distro centos e redhat)</p> Bash<pre><code>xfs_growfs /dev/mapper/vg0-root\n</code></pre>"},{"location":"tools/lvm/LVM/#estensione-usando-tutto-lo-spazio-libero","title":"Estensione usando tutto lo spazio libero","text":"Bash<pre><code>    $ vgs\n        VG   #PV #LV #SN Attr   VSize  VFree\n        vg00   2   2   0 wz--n- 35.75g 11.00g\n\n    $ lvextend -r /dev/mapper/vg00-root -l+100%FREE\n        Size of logical volume vg00/root changed from 18.91 GiB (4840 extents) to 23.91 GiB (6120 extents).\n        Logical volume root successfully resized\n\n    $ vgs\n        VG   #PV #LV #SN Attr   VSize  VFree\n        vg00   2   2   0 wz--n- 35.75g    0\n</code></pre>"},{"location":"tools/paloalto/PALOALTO/","title":"Paloalto","text":"<p>Cercare tutte le regole disabilitate: <code>(disabled eq yes)</code></p>"},{"location":"tools/paloalto/PALOALTO/#upgrade-procedure","title":"Upgrade procedure","text":"<ul> <li> <p>Controllare che i firewall abbiano lo stesso content release version &gt; Device &gt; Dynamic Updates</p> </li> <li> <p>If you want to test that HA is functioning properly before the upgrade, consider upgrading the active peer in an active/passive configuration first to ensure that failover occurs without incident.</p> </li> </ul> <p>Riavviare il master   Riavviare il nuovo master</p> <ul> <li>Cancellare l'OS 8.0.1 da entrambi i firewall</li> <li> <p>Scaricare OS 8.0.17 su entrambi i firewall</p> </li> <li> <p>Export named configuration for each firewall: Device &gt; Setup &gt; Operations and click Export named configuration snapshot: running-config.xml</p> </li> <li> <p>Disable preemption on the first peer in each pair. You only need to disable this setting on one firewall in the HA pair but ensure that the commit is successful before you proceed with the upgrade.</p> </li> </ul> <p>Select Device &gt; High Availability and edit the Election Settings.   If enabled, disable (clear) the Preemptive setting and click OK.</p> <ul> <li>Installare OS 8.0.17 sul passive peer</li> <li>Suspend the active peer (fail over): Device &gt; High Availability &gt; Operational Commands and click Suspend local device.   View the High Availability widget on the Dashboard and verify that the state changes to Passive.</li> <li>Update suspended peer</li> <li>From the CLI of the peer you just upgraded, run the following command to make the firewall functional again:</li> </ul> <p>request high-availability state functional</p> <p>Run the following CLI commands to confirm that the upgrade succeeded:</p> Text Only<pre><code>show session all\nshow high-availability interface ha2\n</code></pre> <ul> <li>Scaricare OS 8.1.0 su entrambi i firewall</li> <li>Installare OS 8.1.0 sul passive peer</li> <li>Suspend the active peer (fail over): Device &gt; High Availability &gt; Operational Commands and click Suspend local device.   View the High Availability widget on the Dashboard and verify that the state changes to Passive.</li> <li>Update suspended peer</li> <li>From the CLI of the peer you just upgraded, run the following command to make the firewall functional again:</li> </ul> <p>request high-availability state functional</p> <p>Run the following CLI commands to confirm that the upgrade succeeded:</p> Text Only<pre><code>show session all\nshow high-availability interface ha2\n</code></pre> <ul> <li>Scaricare OS 8.1.7 su entrambi i firewall</li> <li>Installare OS 8.1.7 sul passive peer e confermare il riavvio</li> <li>Suspend the active peer (fail over): Device &gt; High Availability &gt; Operational Commands and click Suspend local device.   View the High Availability widget on the Dashboard and verify that the state changes to Passive.</li> <li>Update suspended peer</li> <li>From the CLI of the peer you just upgraded, run the following command to make the firewall functional again:</li> </ul> <p>request high-availability state functional</p> <p>Run the following CLI commands to confirm that the upgrade succeeded:</p> Text Only<pre><code>show session all\nshow high-availability interface ha2\n</code></pre>"},{"location":"tools/paloalto/PALOALTO/#globalprotect-disable-unsafe-ciphhers","title":"Globalprotect disable unsafe ciphhers","text":"Text Only<pre><code>PA-820(active)# set shared ssl-tls-service-profile \"SSL VPN Profile\" protocol-settings min-version tls1-2\nPA-820(active)# set shared ssl-tls-service-profile \"SSL VPN Profile\" protocol-settings enc-algo-aes-128-cbc no\nPA-820(active)# set shared ssl-tls-service-profile \"SSL VPN Profile\" protocol-settings enc-algo-aes-128-gcm no\n</code></pre>"},{"location":"tools/postgres/POSTGRES/","title":"Postgres","text":"Bash<pre><code># List databases\npostgres=# \\l\n\n# Connect to DB\n\\c db_name\n\n# List tables\n\\dt\n\n# Dump database\npg_dump -U postgres db_name &gt; db_name.pgsql\n\n# Dump table\npg_dump -U postgres db_name -t table_name &gt; table_name.pgsql\n\n# Restore\npsql -U postgres db_name &lt; db_name.pgsql\n</code></pre>"},{"location":"tools/postgres/POSTGRES/#remote-database","title":"Remote database","text":"Bash<pre><code># Dump\npg_dump -h 127.0.0.1 -p 5432 -U postgres database_to_backup &gt; /root/file_where_to_write_dump.sql\n\n# Restore\nPGPASSWORD=\"$POSTGRES_PASSWORD\" psql --host 127.0.0.1 -U postgres -d database_to_restore -p 5432 &lt; /root/file_to_restore.sql\n</code></pre>"},{"location":"tools/redis/REDIS/","title":"Redis Best Practices","text":"<p>Questa \u00e8 la procedura consolidata di installazione di Redis:</p> <ul> <li>Imposta il max memory delle chiavi in redis</li> <li>Imposta la memoria massima del container docker</li> <li>Installa il prometheus exporter</li> <li>Rende redis disponibile solo in localhost e sulla rete del cliente</li> <li>Imposta l'expire della chiavi</li> </ul> <p>L'expire delle chiavi allkeys-lru va impostato solo se redis \u00e8 usato per la cache.</p>"},{"location":"tools/redis/REDIS/#ansible","title":"Ansible","text":"<p>Installare i ruoli:</p> <ol> <li>cc.docker: installa docker e docker-compose</li> <li>cc.thp: installa redis-tools e imposta il transparent huge page sui valori consigliati da redis</li> </ol>"},{"location":"tools/redis/REDIS/#configurazione-ansible","title":"Configurazione ansible","text":"YAML<pre><code>redis_custom_image: bitnami/redis:5.0.9 #\u00a0Check latest here: https://hub.docker.com/r/bitnami/redis/tags\nredis_custom_exporter_image: oliver006/redis_exporter:v1.5.3 #\u00a0Check latest here: https://hub.docker.com/r/oliver006/redis_exporter/tags\n\nredis_custom_password: SUPER_SECURE_PASSWORD # Set a secure password\nredis_custom_memory_size: 1gb #\u00a0set the max memory for redis keys\nredis_custom_container_memory_size: 2000000000 #\u00a0set the max memory for the container es. 2GB\n\nredis_custom_memory_policy: allkeys-lru #\u00a0Allow all keys deletion\n\ncc_docker_compose_volumes:\n  - path: /opt/redis\n    owner: 1001\n\ncc_docker_compose: |\n\n  version: '2.1'\n\n  services:\n    redis:\n      container_name: redis\n      image: {{\u00a0redis_custom_image }}\n      restart: unless-stopped\n      volumes:\n        - '/opt/redis:/bitnami'\n      ports:\n        - {{ ansible_eth1.ipv4.address }}:6379:6379\n        - 127.0.0.1:6379:6379\n      environment:\n        - REDIS_PASSWORD={{ redis_custom_password }}\n      command: /run.sh --maxmemory {{ redis_custom_memory_size }} --maxmemory-policy {{ redis_custom_memory_policy }}\n      mem_limit: {{\u00a0redis_custom_container_memory_size }}\n      sysctls:\n        net.core.somaxconn: '511'\n\n    redis-exporter:\n      container_name: redis-exporter\n      image: {{\u00a0redis_custom_exporter_image\u00a0}}\n      restart: unless-stopped\n      environment:\n        - REDIS_ADDR=redis:6379\n        - REDIS_PASSWORD={{ redis_custom_password }}\n      ports:\n        - {{ ansible_eth1.ipv4.address }}:9121:9121\n        - 127.0.0.1:9121:9121\n      depends_on:\n        - redis\n</code></pre>"},{"location":"tools/smtp/SMTP/","title":"SMTP Relay","text":"<p>Use this smtp relay for cron or scripts for vm on prem</p> <p>mx domain=mtaccloud.critcalcase.com ip=176.221.54.75 user=root auth by sshkey</p> <p>Useful Commands (from /root):</p> Bash<pre><code>#users list\nsh -x list-smtp-users.sh\n\n#new user\nsh -x add-smpt-user.sh mycronscript@mtaccloud.critcalcase.com mystrongpass\n#delete user\nsh -x del-smtp-user.sh mycronscript@mtaccloud.critcalcase.com\n\n#password change\nsh -x del-smtp-user.sh mycronscript@mtaccloud.critcalcase.com\nsh -x add-smpt-user.sh mycronscript@mtaccloud.critcalcase.com myNEWstrongpass\n</code></pre> <p>Note: edit /root/firewall.sh if you need to change iptables rules or add ip to smtp or ssh.</p>"},{"location":"tools/smtp/SMTP/#exim-smart-host-configuration","title":"Exim smart host configuration","text":"<ol> <li>Install and configure exim4 as smarthost configuration on the your on prem server, insert mtaccloud.criticalcase.com (176.221.54.75) as smarthost</li> </ol> Bash<pre><code>dpkg-reconfigure exim4-config\n</code></pre> <ol> <li>Create in smtp relay server (176.221.54.75) your smtp user and passord</li> </ol> Bash<pre><code>sh -x add-smpt-user.sh mycronscript@mtaccloud.criticalcase.com mystrongpass\n</code></pre> <ol> <li>In the /etc/exim4/passwd.client insert your smtp auth parameters:</li> </ol> Bash<pre><code>cat /etc/exim4/passwd.client\n*:mycronscript@mtaccloud.criticalcase.com:mystrongpassword\n</code></pre> <ol> <li>Create a /etc/exim4/exim4.conf.localmacros file</li> </ol> Bash<pre><code>cat /etc/exim4/exim4.conf.localmacros\nAUTH_CLIENT_ALLOW_NOTLS_PASSWORDS = 1\n</code></pre> <ol> <li>restart exim4 service and try to send an email:</li> </ol> Bash<pre><code>systemctl restart exim4\necho \"test body\"|mailx -s \"test subject\" a.barbaglia@criticalcase.com &amp;&amp; tail -f /var/log/exim4/mainlog\n</code></pre>"},{"location":"tools/ssh/OPENSSL/","title":"CSR","text":"<p>Come generare un CSR</p>"},{"location":"tools/ssh/OPENSSL/#single-domain","title":"Single Domain","text":"Text Only<pre><code>export MY_CN=piperitas.com # domain without www\nopenssl req -new -newkey rsa:2048 -nodes -keyout ${MY_CN}.key -out ${MY_CN}.csr -subj \"/C=IT/ST=Italy/L=Turin/O=CriticalCase/OU=IT Department/CN=${MY_CN}\"\n</code></pre>"},{"location":"tools/ssh/OPENSSL/#wildcard","title":"Wildcard","text":"Text Only<pre><code>export MY_CN=lesacoutlet.it # domain without www\nopenssl req -new -newkey rsa:2048 -nodes -keyout wild.${MY_CN}.key -out wild.${MY_CN}.csr -subj \"/C=IT/ST=Italy/L=Turin/O=CriticalCase/OU=IT Department/CN=*.${MY_CN}\"\n</code></pre>"},{"location":"tools/ssh/OPENSSL/#multidomain","title":"Multidomain","text":"<p>Creare un di configurazione, es.: <code>basefarma.it.cnf</code></p> Text Only<pre><code>[ req ]\ndefault_bits = 4096\nprompt = no\nencrypt_key = no\ndefault_md = sha256\ndistinguished_name = dn\nreq_extensions = req_ext\n[ dn ]\nCN = www.basefarma.it\nO = CritiCalcase Srl\nOU = IT\nL = Turin\nST = Piedmont\nC = IT\n[ req_ext ]\nsubjectAltName = @alt_names\n[alt_names]\nDNS.1   = www.uninetfarma.it\n</code></pre> <p>Generare il CSR</p> Text Only<pre><code>openssl req -new -config basefarma.it.cnf -keyout basefarma.it.key -out basefarma.it.csr\n</code></pre>"},{"location":"tools/ssh/SSH/","title":"SSH","text":""},{"location":"tools/ssh/SSH/#remove-bash-history","title":"Remove bash history","text":"<p>root</p> Bash<pre><code>rm -f /root/.bash_history &amp;&amp; unset HISTFILE &amp;&amp; exit\n</code></pre> <p>current user</p> Bash<pre><code>cat /dev/null &gt; ~/.bash_history &amp;&amp; history -c &amp;&amp; exit\n</code></pre>"},{"location":"tools/ssh/SSH/#fixing-timeout-issue-when-executing-ansible","title":"Fixing timeout issue when executing Ansible","text":"<p>If you faced with error below you should add the parameter below in this path: .ssh/config</p> Text Only<pre><code> fatal: [prodredpr05.kooomo.local]: UNREACHABLE! =&gt; {\"changed\": false, \"msg\": \"Connection timed out.\", \"unreachable\": true}\n</code></pre> <p>add the parameter below to this path : .ssh/config</p> Bash<pre><code>Host *\n#    User a.kolahdouzan\n    GSSAPIAuthentication no\n</code></pre>"},{"location":"tools/ssh/SSH/#what-is-my-ip","title":"What is my IP?","text":"<p>Good service not rate-limited:</p> Bash<pre><code>curl https://ifconfig.io/ip\n</code></pre>"},{"location":"tools/ssh/SSMTP/","title":"SSMTP: test smtp server","text":"<p>Install SSMTP</p> Bash<pre><code>sudo apt-get install ssmtp\n</code></pre> <p>Edit <code>/etc/ssmtp/ssmtp.conf</code></p> Bash<pre><code>mailhub=smtp.gmail.com:587\nuseSTARTTLS=YES\nAuthUser=username-here\nAuthPass=password-here\nTLS_CA_File=/etc/pki/tls/certs/ca-bundle.crt\n</code></pre> <p>Edit <code>/etc/ssmtp/revaliases</code></p> Bash<pre><code>root:someone@yourdomain.tld\n</code></pre> <p>Send test mail</p> Bash<pre><code>echo \"Test message from Linux server using ssmtp\" | sudo ssmtp -vvv revceivers-email@gmail.com\n</code></pre>"},{"location":"tools/ssh/gh-actions/","title":"Github Actions","text":""},{"location":"tools/ssh/gh-actions/#runner-on-premise","title":"Runner on-premise","text":"Bash<pre><code>apt -y update\napt -y install mkisofs unzip perl\n\nadduser github --gecos \"\" --disabled-password\nsu - github\n\nmkdir actions-runner &amp;&amp; cd actions-runner\ncurl -o actions-runner-linux-x64-2.301.1.tar.gz -L https://github.com/actions/runner/releases/download/v2.301.1/actions-runner-linux-x64-2.301.1.tar.gz\necho \"3ee9c3b83de642f919912e0594ee2601835518827da785d034c1163f8efdf907  actions-runner-linux-x64-2.301.1.tar.gz\" | shasum -a 256 -c\ntar xzf ./actions-runner-linux-x64-2.301.1.tar.gz\n\n./config.sh --unattended --url https://github.com/criticalcase/packer-template --token AAH7ZMB73UTZAXNHI4DWBJLD6U3UO --labels vmware-packer,vmware-packer-rz2,rz2\n\nexit\n\ncd /home/github/actions-runner/\n\n./svc.sh install github\n./svc.sh start\n</code></pre>"},{"location":"tools/ssh/ncdu/","title":"NCDU","text":"<p>Come trovare file/cartelle pesanti nel filesystem.</p> Bash<pre><code># Ubuntu\nsudo apt install -y ncdu\n\n# Centos\nsudo yum install ncdu\n</code></pre> <p>Es.</p> Bash<pre><code>ncdu --exclude /tmp /\n</code></pre>"},{"location":"tools/steampipe/query/","title":"Queries","text":"Bash<pre><code># Ricerca per payer\nselect account_id, account_aliases, organization_master_account_id, organization_master_account_email from aws_cc.aws_account\n\n# Ricerca MFA\nselect * from aws_cc.aws_iam_virtual_mfa_device\n\n# EKS clusters\nselect name, arn, endpoint, identity, status from b2c.aws_eks_cluster;\n\n# Ricerca di istanze attive con launch_time\nselect _ctx, account_id,instance_id, instance_type, launch_time,instance_lifecycle from aws_dt.aws_ec2_instance where instance_lifecycle!='spot' order by launch_time;\n</code></pre>"},{"location":"tools/sync/RCLONE/","title":"rclone","text":"<ul> <li>https://rclone.org/</li> </ul>"},{"location":"tools/sync/RCLONE/#config","title":"Config","text":"Text Only<pre><code>[aws]\ntype = s3\nprovider = AWS\nenv_auth = false\naccess_key_id = XXXXX\nsecret_access_key = XXXXX\nregion = eu-central-1\nacl = public-read\nstorage_class = STANDARD\n\n\n[azure]\ntype = azureblob\naccount = STORAGE_ACCOUNT_NAME\nkey = XXXXX\n</code></pre>"},{"location":"tools/sync/RCLONE/#list-bucket","title":"List bucket","text":"Bash<pre><code>rclone lsd aws:\nrclone lsd azure:\n</code></pre>"},{"location":"tools/sync/RCLONE/#copy","title":"Copy","text":"<p>Fix mime type issue from source:</p> Bash<pre><code># Copy azure to local VM\nrclone copy --transfers=32 -P azure:storage-foto azure/storage-foto/\nrclone copy --transfers=32 -P azure:storage-foto-dev azure/storage-foto-dev/\n\n# Copy local VM to AWS\nrclone copy --transfers=32 -P azure/ aws:brandsdistribution-images\n</code></pre>"},{"location":"tools/sync/RSYNC/","title":"rsync","text":""},{"location":"tools/sync/RSYNC/#server-to-server","title":"Server to Server","text":"Bash<pre><code>rsync -e ssh -avz --numeric-ids --delete --progress --exclude-from=/root/hotclone_exclude / root@TARGET_IP_ADDRESS:/\n\n    /boot\n    /lib/modules\n    /etc/modules\n    /etc/inittab\n    /etc/lilo.conf\n    /etc/fstab\n    /etc/mtab\n    /proc\n    /dev\n    lost+found/\n    /var/log\n    /etc/network\n    /etc/udev/rules.d\n    /tmp\n    /sys\n    /etc/resolv.conf\n\ntime rsync -av --numeric-ids --delete --progress --exclude /proc --exclude /sys --exclude /boot --exclude /dev  --exclude /etc/mtab --exclude /etc/fstab --exclude /etc/udev/rules.d --exclude /lib/modules --exclude /etc/inittab --exclude /etc/network --exclude /etc/resolv.conf root@TARGET_IP_ADDRESS.it:/ /\n</code></pre>"},{"location":"tools/templates/","title":"Company name @ C0000","text":"<p>Per un cliente con il progetto sotto servizio ITO H24 (le sezioni <code>Progetti senza ITO</code> e <code>Note H24</code>sono opzionali):</p>"},{"location":"tools/templates/#ito-h24","title":"ITO H24","text":"<ul> <li>CR200804: Description</li> </ul>"},{"location":"tools/templates/#ito-lite","title":"ITO LITE","text":"<ul> <li>CR200805: Description</li> </ul>"},{"location":"tools/templates/#progetti-senza-ito","title":"Progetti senza ITO","text":"<ul> <li>CR200806: Description</li> </ul>"},{"location":"tools/templates/#note-h24","title":"Note H24","text":"<ul> <li>Il cliente vuole sempre essere avvisato telefonicamente a prescindere dagli orari.</li> </ul> <p>\\ \\ \\ Per un cliente con il progetto sotto servizio ITO Lite (le sezioni <code>Progetti senza ITO</code> e <code>Note H24</code>sono opzionali):</p>"},{"location":"tools/templates/#ito-lite_1","title":"ITO LITE","text":"<ul> <li>CR200804: Description</li> </ul>"},{"location":"tools/templates/#progetti-senza-ito_1","title":"Progetti senza ITO","text":"<ul> <li>CR200805: Description</li> </ul>"},{"location":"tools/templates/#note-h24_1","title":"Note H24","text":"<ul> <li>Il cliente vuole sempre essere avvisato telefonicamente a prescindere dagli orari.   \\   \\   \\   Per un cliente senza servizio ITO (le sezioni <code>Progetti senza ITO</code> e <code>Note H24</code>sono opzionali):</li> </ul>"},{"location":"tools/templates/#progetti-senza-ito_2","title":"Progetti senza ITO","text":"<ul> <li>CR200804: Description</li> </ul>"},{"location":"tools/templates/#note-h24_2","title":"Note H24","text":"<ul> <li>Il cliente vuole sempre essere avvisato telefonicamente a prescindere dagli orari.</li> </ul>"},{"location":"tools/templates/#company-name","title":"Company Name","text":"<p>Internal data:</p> <ul> <li>Start date: 00/00/2021</li> <li>Sales: Vittorino/Zoncu/Silvano</li> <li>Support level: [ ] Silver [ ] Gold</li> <li>Phone: +39 011 11111111</li> <li>Email: n.cognome@criticalcase.com</li> </ul> <p>Customer's SPOC:</p> <ul> <li>Full name: Name Surname</li> <li>Email: hello@customer.com</li> <li>Phone: +39 XXX XXXXXXX</li> <li>Severity 1 notification: [ ] Email, [ ] Telefono</li> <li>Severity 2 notification: [ ] Email, [ ] Telefono</li> </ul>"},{"location":"tools/templates/#data-center","title":"Data Center","text":"<ul> <li> AWS</li> <li> Azure</li> <li> GCP</li> <li> Tencent</li> <li> Critical Case RZ1</li> <li> Critical Case RZ2</li> </ul>"},{"location":"tools/templates/#descrizione-ambito-funzionale-e-tecnologico","title":"Descrizione ambito funzionale e tecnologico","text":"<p>Fornire una sintetica descrizione dell\u2019ambito funzionale nel quale opera il sistema IT del Cliente. Inserire qui eventuali descrizioni dell\u2019architettura infrastrutturale, integrazioni, ecc...</p>"},{"location":"tools/templates/#elenco-server-e-servizi","title":"Elenco Server e servizi","text":""},{"location":"tools/templates/#nome-macchina","title":"Nome macchina","text":"<p>IP: Funzione: Credenziali accesso: Sistema Operativo:</p> <p>Tipologia:</p> <ul> <li> Servizio</li> <li> VM</li> <li> Fisica</li> <li> Parte di un cluster</li> </ul> <p>Elenco Midlleware Installati:</p> <ul> <li>Apache ver.</li> <li>PHP Ver.</li> <li>JBoss ver.</li> <li>Oracle</li> </ul> <p>Applicazioni installate + Referente:</p> <ul> <li>Gestione Fondi: Paolo Rossi p.rossi@mail.com +39 33946025413</li> </ul> <p>Ref.resp gestione Middleware:</p> <ul> <li>Apache: indicare il referente o i contatti dell\u2019azienda che segue la piattaforma, o eventualmente se \u00e8 gestito da CriticalCase o subfornitori come 2\u00b0 o 3\u00b0 livello</li> </ul> <p>Note:</p> <ul> <li>Inserire qui eventuali note tecniche o eventuali eccezioni sulla modalit\u00e0 di gestione</li> </ul>"},{"location":"tools/templates/welcome/","title":"Welcome e-mail","text":""},{"location":"tools/templates/welcome/#istruzioni","title":"Istruzioni","text":"<p>I campi <code>[CUSTOMER_NAME]</code>, <code>[COMPANY_NAME]</code>, <code>[CXXXX]</code>, <code>[EMAIL]</code>, <code>[INSERIRE_RIEPILOGO_SERVIZI_ITO]</code>, <code>[CRXXXXXXX]</code> e <code>[TEMPORARY_PASSWORD]</code> vanno messi in grassetto.</p> <p>La parte sotto * * * va messa in grigio con font pi\u00f9 piccolo.</p> <p>Mettere in CC il commerciale di riferimento</p>"},{"location":"tools/templates/welcome/#attivazione-servizio-ito-it-operations","title":"Attivazione servizio ITO (IT Operations)","text":"<p>Buongiorno <code>[CUSTOMER_NAME]</code>,</p> <p>stiamo completando l'attivazione del servizio ITO (IT Operations) di Criticalcase per la societ\u00e0 <code>[COMPANY_NAME]</code> con ID <code>[CXXXX]</code>.</p> <p>Il contratto di riferimento \u00e8 <code>[CRXXXXXXX]</code>, sar\u00e0 richiesto per le segnalazioni via telefono.</p> <p>Il servizio ITO sar\u00e0 attivo solo per i servizi di questo contratto:</p> <ul> <li><code>[INSERIRE_RIEPILOGO_SERVIZI_ITO]</code></li> </ul> <p>Non cambiano le modalit\u00e0 di supporto di eventuali altri contratti con Criticalcase.</p> <p>Con la presente Vi chiediamo di comunicarci l'indirizzo e-mail, che sar\u00e0 da voi utilizzato come unica utenza per l'accesso al sistema di Ticketing del servizio ITO, e per tutte le relative notifiche e segnalazioni.</p> <p>In attesa di un vostro cortese riscontro.</p> <p>Grazie e Cordiali Saluti.</p> <p>Note:</p> <p>Il sistema di Ticketing sar\u00e0 accessibile al seguente link: https://ito.criticalcase.cloud con le credenziali che Vi saranno inviate in fase di attivazione.</p> <p>Di seguito un riepilogo delle priorit\u00e0 del servizio ITO con le regole di ingaggio.</p> <p>Severity 1 (Alta)</p> <p>Per gli ambienti di produzione l'assistenza \u00e8 H24 7x7</p> <p>Assegnato al blocco degli errori / guasti che si verificano durante un'attivit\u00e0 operativa aziendale obbligatoria e di produzione. Sono errori, o pi\u00f9 in generale problemi, che inibiscono completamente un'attivit\u00e0 core business, che non pu\u00f2 essere ritardata, e che coinvolge uno o pi\u00f9 Utenti finali, determinando cos\u00ec un grave impatto sul business dell'azienda. Non \u00e8 possibile applicare soluzioni alternative che consentano al cliente di procedere con le operazioni aziendali interessate non possibili o presenti.</p> <p>Modalit\u00e0 di apertura segnalazione: via telefono chiamando il numero +39 011 0888761</p> <p>Severity 2 (Normale)</p> <p>Assegnati a problemi che rallentano o compromettono il funzionamento di uno o pi\u00f9 Utenti Finali, e se non vengono affrontati adeguatamente nel tempo, potrebbero causare un grave impatto sull'erogazione dei Servizi. Gli utenti finali possono comunque eseguire operazioni, anche se in modalit\u00e0 deteriorata, o posticipare le operazioni con una pianificazione diversa.</p> <p>Modalit\u00e0 di apertura segnalazione: via ticket tramite l'interfaccia https://ito.criticalcase.cloud</p> <p>Severity 3 (Bassa)</p> <p>Viene assegnato a problematiche che incidono sull'attivit\u00e0 operativa di uno o pi\u00f9 Utenti Finali, senza per\u00f2 provocare impatti sul business, ad esempio funzioni di reporting locale o reporting aziendale che non prevede una scadenza obbligatoria.</p> <p>Modalit\u00e0 di apertura segnalazione: via ticket tramite l'interfaccia https://ito.criticalcase.cloud</p>"},{"location":"tools/templates/welcome/#nuova-piattaforma-di-supporto-ito-it-operations","title":"Nuova piattaforma di supporto ITO (IT Operations)","text":"<p>Buongiorno <code>[CUSTOMER_NAME]</code>,</p> <p>stiamo spostando la piattaforma del supporto JIRA per la societ\u00e0 <code>[COMPANY_NAME]</code> con ID <code>[CXXXX]</code>.</p> <p>Il contratto di riferimento \u00e8 <code>[CRXXXXXXX]</code>, sar\u00e0 richiesto per le segnalazioni via telefono.</p> <p>Attualmente usate https://support.criticalcase.com/servicedesk che stiamo dismettendo. Mentre la nuova piattaforma \u00e8 https://ito.criticalcase.cloud.</p> <p>Cambia anche il numero di telefono per le segnalazioni bloccanti e urgenti con priorit\u00e0 alta: +39 011 0888761</p> <p>Con la presente Vi chiediamo di comunicarci l'indirizzo e-mail, che sar\u00e0 da voi utilizzato come unica utenza per l'accesso al sistema di Ticketing del servizio ITO, e per tutte le relative notifiche e segnalazioni.</p> <p>In attesa di un vostro cortese riscontro.</p> <p>Grazie e Cordiali Saluti. Il sistema di Ticketing sar\u00e0 accessibile al seguente link: https://ito.criticalcase.cloud con le credenziali che Vi saranno inviate in fase di attivazione.</p> <p>Di seguito un riepilogo delle priorit\u00e0 del servizio ITO con le regole di ingaggio.</p> <p>Severity 1 (Alta)</p> <p>Per gli ambienti di produzione l'assistenza \u00e8 H24 7x7</p> <p>Assegnato al blocco degli errori / guasti che si verificano durante un'attivit\u00e0 operativa aziendale obbligatoria e di produzione. Sono errori, o pi\u00f9 in generale problemi, che inibiscono completamente un'attivit\u00e0 core business, che non pu\u00f2 essere ritardata, e che coinvolge uno o pi\u00f9 Utenti finali, determinando cos\u00ec un grave impatto sul business dell'azienda. Non \u00e8 possibile applicare soluzioni alternative che consentano al cliente di procedere con le operazioni aziendali interessate non possibili o presenti.</p> <p>Modalit\u00e0 di apertura segnalazione: via telefono chiamando il numero +39 011 0888761</p> <p>Severity 2 (Normale)</p> <p>Assegnati a problemi che rallentano o compromettono il funzionamento di uno o pi\u00f9 Utenti Finali, e se non vengono affrontati adeguatamente nel tempo, potrebbero causare un grave impatto sull'erogazione dei Servizi. Gli utenti finali possono comunque eseguire operazioni, anche se in modalit\u00e0 deteriorata, o posticipare le operazioni con una pianificazione diversa.</p> <p>Modalit\u00e0 di apertura segnalazione: via ticket tramite l'interfaccia https://ito.criticalcase.cloud</p> <p>Severity 3 (Bassa)</p> <p>Viene assegnato a problematiche che incidono sull'attivit\u00e0 operativa di uno o pi\u00f9 Utenti Finali, senza per\u00f2 provocare impatti sul business, ad esempio funzioni di reporting locale o reporting aziendale che non prevede una scadenza obbligatoria.</p> <p>Modalit\u00e0 di apertura segnalazione: via ticket tramite l'interfaccia https://ito.criticalcase.cloud</p>"},{"location":"tools/templates/welcome/#jira-mail-attesa-validazione-account","title":"JIRA Mail attesa validazione account","text":"<p>Buongiorno [CUSTOMER NAME],</p> <p>abbiamo inviato la mail di attivazione del portale JIRA all'indirizzo [EMAIL].</p> <p>\u00c8 necessario seguire il link presente nella e-mail per completare la procedura di registrazione. Verr\u00e0 richiesto di impostare una password.</p> <p>In attesa di un vostro cortese riscontro.</p> <p>Grazie e Cordiali Saluti.</p>"},{"location":"tools/templates/welcome/#ito-ssm-mail-conferma-attivazione","title":"ITO SSM: Mail conferma attivazione","text":""},{"location":"tools/templates/welcome/#credenziali-accesso-a-itocriticalcasecom","title":"Credenziali accesso a ito.criticalcase.com","text":"<p>Buonasera,</p> <p>\u00e8 stata creata l'utenza <code>[EMAIL]</code> con la seguente password temporanea <code>[TEMPORARY_PASSWORD]</code> per poter accedere a https://ito.criticalcase.com/</p> <p>Si tratta di una piattaforma in cui \u00e8 possibile controllare quali pacchetti dovrebbero essere aggiornati all'interno dei propri servers con relativa severity</p> <p>Sono presenti anche altre informazioni, quali IP, tipo di sistema operativo e nome della macchina.</p> <p>Una volta effettuato l'accesso, potreste darci una opinione sull'esperienza d'uso?</p> <p>Grazie e cordiali saluti</p>"},{"location":"tools/templates/welcome/#credentials-for-accessing-itocriticalcasecom","title":"Credentials for accessing ito.criticalcase.com","text":"<p>Hello,</p> <p>the user <code>[EMAIL]</code> was created with the following temporary password <code>[TEMPORARY_PASSWORD]</code> to access https://ito.criticalcase.com/ It is a platform where you can control which packages should be updated within your servers with relative severity There is also other information, such as IP, OS type and machine name.</p> <p>Could you give us an opinion on the user experience?</p> <p>Thank you Best Regards</p>"},{"location":"tools/templates/welcome/#piattaforma-jira-sbagliata","title":"Piattaforma JIRA sbagliata","text":"<p>Buongiorno  <code>[CUSTOMER_NAME]</code>,</p> <p>ti ringraziamo per la segnalazione. Per una corretta gestione, ti chiediamo gentilmente di seguire la procedura ufficiale aprendo un ticket sulla piattaforma ITO al seguente link: https://ito.criticalcase.cloud Bisogna effettuare l'accesso con l'account: <code>[EMAIL]</code></p> <p>Questo ticket sar\u00e0 chiuso.</p> <p>Restiamo a disposizione sulla piattaforma ITO per qualsiasi supporto.</p> <p>Grazie e cordiali saluti</p>"},{"location":"tools/tencent/COS/","title":"COS","text":"<p>title: COS: Object Storage editor: markdown</p>"},{"location":"tools/tencent/COS/#tencent-cos-object-storage","title":"Tencent COS: Object Storage","text":""},{"location":"tools/tencent/COS/#coscmd","title":"coscmd","text":"Bash<pre><code># Configure coscmd\ncoscmd config -a IKIDWx00veDfw1yiDbpwraAtW0JPN7jDVddV -s XXXXXXXXXXXXXXX -b mmexchange-proxy-1304124894 -r ap-shanghai\n\n# List\ncoscmd list -a -r\n\n# Download\ncoscmd download test/setRps.log ~/Downloads/\n\n# Upload\ncoscmd upload ~/Downloads/setRps.log test/setRps-copy.log\n</code></pre>"},{"location":"tools/tencent/ExtenddiskonTencent/","title":"Extend disk on Tencent","text":"<p>You can increase the size of the disks, but you cannot reduce it to ensure data integrity.</p> <p>There are 2 ways to increase the disk capacity.</p> <ol> <li>increase the existing disk</li> <li>add a disk and use lvm</li> </ol> <p>*recommended</p>"},{"location":"tools/tencent/ExtenddiskonTencent/#increase-the-existing-disk","title":"Increase the existing disk","text":"<ul> <li> <p>On Tencent portal go to the Instances page and select the correct region (Shanghai 18):   https://console.intl.cloud.tencent.com/cvm/instance</p> </li> <li> <p>Click on More link -&gt; Resource Adjustment -&gt; Expand Cloud Disks </p> </li> <li> <p>Select system disk   </p> </li> <li> <p>Adjust capacity   </p> </li> <li> <p>Finalize   </p> </li> <li> <p>Now start the instance and automagically the cloud init script will be resize the partition to the new size, but please check manually if this is true!</p> </li> </ul>"},{"location":"tools/terraform/","title":"Terraform / OpenTofu","text":"<p>Terraform simplifies infrastructure management by automating provisioning, scaling, and decommissioning of resources. It promotes consistency, reduces manual errors, and improves collaboration by enabling version-controlled infrastructure.</p> <p>We chose OpenTofu because it is fully open-source with a permissive license, avoiding the restrictions introduced by Terraform\u2019s BSL. Its open governance ensures transparency and prevents vendor lock-in, aligning with our values. Additionally, it remains fully compatible with Terraform, making the transition seamless.</p>"},{"location":"tools/terraform/#run-terraform-code","title":"Run terraform code","text":"<p>Internally, a service called Critical0 is used, which functions similarly to Terraform Cloud but at a significantly lower cost.</p> <p>This service provides the same essential features, such as remote state management, team collaboration, and automation workflows, making it an efficient and cost-effective alternative for managing Terraform operations within the organization. By leveraging Critical0, the team benefits from a robust infrastructure as code (IaC) management solution while optimizing operational expenses.</p> <p> critical0</p>"},{"location":"tools/terraform/#create-a-new-terraform-module","title":"Create a new terraform module","text":"<p>When a new Git repository is required for Terraform modules, a Terraform workspace is utilized to automate several critical tasks. This workspace configures the appropriate team permissions, assigns internal GitHub Actions runners, adds relevant topics, and sets a meaningful repository description.</p> <p> terraform-modules-github-repo</p>"},{"location":"tools/terraform/#use-a-module-in-the-code","title":"Use a module in the code","text":"Terraform<pre><code>module \"vm\" {\n  source = \"git::https://github.com/criticalcase/criticalcase-vmware-module.git?ref=v1.0.0\"\n  name   = \"criticalcase-vm\"\n\n  # ...omitted for brevity\n}\n</code></pre>"},{"location":"tools/terraform/#additionals-tools","title":"Additionals tools","text":"<ul> <li>https://www.terratag.io</li> <li>https://www.infracost.io</li> <li>https://runterrascan.io</li> <li>https://driftctl.com</li> </ul>"},{"location":"tools/terraform/Terraformer/","title":"Terraformer","text":"<p>Terraformer \u00e8 un tool per esportare risorse esistenti in codice terraform con il proprio stato. E' un servizio multicloud</p>"},{"location":"tools/terraform/Terraformer/#installazione","title":"Installazione","text":"<p>Su Windows, oltre ad aver seguito la guida: https://github.com/GoogleCloudPlatform/terraformer</p> <p>Ho dovuto creare questo percorso:</p> PowerShell<pre><code>C:\\.terraform.d\\plugins\\windows_amd64\n</code></pre> <p>e copiare l'eseguibile Terraform</p> <p>nelle variabili di ambiente inserire il path</p> PowerShell<pre><code>C:\\.terraform.d\\plugins\\windows_amd64\n</code></pre>"},{"location":"tools/terraform/Terraformer/#esempio-con-aws-route53","title":"Esempio con AWS Route53","text":"<p>Il modulo Terraformer permette di estrarre tutte le zone presenti in Route53. Nel caso di un cliente con molti records \u00e8 possibile che si voglia separare le zone esportate in differenti stati terraform. Per farlo bisogna prima lanciare un comando terraformer, poi utilizzare uno script script.py (non nostro) per separare le zone. Successivamente \u00e8 possibile creare i files terraform delle singole zone attraverso terraformer:</p> Bash<pre><code>terraformer import plan aws --resources=route53 --profile=${account-name}\npython script.py\nterraformer import plan zona.json\n</code></pre> <p>Il parametro --resources utilitza per specificare da quale servizio vuoi importare le risorse. Per importare risorse da tutti i servizi, usa --resources=\"\". Se vuoi escludere determinati servizi, puoi combinare il parametro con --excludes per escludere le risorse dai servizi che non vuoi importare, ad esempio: --resources=\"\" --excludes=\"iam\".</p> <p>Per Terraform &gt;= 0.13, puoi utilizzare replace-provider per migrare lo stato dalle versioni precedenti.</p> Bash<pre><code>terraform state replace-provider -auto-approve \"registry.terraform.io/-/aws\" \"hashicorp/aws\"\n</code></pre> <p>I files terraform verranno creati automaticamente sotto:</p> Bash<pre><code>.\\generated\\aws\\route53\n</code></pre>"},{"location":"tools/terraform/best-practices/","title":"Terraform best practices","text":""},{"location":"tools/terraform/best-practices/#organize-files","title":"Organize Files","text":"<p>Structure your code logically using separate files:</p> <ul> <li><code>critical0.tf</code>: Reads secrets from AWS SSM Parameter Store for use in the configuration.</li> <li><code>data.tf</code>: Fetches external or existing data (e.g., resources, configurations) to use in the configuration.</li> <li><code>locals.tf</code>: Defines local values (intermediate variables) for simplifying and reusing logic within the configuration.</li> <li><code>main.tf</code>: Defines the primary resources and configuration for the infrastructure.</li> <li><code>outputs.tf</code>: Specifies outputs to expose key information about the deployed infrastructure.</li> <li><code>providers.tf</code>: Configures the providers (e.g., AWS, Azure) required for managing resources.</li> <li><code>variables.tf</code>: Declares input variables to make the configuration dynamic and reusable.</li> </ul> <p>Leverage modules to group related resources and promote code reusability. For shared modules, use dedicated repositories and apply version control.</p>"},{"location":"tools/terraform/best-practices/#use-version-control","title":"Use Version Control","text":"<p>Always specify provider and module versions explicitly in the <code>required_providers</code> and <code>source</code> blocks to avoid unexpected updates and ensure stability across environments.</p>"},{"location":"tools/terraform/best-practices/#terraform-modules","title":"Terraform modules","text":"<p>Use official modules if avaialable. I.e. for <code>aws</code> terraform-aws-modules.</p>"},{"location":"tools/terraform/best-practices/#amazon-s3","title":"Amazon S3","text":""},{"location":"tools/terraform/best-practices/#add-aws-account-id-after-bucket-name","title":"Add AWS Account ID after bucket name","text":"Terraform<pre><code>bucket = \"bucket-name-${data.aws_caller_identity.current.account_id}\"\n</code></pre>"},{"location":"tools/terraform/best-practices/#add-lifecycle-rules","title":"Add lifecycle rules","text":"Terraform<pre><code>  lifecycle_rule = [\n    {\n      id      = \"expire-90d\"\n      enabled = true\n\n      transition = {\n        days          = 0\n        storage_class = \"GLACIER_IR\"\n      }\n      expiration = {\n        days = 90\n      }\n    },\n    {\n      id      = \"expire-delete-marker-5d\"\n      enabled = true\n      abort_incomplete_multipart_upload_days = 5\n      expiration = {\n        expired_object_delete_marker = true\n      }\n    }\n  ]\n</code></pre>"},{"location":"tools/terraform/compare/","title":"Terraform cloud Comparison","text":"<p>Il nostro collega Salvatore vorrebbe staccarsi completamente da Terraform Cloud. Pertanto stiamo valutando alcune alternative.</p> <p>Guida utile per la migrazione dello stato: https://medium.com/@DiggerHQ/migrating-from-terraform-cloud-to-amazon-s3-and-dynamodb-a-guide-b03183923f20</p>"},{"location":"tools/terraform/compare/#digger","title":"Digger","text":"<p>https://digger.dev/</p> <ul> <li>Utilizza DynamoDB per effettuare il lock delle Pull Requests.</li> <li>No software da mantenere, usa i runner di Github Actions</li> <li>Non bisogna condividere credenziali con altri sistemi esterni.</li> <li>Le credenziali di accesso al cloud vengono gestite nei secret di github</li> <li>Non ha una dashboard centralizzata</li> <li>Se usato con terraform cloud bisogna inserire un tf_cloud_token personale. Questo da accesso a tutti gli state.</li> <li>Bisogna aprire una PR ogni volta che voglio applicare il codice. Si pu\u00f2 automatizzare con <code>gh pr create</code>.</li> </ul> <p>Da verificare:</p> <ul> <li>Come vengono importati i moduli esterni? tramite directory oppure tramite token</li> <li>Utilizzare la cache tra una run e l'altra per ottimizzare il tempo di build.</li> </ul>"},{"location":"tools/terraform/compare/#terrakube","title":"Terrakube","text":"<p>https://docs.terrakube.io/</p> <p>Il progetto \u00e8 molto simile a terraform cloud, scritto in java e usa i microservizi.</p> <ul> <li>Gira su kubernetes</li> <li>Ha lo stato su S3</li> <li>Il database viene usato per le utenze/organizzazioni/secret</li> </ul> <p>Con la versione <code>2.14.0</code> e il database MySQL da errore.</p>"},{"location":"tools/terraform/compare/#otf","title":"OTF","text":"<p>https://github.com/leg100/otf</p> <p>Simile a terraform cloud con una grafica spartana.</p> <ul> <li>Gira su kubernetes</li> <li>Quando fai una commit, automaticamente ti fa una plan.</li> <li>Richiede il Github PAT personale per accedere ai repository su git. (da verificare)</li> <li>Lo stato viene salvato sul database postgres</li> </ul>"},{"location":"tools/terraform/compare/#runatlantis","title":"RunAtlantis","text":"<p>https://www.runatlantis.io</p> <p>Automatizza terraform+git tramite l'uso di PR. Funziona molto bene su un solo cliente, ma non gestisce i secret per pi\u00f9 clienti.</p> <ul> <li>Gira su kubernetes</li> </ul> <p>Interessante anche il fork https://github.com/lyft/atlantis</p>"},{"location":"tools/terraform/compare/#hatchet","title":"Hatchet","text":"<p>https://hatchet.run/</p> <p>Progetto molto interessante scritto in go. Pu\u00f2 monitorare il download dei moduli terraform per indicarti quelli pi\u00f9 usati.</p> <p>A luglio 2022 non \u00e8 consigliabile usarlo in produzione per via del database che pu\u00f2 essere solo SQLlite.</p>"},{"location":"tools/terraform/compare/#altro","title":"Altro","text":"<ul> <li>https://www.trackawesomelist.com/shuaibiyy/awesome-terraform/</li> </ul>"},{"location":"tools/varnish/VARNISH/","title":"Varnish","text":"<p>Flush all cache</p> Bash<pre><code>varnishadm \"ban req.url ~ /\"\n\n# OR\ncurl 'https://$VARNISHURL/ban' -H 'Authorization: Basic $BASICTOKEN' --data 'req.url ~ /'\n</code></pre> <p>Flush cache by hostname</p> Bash<pre><code>varnishadm \"ban req.http.host == criticalcase.com\"\n</code></pre>"},{"location":"tools/zabbix/MSSQL/","title":"Microsoft SQL Server template","text":""},{"location":"tools/zabbix/MSSQL/#create-the-service-user-on-the-sql-server","title":"Create the service user on the SQL server","text":"<p>Create a user on each istance.   Then grant permission</p> Bash<pre><code>USE master\nGO\nGRANT VIEW SERVER STATE TO zabbix_monitoring\n\n\nUSE master\nGO\nGRANT VIEW ANY DEFINITION TO zabbix_monitoring\n</code></pre>"},{"location":"tools/zabbix/MSSQL/#install-template-on-the-zabbix-server","title":"Install template on the zabbix server","text":"<ul> <li>Get   https://git.zabbix.com/projects/ZBX/repos/zabbix/browse/templates/db/mssql_odbc</li> <li>Then   https://monitoring.rz2.criticalcase.cloud/conf.import.php?rules_preset=template</li> </ul>"},{"location":"tools/zabbix/MSSQL/#install-microsoft-odbc-driver-on-the-zabbix-server-or-zabbix-proxy","title":"Install Microsoft ODBC driver on the Zabbix server or Zabbix proxy","text":"<ul> <li>If VM is monitoring by proxy you must install and configure Miscrosoft odbc driver only on the proxy</li> </ul> Bash<pre><code>curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\n#Ubuntu 16.04\ncurl https://packages.microsoft.com/config/ubuntu/16.04/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list\n\n#Ubuntu 18.04\ncurl https://packages.microsoft.com/config/ubuntu/18.04/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list\n\n#Ubuntu 20.04\ncurl https://packages.microsoft.com/config/ubuntu/20.04/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list\n\n#Ubuntu 20.10\ncurl https://packages.microsoft.com/config/ubuntu/20.10/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list\n</code></pre> <ul> <li>Install msodbc driver</li> </ul> Bash<pre><code>apt-get update\nACCEPT_EULA=Y apt-get install -y msodbcsql17\n</code></pre> <ul> <li>Optional install tools for testing odbc connection</li> </ul> Bash<pre><code>ACCEPT_EULA=Y apt-get install -y mssql-tools\necho 'export PATH=\"$PATH:/opt/mssql-tools/bin\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <ul> <li>Edit /etc/odbc.ini and set dsn</li> </ul> Bash<pre><code>cat odbc.ini\n#no instances\n[DSN_NAME]\nDriver = ODBC Driver 17 for SQL Server\nServer = tcp:sql_server_ip,1433\n#to ISTANCE_1 instance\n[DSN_NAME-ISTANCE_1]\nDriver = ODBC Driver 17 for SQL Server\nServer = sql_server_ip\\ISTANCE_1\nPort   = 1433\n#to ISTANCE_2 instance\n[DSN_NAME-ISTANCE_2]\nDriver = ODBC Driver 17 for SQL Server\nServer = sql_server_ip\\ISTANCE_2\nPort   = 1433\n</code></pre> <ul> <li>Test connection</li> </ul> Bash<pre><code>sqlcmd -D -S DSN_NAME-ISTANCE_2 -U your_zabbix_user -P you_zabbix_user_password -d master -Q \"SELECT @@servername; SELECT @@version;\"\n\n#Result:\n--------------------------------------------------------------------------------------------------------------------------------\nSERVER_NAME\\ISTANCE_NAME\n\n(1 rows affected)\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nMicrosoft SQL Server 2019 (RTM-GDR) (KB4583458) - 15.0.2080.9 (X64)\n        Nov  6 2020 16:50:01\n        Copyright (C) 2019 Microsoft Corporation\n        Standard Edition (64-bit) on Windows Server 2019 Standard 10.0 &lt;X64&gt; (Build 17763: ) (Hypervisor)\n\n\n(1 rows affected)\n</code></pre>"},{"location":"tools/zabbix/MSSQL/#configure-zabbix-server","title":"Configure Zabbix server","text":"<ul> <li>Go to zabbix server -&gt; configuration -&gt; host -&gt; template and add MSSQL by ODBC template</li> <li>Go to zabbix server -&gt; configuration -&gt; host -&gt; macros</li> </ul>"},{"location":"services/Sherlock-Vault/","title":"Sherlock Hasicorp Vault","text":""},{"location":"services/Sherlock-Vault/#first-deployment","title":"First deployment","text":"<ul> <li>Apply terraform code (Vault is not available)</li> <li>Add vault to kubernetes (Vault will be available), see vdc-digitalsolutions</li> <li>Apply terraform code again (Vault will be configured)</li> </ul>"},{"location":"services/Sherlock-Vault/#vault-setup","title":"Vault setup","text":"Bash<pre><code>kubectl exec vault-0 -- vault status\nkubectl exec -it vault-0 -- vault operator init\n\nRecovery Key 1: &lt;redacted&gt;\nRecovery Key 2: &lt;redacted&gt;\nRecovery Key 3: &lt;redacted&gt;\nRecovery Key 4: &lt;redacted&gt;\nRecovery Key 5: &lt;redacted&gt;\n\nInitial Root Token: &lt;redacted&gt;\n\nkubectl exec -it vault-0 -- vault operator unseal &lt;key-1&gt;\nkubectl exec -it vault-0 -- vault operator unseal &lt;key-2&gt;\nkubectl exec -it vault-0 -- vault operator unseal &lt;key-3&gt;\n\nexport CLUSTER_ROOT_TOKEN=&lt;root-token&gt;\nkubectl exec vault-0 -- vault login $CLUSTER_ROOT_TOKEN\nkubectl exec vault-0 -- vault operator raft list-peers\n</code></pre> <p>Try to delete a vault to check auto unseal feature.</p> Bash<pre><code>kubectl delete pod vault-0\n</code></pre>"},{"location":"services/Sherlock-Vault/#restore-vault","title":"Restore vault","text":"Bash<pre><code>vault operator raft list-peers\nNode       Address                        State       Voter\n----       -------                        -----       -----\nvault-0    vault-0.vault-internal:8201    follower    true\nvault-1    vault-1.vault-internal:8201    follower    true\nvault-2    vault-2.vault-internal:8201    leader      true\n\n# Use the leader\nkubectl cp ~/Download/my-latest.snapshot vault-2:/vault/file/my-latest.snapshot\nkubectl exec -it vault-2 -- sh\nvault login\nvault operator raft snapshot restore -force /vault/file/my-latest.snapshot # wait for restarts\n</code></pre>"},{"location":"services/Sherlock-Vault/#audit-logs","title":"Audit logs","text":"<ul> <li>Enable audit logs: https://support.hashicorp.com/hc/en-us/articles/13444259109523-Information-in-regards-to-Audit-Log-when-running-Vault-in-Kubernetes</li> <li>Enable audit logs rotator: https://github.com/hashicorp/vault-helm/issues/142</li> </ul>"},{"location":"services/Sherlock-Vault/#login-to-vault","title":"Login to vault","text":"<p>Login to vault memeber:</p> Bash<pre><code>kubectl exec -it vault-0 -- sh\nvault login {CLUSTER_TOKEN}\nvault status\nvault operator raft list-peers\n</code></pre>"},{"location":"services/Sherlock-Vault/#init","title":"Init","text":"<p>Init vault and backup Recovery keys and Initial root token.</p> Bash<pre><code>vault operator init\nvault status\n</code></pre> <p>Join other nodes: <code>vault-1</code> and <code>vault-2</code>.</p> Bash<pre><code>kubectl exec -it vault-1 -- sh\nvault operator raft join http://vault-0.vault-internal:8200\nvault operator unseal {VAULT_UNSEAL_KEY}\n</code></pre>"},{"location":"services/Sherlock-Vault/#configure-aws-ssm-parameter-store","title":"Configure AWS SSM Parameter store","text":"<p>For AWS Cloud parameters you need to set the variables via UI (cc-critical0 account) or CLI:</p> Bash<pre><code># Set SECURE value\naws ssm put-parameter \\\n--name \"/critical0-sherlock/root\" \\\n--type \"SecureString\" \\\n--value ''\n\naws ssm put-parameter \\\n--name \"/critical0-sherlock/oidc_client_secret\" \\\n--type \"SecureString\" \\\n--value ''\n\n# Read parameters\n\naws ssm get-parameter --name \"/critical0-sherlock/root\" --with-decryption | jq .Parameter.Value\naws ssm get-parameter --name \"/critical0-sherlock/oidc_client_secret\" --with-decryption | jq .Parameter.Value\n</code></pre>"},{"location":"services/Sherlock-Vault/#more-docs","title":"More docs","text":"<ul> <li>https://developer.hashicorp.com/vault/tutorials/standard-procedures/sop-backup</li> <li>https://michaellin.me/backup-vault-with-raft-storage-on-kubernetes/</li> <li>https://tekanaid.com/posts/hashicorp-vault-backup-and-restore-raft-snapshots-from-kubernetes-to-aws-s3/</li> </ul>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/","title":"CR240263: Assessment Kubernetes","text":"<p>Studio e Implementazione di una Soluzione Architetturale Basata su Kubernetes per l\u2019Hosting dei Servizi di Posta Elettronica.</p>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/#introduzione","title":"Introduzione","text":"<p>Italiaonline intende realizzare un'infrastruttura dedicata all\u2019hosting di un servizio di posta elettronica basato sul Appsuite, personalizzato e integrato con il supporto di Open-Xchange, un fornitore esterno. Criticalcase fornir\u00e0 il supporto necessario per la progettazione, implementazione e messa in produzione della soluzione infrastrutturale.</p>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/#architettura-della-soluzione","title":"Architettura della soluzione","text":"<p>La soluzione infrastrutturale sar\u00e0 basata su Kubernetes Vanilla, implementato su server fisici (bare-metal) dedicati al servizio di posta. L\u2019architettura prevede:</p> <ul> <li>Due ambienti separati: Produzione e Staging.</li> <li>Cluster Kubernetes per ciascun ambiente, ognuno composto da 3 nodi worker fisici.</li> <li>Control-Plane ospitata su macchine virtuali.</li> <li>Scalabilit\u00e0 progressiva fino a 23 server fisici, partendo da un\u2019implementazione iniziale su 6 server.</li> </ul> <p>L\u2019obiettivo \u00e8 garantire un\u2019infrastruttura resiliente e scalabile, capace di supportare la crescita futura e ottimizzare la gestione delle risorse.</p>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/#automazione-e-ottimizzazione","title":"Automazione e ottimizzazione","text":"<p>Per garantire un'efficiente gestione dell\u2019ambiente, Criticalcase individuer\u00e0 e proporr\u00e0 metodologie, strategie e strumenti di automazione per:</p> <ul> <li>Ottimizzare e automatizzare l\u2019installazione e configurazione di nuovi nodi nel cluster Kubernetes.</li> <li>Automatizzare attivit\u00e0 operative per ridurre gli interventi manuali e aumentare l'efficienza operativa.</li> <li>Integrare strumenti di monitoraggio e alerting, per garantire la visibilit\u00e0 e la gestione proattiva dell\u2019infrastruttura.</li> </ul>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/#formazione-e-condivisione-della-conoscenza","title":"Formazione e condivisione della conoscenza","text":"<p>Al fine di garantire al Cliente la massima autonomia nella gestione della soluzione, saranno previste attivit\u00e0 di formazione e documentazione, tra cui:</p> <ul> <li>Discussione e condivisione delle scelte architetturali prima dell\u2019implementazione.</li> <li>Sessioni di training on-the-job, organizzate in un pacchetto di giornate a consumo, per trasferire le competenze necessarie.</li> <li>Documentazione tecnica dettagliata sulle configurazioni e sulle procedure operative.</li> </ul>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/#documentazione-open-xchange","title":"Documentazione Open-Xchange","text":"<ul> <li>https://documentation.open-xchange.com/appsuite/operation-guides/requirements.html</li> <li>https://documentation.open-xchange.com/appsuite/operation-guides/kubernetes.html</li> </ul>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/bastion-host/","title":"Bastion Host","text":"<p>Il bastion host <code>k8s-maildev-bh-01.iol.sys</code> ha il compito di fornire un punto di accesso sicuro e centralizzato per accedere agli ambienti privati, come i cluster Kubernetes.</p> <p>Sul bastion host \u00e8 in esecuzione un cluster K3S single node, al cui interno viene installato Teleport tramite Flux.</p>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/bastion-host/#installazione-di-k3s","title":"Installazione di K3S","text":"<p>Tramite i seguenti comandi, \u00e8 possibile installare la pi\u00f9 recente versione di K3S, configurando l'hostname della macchina come nome del nodo, e recuperando automaticamente l'indirizzo IP pubblico attraverso il quale raggiungere il cluster</p> Bash<pre><code>export BASTION_PUBLIC_IP=$(curl -s ipconfig.io/ip)\ncurl -sfL https://get.k3s.io | sh -s - --tls-san ${BASTION_PUBLIC_IP} --tls-san ${HOSTNAME} --disable traefik --node-name ${HOSTNAME}\n</code></pre> <p>Una volta completata l'installazione, sar\u00e0 possibile recuperare il file kubeconfig da utilizzare per interagire con il cluster dalla propria macchina personale:</p> Bash<pre><code>cat /etc/rancher/k3s/k3s.yaml\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/bastion-host/#configurazione-continuous-delivery","title":"Configurazione Continuous Delivery","text":"<p>FluxCD \u00e8 uno strumento di continuous delivery che permette di mantenere allineato lo stato del cluster con le definizioni delle risorse contenuti in uno o pi\u00f9 repository Git.</p>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/bastion-host/#configurazione-gitlab","title":"Configurazione GitLab","text":"<p>FluxCD utilizza un Personal Access Token (PAT) per accedere al repository Git e recuperare i file. I PAT sono legati a una specifica utenza GitLab. I seguenti step sono stati seguiti per configurare le credenziali e il repository da utilizzare:</p> <ol> <li>\u00c8 stata creata l'utenza <code>@k8smail</code> le cui credenziali sono memorizzate in Sherlock</li> <li>\u00c8 stato creato un PAT da https://gitlab.italiaonline.it/profile/personal_access_tokens con scadenza a 10 anni e permessi completi</li> <li>\u00c8 stato individuato in k8s-utilities-flux il repository da sincronizzare, in particolare il path <code>/bh</code> per quanto riguarda il bastion host</li> </ol>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/bastion-host/#fluxcd","title":"FluxCD","text":"<p>Tramite i seguenti step \u00e8 possibile installare FluxCD e configurarlo per mantenere sincronizzato il cluster kubernetes con il repository individuato</p> <ol> <li> <p>Se non presente, installare sulla propria macchina la Flux CLI</p> </li> <li> <p>Assicurarsi di aver selezionato il context del cluster K3S installato sul Bastion Host. \u00c8 possibile verificarlo con il comando:</p> </li> </ol> Bash<pre><code>kubectl get nodes\n</code></pre> <ol> <li>Eseguire il seguente comando di bootstrap, e utilizzare il PAT precedentemente creato per autorizzare la sincronizzazione</li> </ol> Bash<pre><code>flux bootstrap gitlab --ssh-hostname=gitlab.italiaonline.it:443 --hostname=gitlab.italiaonline.it --owner=k8s-mail-appsuite --repository=k8s-utilities-flux --branch=master --components-extra=image-reflector-controller,image-automation-controller --read-write-key --path=./bh --token-auth\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/bastion-host/#sealed-secret","title":"Sealed Secret","text":"<p>Sealed Secret \u00e8 un tool che permette di caricare in maniera sicura i secret Kubernetes su repository Git. Esso infatti genera una coppia di chiavi, delle quali la privata risiede esclusivamente all'interno del cluster, mentre la pubblica pu\u00f2 essere utilizzata per cifrare i secret.</p> <p>L'installazione del tool \u00e8 stata effettuata utilizzando i file manifest presenti nel path '/bh/sealed-secrets' del repository sincronizzato.</p> <p>Una volta effettuata l'installazione di Sealed Secret, \u00e8 possibile recuperare la chiave pubblica tramite il seguente comando per memorizzarla e poi copiarla nel path <code>/bh</code></p> Bash<pre><code>kubeseal --fetch-cert \\\n--controller-name=sealed-secrets-controller \\\n--controller-namespace=kube-system \\\n&gt; pub-sealed-secrets.pem\n</code></pre> <p>\u00c8 importante che <code>secret-*.yaml</code> sia presente nel file <code>.gitignore</code> per evitare l'upload indesiderato di dati sensibili.</p>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/bastion-host/#teleport","title":"Teleport","text":"<p>Teleport \u00e8 uno strumento che permette di centralizzare gli accessi ai componenti privati della propria architettura, garantendo MFA, logging e registrazione delle sessioni.</p> <p>L'installazione del tool \u00e8 stata effettuata utilizzando i file manifest presenti nel path '/bh/teleport' del repository sincronizzato.</p> <p>In particolare:</p> <ul> <li>il file teleport.yaml contiene i parametri di configurazione di Teleport e la versione installata</li> <li>il file sealed-tls-teleport.yaml contiene il certificato TLS usato per validare il traffico HTTPS.   Per generarlo, \u00e8 possibile creare il file <code>secret-tls-teleport.yaml</code> con il seguente contenuto:</li> </ul> YAML<pre><code>apiVersion: v1\ndata:\n  tls.crt: &lt;tls.crt&gt;\n  tls.key: &lt;tls.key&gt;\nkind: Secret\nmetadata:\n  name: tls-web\n  namespace: teleport\ntype: kubernetes.io/tls\n</code></pre> <p>e poi cifrarlo con il seguente comando, da eseguire dall'interno del path <code>/bh/teleport</code>:</p> Bash<pre><code>kubeseal --format=yaml --cert=../pub-sealed-secrets.pem &lt; ./secret-tls-teleport.yaml &gt; ./sealed-tls-teleport.yaml\n</code></pre> <ul> <li>il file sealed-token-teleport.yaml contiene il token che viene utilizzato per registrare nodi, cluster ecc. al server Teleport.   Per generarlo, \u00e8 possibile creare il file <code>secret-tls-teleport.yaml</code> con il seguente contenuto:</li> </ul> YAML<pre><code>apiVersion: v1\ndata:\n  token: &lt;randomly-generated-token&gt;\nkind: Secret\nmetadata:\n  name: node-token-teleport\n  namespace: teleport\ntype: kubernetes.io/tls\n</code></pre> <p>E poi cifrarlo con il seguente comando, da eseguire dall'interno del path <code>/bh/teleport</code>:</p> Bash<pre><code>kubeseal --format=yaml --cert=../pub-sealed-secrets.pem &lt; ./secret-token-teleport.yaml &gt; ./sealed-token-teleport.yaml\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/fluxcd/","title":"FluxCD Kubernetes","text":""},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/fluxcd/#pre-requisites","title":"Pre requisites","text":"<ul> <li>Install flux cli: https://fluxcd.io/flux/installation/</li> <li>Install kubeseal: https://github.com/bitnami-labs/sealed-secrets?tab=readme-ov-file#linux</li> </ul>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/fluxcd/#install-pre-commit","title":"Install pre-commit","text":"<p>Before you can run hooks, you need to have the pre-commit package manager installed: https://pre-commit.com/#installation</p> Bash<pre><code>pre-commit install\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/fluxcd/#flux-bootstrap","title":"Flux bootstrap","text":"<ul> <li>Get a Gitlab path for <code>k8s-mail-appsuite</code> user: https://gitlab.italiaonline.it/profile/personal_access_tokens</li> </ul>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/fluxcd/#maildev","title":"maildev","text":"Bash<pre><code>kubie ctx iol-maildev\nflux bootstrap gitlab --ssh-hostname=gitlab.italiaonline.it:443 --hostname=gitlab.italiaonline.it --owner=k8s-mail-appsuite --repository=k8s-flux --branch=master --components-extra=image-reflector-controller,image-automation-controller --read-write-key --path=./clusters/dev --token-auth\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/fluxcd/#mailprod-exp-experimental-prende-una-piccola-percentuale-della-customer-base","title":"mailprod-exp (experimental): prende una piccola percentuale della customer base","text":"Bash<pre><code>kubie ctx iol-mailprd-exp\nflux bootstrap gitlab --ssh-hostname=gitlab.italiaonline.it:443 --hostname=gitlab.italiaonline.it --owner=k8s-mail-appsuite --repository=k8s-flux --branch=master --components-extra=image-reflector-controller,image-automation-controller --read-write-key --path=./clusters/mailprod-exp/main --token-auth\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/fluxcd/#mailprod-std-standard","title":"mailprod-std (standard)","text":"Bash<pre><code>kubie ctx iol-mailprd-exp\nflux bootstrap gitlab --ssh-hostname=gitlab.italiaonline.it:443 --hostname=gitlab.italiaonline.it --owner=k8s-mail-appsuite --repository=k8s-flux --branch=master --components-extra=image-reflector-controller,image-automation-controller --read-write-key --path=./clusters/mailprod-std/main --token-auth\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/fluxcd/#grafana-secrets","title":"Grafana secrets","text":"Bash<pre><code>cd clusters/$CLUSTER_NAME/main\nkubectl create secret generic grafana-credentials --from-literal=admin-user=admin --from-literal=admin-password=XXXXXXX --dry-run=client -o yaml -n kube-monitoring &gt; monitoring/secret-grafana-credentials.yaml\nkubeseal --format=yaml --cert=pub-sealed-secrets.pem &lt; monitoring/secret-grafana-credentials.yaml &gt; monitoring/sealed-grafana-credentials.yaml\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/fluxcd/#create-kubeconfig-for-namespace","title":"Create kubeconfig for namespace","text":"Bash<pre><code>cd bin\n./create-kubeconfig as8dev\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/fluxcd/#sealed-secret","title":"Sealed Secret","text":"<p>See kubeseal.md</p>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/image-registry/","title":"Image registry","text":"<p>Sul server <code>k8s-maildev-rg-01.iol.sys</code> sono in esecuzione il container registry Harbor e i servizi necessari al suo funzionamento, come database e Redis.</p> <p>Sul server \u00e8 in esecuzione un cluster K3S single node, al cui interno viene installato Teleport tramite Flux.</p>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/image-registry/#installazione-di-k3s","title":"Installazione di K3S","text":"<p>Tramite i seguenti comandi, \u00e8 possibile installare la pi\u00f9 recente versione di K3S, configurando l'hostname della macchina come nome del nodo, e recuperando automaticamente l'indirizzo IP pubblico attraverso il quale raggiungere il cluster</p> Bash<pre><code>export SERVER_PUBLIC_IP=$(curl -s ipconfig.io/ip)\ncurl -sfL https://get.k3s.io | sh -s - --tls-san ${SERVER_PUBLIC_IP} --tls-san ${HOSTNAME} --disable traefik --node-name ${HOSTNAME}\n</code></pre> <p>Una volta completata l'installazione, sar\u00e0 possibile recuperare il file kubeconfig da utilizzare per interagire con il cluster dalla propria macchina personale:</p> Bash<pre><code>cat /etc/rancher/k3s/k3s.yaml\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/image-registry/#configurazione-continuous-delivery","title":"Configurazione Continuous Delivery","text":"<p>FluxCD \u00e8 uno strumento di continuous delivery che permette di mantenere allineato lo stato del cluster con le definizioni delle risorse contenuti in uno o pi\u00f9 repository Git.</p>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/image-registry/#configurazione-gitlab","title":"Configurazione GitLab","text":"<p>FluxCD utilizza un Personal Access Token (PAT) per accedere al repository Git e recuperare i file. I PAT sono legati a una specifica utenza GitLab. I seguenti step sono stati seguiti per configurare le credenziali e il repository da utilizzare:</p> <ol> <li>\u00c8 stata creata l'utenza <code>@k8smail</code> le cui credenziali sono memorizzate in Sherlock</li> <li>\u00c8 stato creato un PAT da https://gitlab.italiaonline.it/profile/personal_access_tokens con scadenza a 10 anni e permessi completi</li> <li>\u00c8 stato individuato in k8s-utilities-flux il repository da sincronizzare, in particolare il path <code>/rg</code> per quanto riguarda l'image registry</li> </ol>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/image-registry/#fluxcd","title":"FluxCD","text":"<p>Tramite i seguenti step \u00e8 possibile installare FluxCD e configurarlo per mantenere sincronizzato il cluster kubernetes con il repository individuato</p> <ol> <li> <p>Se non presente, installare sulla propria macchina la Flux CLI</p> </li> <li> <p>Assicurarsi di aver selezionato il context del cluster K3S installato sul server. \u00c8 possibile verificarlo con il comando:</p> </li> </ol> Bash<pre><code>kubectl get nodes\n</code></pre> <ol> <li>Eseguire il seguente comando di bootstrap, e utilizzare il PAT precedentemente creato per autorizzare la sincronizzazione    Bash<pre><code>flux bootstrap gitlab --ssh-hostname=gitlab.italiaonline.it:443 --hostname=gitlab.italiaonline.it --owner=k8s-mail-appsuite --repository=k8s-utilities-flux --branch=master --components-extra=image-reflector-controller,image-automation-controller --read-write-key --path=./rg --token-auth\n</code></pre></li> </ol>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/image-registry/#sealed-secret","title":"Sealed Secret","text":"<p>Sealed Secret \u00e8 un tool che permette di caricare in maniera sicura i secret Kubernetes su repository Git. Esso infatti genera una coppia di chiavi, delle quali la privata risiede esclusivamente all'interno del cluster, mentre la pubblica pu\u00f2 essere utilizzata per cifrare i secret.</p> <p>L'installazione del tool \u00e8 stata effettuata utilizzando i file manifest presenti nel path '/rg/sealed-secrets' del repository sincronizzato.</p> <p>Una volta effettuata l'installazione di Sealed Secret, \u00e8 possibile recuperare la chiave pubblica tramite il seguente comando per memorizzarla e poi copiarla nel path <code>/rg</code></p> Bash<pre><code>kubeseal --fetch-cert \\\n--controller-name=sealed-secrets-controller \\\n--controller-namespace=kube-system \\\n&gt; pub-sealed-secrets.pem\n</code></pre> <p>\u00c8 importante che <code>secret-*.yaml</code> sia presente nel file <code>.gitignore</code> per evitare l'upload indesiderato di dati sensibili.</p>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/image-registry/#local-path-provisione","title":"Local path provisione","text":"<p>Local Path \u00e8 un provisioner che permette di utilizzare lo storage locale dei nodi Kubernetes per fornire Persistent Volume ai pod.</p> <p>L'installazione del tool \u00e8 stata effettuata utilizzando il file manifest 'rg/local-path/local-path.yaml'. La configurazione mette a disposizione del cluster la storage class <code>local-path</code>.</p>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/image-registry/#nfs-subdir-external-provisioner","title":"NFS Subdir External Provisioner","text":"<p>NFS Subdir External Provisioner \u00e8 un provisioner che permette di utilizzare un server NFS esistente per fornire Persistent Volume ai pod.</p> <p>L'installazione del tool \u00e8 stata effettuata utilizzando il file manifest 'rg/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner.yaml'. La configurazione mette a disposizione del cluster la storage class <code>nfs-client</code>.</p>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/image-registry/#cert-manager","title":"Cert manager","text":"<p>Cert manager \u00e8 un tool che permette di generare automaticamente certificati TLS.</p> <p>L'installazione del tool \u00e8 stata effettuata utilizzando i file manifest presenti nel path '/rg/cert-manager' del repository sincronizzato.</p> <p>In particolare:</p> <ul> <li>il file cert-manager.yaml contiene i parametri di configurazione di Cert manager e la versione installata</li> <li>il file sealed-internal-ca.yaml contiene la certification authority usata per generare i certificati.   Per generarlo, \u00e8 possibile creare il file <code>secret-internal-ca.yaml</code> con il seguente contenuto:</li> </ul> YAML<pre><code>apiVersion: v1\ndata:\n  tls.crt: &lt;tls.crt&gt;\n  tls.key: &lt;tls.key&gt;\nkind: Secret\nmetadata:\n  name: internal-ca\n  namespace: cert-manager\ntype: kubernetes.io/tls\n</code></pre> <p>e poi cifrarlo con il seguente comando, da eseguire dall'interno del path <code>/rg/cert-manager</code>:</p> Bash<pre><code>kubeseal --format=yaml --cert=../pub-sealed-secrets.pem &lt; ./secret-internal-ca.yaml &gt; ./sealed-internal-ca.yaml\n</code></pre> <ul> <li>il file cluster-issuer.yaml contiene la Custom Resource Definition che verr\u00e0 utilizzata da Cert manager per identificare quale CA utilizzare per la generazione dei certificati.</li> </ul>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/image-registry/#redis","title":"Redis","text":"<p>Harbor necessit\u00e0 di un'istanza Redis, la quale viene installata tramite il file redis.yaml. La configurazione prevede l'uso della modalit\u00e0 standalone, con autenticazione disabilitata e l'uso della storage class <code>local-path</code> per garantire la persistenza dei dati.</p>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/image-registry/#nginx-ingress-controller","title":"Nginx Ingress Controller","text":"<p>Nginx Ingress Controller permette di utilizzare Nginx come reverse proxy e load balancer all'interno del cluster Kubernetes.</p> <p>L'installazione del tool \u00e8 stata effettuata utilizzando i file manifest presenti nel path 'rg/ingress-controller' del repository sincronizzato. La configurazione nel file 'rg/ingress-controller/ingress-controller.yaml' prevede il mapping delle porte 80 e 443 per ricevere il traffico sul pod di Nginx attraverso un service di tipo NodePort.</p>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/image-registry/#cnpg","title":"CNPG","text":"<p>CloudNativePG \u00e8 una soluzione per effettuare il deploy di database PostgreSQL all'interno di cluster Kubernetes.</p> <p>L'installazione dell'operator, il quale si occuper\u00e0 di effettuare il deploy dei database osservando le CRD deployate sul cluster, \u00e8 stata effettuata utilizzando i file manifest presenti nel path '/rg/cnpg'.</p>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/image-registry/#harbor","title":"Harbor","text":"<p>Harbor \u00e8 un image registry open source.</p> <p>L'installazione del tool \u00e8 stata effettuata utilizzando i file manifest presenti nel path '/rg/harbor' del repository sincronizzato.</p> <p>In particolare:</p> <ul> <li>il file sealed-cnpg-harbor-credentials.yaml contiene le credenziali per l'interazione con il database.   Per generarlo, \u00e8 possibile creare il file <code>secret-cnpg-harbor-credentials.yaml</code> con il seguente contenuto:</li> </ul> YAML<pre><code>apiVersion: v1\ndata:\n  password: &lt;psql-password&gt;\n  username: &lt;username&gt;\nkind: Secret\nmetadata:\n  name: cnpg-harbor-credentials\n  namespace: harbor\ntype: kubernetes.io/basic-auth\n</code></pre> <p>E poi cifrarlo con il seguente comando, da eseguire dall'interno del path <code>/rg/harbor</code>:</p> Bash<pre><code>kubeseal --format=yaml --cert=../pub-sealed-secrets.pem &lt; ./secret-cnpg-harbor-credentials.yaml &gt; ./sealed-cnpg-harbor-credentials.yaml\n</code></pre> <ul> <li>il file cnpg-db.yaml contiene la configurazione del database PostgreSQL.   La configurazione prevede:</li> <li>l'uso della storage class 'local-path' per la conservazione dei dati.</li> <li>la creazione di un database con nome 'harbor'</li> <li>l'indicazione del nome del 'secret' contenente la password dell'utente 'harbor'</li> <li>il file sealed-tls-harbor.yaml contiene il certificato TLS usato per validare il traffico HTTPS.   Per generarlo, \u00e8 possibile creare il file <code>secret-tls-harbor.yaml</code> con il seguente contenuto:</li> </ul> YAML<pre><code>apiVersion: v1\ndata:\n  ca.crt: &lt;ca.crt&gt;\n  tls.crt: &lt;tls.crt&gt;\n  tls.key: &lt;tls.key&gt;\nkind: Secret\nmetadata:\n  name: harbor-tls\n  namespace: harbor\ntype: kubernetes.io/tls\n</code></pre> <p>e poi cifrarlo con il seguente comando, da eseguire dall'interno del path <code>/rg/harbor</code>:</p> Bash<pre><code>kubeseal --format=yaml --cert=../pub-sealed-secrets.pem &lt; ./secret-tls-harbor.yaml &gt; ./sealed-tls-harbor.yaml\n</code></pre> <ul> <li>il file sealed-harbor-credentials.yaml contiene la password dell'admin Harbor, la password dell'utente del database e la secretKey usata per la cifratura delle immagini.   Per generarlo, \u00e8 possibile creare il file <code>secret-harbor-credentials.yaml</code> con il seguente contenuto:</li> </ul> YAML<pre><code>apiVersion: v1\ndata:\n  HARBOR_ADMIN_PASSWORD: &lt;harbor-admin-password&gt;\n  password: &lt;db-password&gt;\n  secretKey: &lt;16-char-string&gt;\nkind: Secret\nmetadata:\n  name: harbor-credentials\n  namespace: harbor\ntype: Opaque\n</code></pre> <p>e poi cifrarlo con il seguente comando, da eseguire dall'interno del path <code>/rg/harbor</code>:</p> Bash<pre><code>kubeseal --format=yaml --cert=../pub-sealed-secrets.pem &lt; ./secret-harbor-credentials.yaml &gt; ./sealed-harbor-credentials.yaml\n</code></pre> <ul> <li>il file harbor.yaml contiene la configurazione del tool Harbor.   La configurazione prevede le seguenti sezioni:</li> <li>l'nginx ingress controller viene usato per esporre il registry all'esterno di Kubernetes</li> <li>viene specificato il nome del secret che contiene la password dell'admin, la secret key e la password del database</li> <li>viene usata la storage class 'nfs-client' per garantire la persistenza dei dati.</li> <li>viene utilizzato il database postgreSQL deployato tramite CNPG</li> <li>viene specificato l'endpoint di redis</li> </ul>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/kubespray/","title":"Kubespray","text":""},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/kubespray/#setup-bastion-host","title":"Setup bastion host","text":"Bash<pre><code>sudo su -\n\n# Install the required packages using DNF (Dandified Yum)\ndnf install -y git tmux python3.12\n\n# Create a symbolic link to the Python 3.12 executable\nln -f /usr/bin/python3.12 /usr/bin/python\nln -f /usr/bin/python3.12 /usr/bin/python3\n\n# Clone the kubespray repository from GitLab\ngit clone https://k8smail:PASSWORD@gitlab.italiaonline.it/k8s-mail-appsuite/k8s-maildev-kubespray.git kubespray\n\n# Activate the virtual environment\nVENVDIR=kubespray-venv\nKUBESPRAYDIR=kubespray\npython3 -m venv $VENVDIR\nsource $VENVDIR/bin/activate\ncd $KUBESPRAYDIR\n\n# Install the required packages for kubespray\npip install -U -r requirements.txt\npip install -U ruamel.yaml\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/kubespray/#connection","title":"Connection","text":"Bash<pre><code>ssh k8s-maildev-bh-01.iol.sys\nsudo su -\n\n# Activate the virtual environment\ntmux\n# tmux attach\nsource kubespray-venv/bin/activate\ncd kubespray\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/kubespray/#ansible-commands","title":"Ansible commands","text":""},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/kubespray/#pre-deployment","title":"Pre deployment","text":"Bash<pre><code>ansible-playbook -i inventory-k8s-maildev/hosts.yaml --become --become-user=root --private-key /root/.ssh/id_ed25519 --extra-vars '@inventory-k8s-maildev/extra_vars.yml' iol.yml\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/kubespray/#setup-cluster","title":"Setup cluster","text":"Bash<pre><code>ansible-playbook -i inventory-k8s-maildev/hosts.yaml --become --become-user=root --private-key /root/.ssh/id_ed25519 --extra-vars '@inventory-k8s-maildev/extra_vars.yml' cluster.yml\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/kubespray/#upgrade-cluster","title":"Upgrade cluster","text":"Bash<pre><code>ansible-playbook -i inventory-k8s-maildev/hosts.yaml --become --become-user=root --private-key /root/.ssh/id_ed25519 --extra-vars '@inventory-k8s-maildev/extra_vars.yml' upgrade-cluster.yml\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/tools/flux/","title":"Flux CLI","text":"Bash<pre><code>flux reconcile kustomization -n flux-system flux-system --with-source\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/tools/kubeseal/","title":"Kubeseal","text":""},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/tools/kubeseal/#create-a-secret-from-files","title":"Create a secret from files","text":"Bash<pre><code>kubectl create secret generic sample-key \\\n --from-file=sample-key-private=oauth-private.key \\\n --from-file=sample-key-public=oauth-public.key \\\n --dry-run=client -o yaml &gt; secret-unencrypted-sample-key.yaml`\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/tools/kubeseal/#create-a-secret-from-literals","title":"Create a secret from literals","text":"Bash<pre><code>kubectl create secret generic sample-key \\\n --from-literal=sample-key-private=abc223 \\\n --dry-run=client -o yaml &gt; secret-unencrypted-sample-key.yaml\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/tools/kubeseal/#retrieve-sealed-secret-public-key","title":"Retrieve sealed secret public key","text":"Bash<pre><code>kubeseal --fetch-cert \\\n  --controller-name=sealed-secrets-controller \\\n  --controller-namespace=kube-system \\\n  &gt; pub-sealed-secrets.pem\n</code></pre>"},{"location":"customers/italiaonline-C2287/Kubernetes-onpremise/tools/kubeseal/#encrypt-your-secret-using-kubeseal","title":"Encrypt your secret using kubeseal","text":"<p>Warning: you must use <code>secret-*.yaml</code> for the unencrypted filename because is ignored in <code>.gitignore</code>.</p> <p>Note: you should use <code>sealed-*.yml</code> for the sealed filename.</p> Bash<pre><code>kubeseal --format=yaml --cert=pub-sealed-secrets.pem &lt; secret-unencrypted-sample-key.yaml &gt; sealed-sample-key.yaml\n</code></pre> <p>Remove <code>secret-unencrypted-sample-key.yaml</code> and push the new encrypted file</p> Bash<pre><code>rm secret-unencrypted-sample-key.yaml\n</code></pre>"}]}